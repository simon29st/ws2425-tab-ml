% [1 Introduction]

% cf https://github.com/sumny/eagga/ + https://dl.acm.org/doi/10.1145/3583131.3590380
@inproceedings{EAGGA,
  author = {Schneider, Lennart and Bischl, Bernd and Thomas, Janek},
  title = {Multi-Objective Optimization of Performance and Interpretability of Tabular Supervised Machine Learning Models},
  year = {2023},
  isbn = {9798400701191},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3583131.3590380},
  doi = {10.1145/3583131.3590380},
  abstract = {We present a model-agnostic framework for jointly optimizing the predictive performance and interpretability of supervised machine learning models for tabular data. Interpretability is quantified via three measures: feature sparsity, interaction sparsity of features, and sparsity of non-monotone feature effects. By treating hyperparameter optimization of a machine learning algorithm as a multi-objective optimization problem, our framework allows for generating diverse models that trade off high performance and ease of interpretability in a single optimization run. Efficient optimization is achieved via augmentation of the search space of the learning algorithm by incorporating feature selection, interaction and monotonicity constraints into the hyperparameter search space. We demonstrate that the optimization problem effectively translates to finding the Pareto optimal set of groups of selected features that are allowed to interact in a model, along with finding their optimal monotonicity constraints and optimal hyperparameters of the learning algorithm itself. We then introduce a novel evolutionary algorithm that can operate efficiently on this augmented search space. In benchmark experiments, we show that our framework is capable of finding diverse models that are highly competitive or outperform state-of-the-art XGBoost or Explainable Boosting Machine models, both with respect to performance and interpretability.},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  pages = {538–547},
  numpages = {10},
  keywords = {group structure, evolutionary computation, multi-objective, tabular data, interpretability, performance, supervised learning},
  location = {Lisbon, Portugal},
  series = {GECCO '23}
}


% [2 Background and Related Work]

@inproceedings{Zach2019InterpretabilityOD,
  title={Interpretability of deep neural networks},
  author={J. Zach},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:198979458}
}


@article{review_NN_interpretability,
  title = {Interpretability of deep neural networks: A review of methods, classification and hardware},
  journal = {Neurocomputing},
  volume = {601},
  pages = {128204},
  year = {2024},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2024.128204},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231224009755},
  author = {Thanasis Antamis and Anastasis Drosou and Thanasis Vafeiadis and Alexandros Nizamis and Dimosthenis Ioannidis and Dimitrios Tzovaras},
  keywords = {XAI, Deep neural networks, xDNN, Survey},
  abstract = {Artificial intelligence, and especially deep neural networks, have evolved substantially in the recent years, infiltrating numerous domains of applications, often greatly impactful to society’s well-being. As a result, the need to understand how these models operate in depth and to access explanations of their decisions has become more vital than ever. Tending to this demand, the following paper aims to provide a thorough overview of the methods that have so far been developed to explain deep neural networks. Key aspects of explainability are defined and a straightforward classification of existing approaches is introduced, along with numerous examples. The task of realizing these methods on hardware is also discussed to complete the understanding of their application.}
}


@ARTICLE{survey_NN_interpretability,
  author={Zhang, Yu and Tiňo, Peter and Leonardis, Aleš and Tang, Ke},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={A Survey on Neural Network Interpretability}, 
  year={2021},
  volume={5},
  number={5},
  pages={726-742},
  keywords={Taxonomy;Deep learning;Tools;Reliability;Decision trees;Training;Task analysis;Machine learning;neural networks;inter-pretability;survey},
  doi={10.1109/TETCI.2021.3100641}
}


@book{molnar2022,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  year       = {2022},
  subtitle   = {A Guide for Making Black Box Models Explainable},
  edition    = {2},
  url        = {https://christophm.github.io/interpretable-ml-book}
}


@article{DoshiVelez2017TowardsAR,
  title={Towards A Rigorous Science of Interpretable Machine Learning},
  author={Finale Doshi-Velez and Been Kim},
  journal={arXiv: Machine Learning},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:11319376}
}


@inproceedings{feurer_hyperparameter_2019,
    author = {Feurer, Matthias and Hutter, Frank},
    title = {Hyperparameter Optimization},
    pages = {3-38},
    chapter = {1},
    crossref = {automl}
}
% TODO: for some reason this doesn't show up in references, even though 'feurer_hyperparameter_2019' is cited and has this as crossref
@book{automl,
    editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
    publisher = {Springer},
    title = {Automatic Machine Learning: Methods, Systems, Challenges},
    year = {2019}
}


@book{genetic_algos,
  author = {Goldberg, David E.},
  title = {Genetic Algorithms in Search, Optimization and Machine Learning},
  year = {1989},
  isbn = {0201157675},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  address = {USA},
  edition = {1st},
  abstract = {From the Publisher:This book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.}
}


@misc{hansen2023cmaevolutionstrategytutorial,
      title={The CMA Evolution Strategy: A Tutorial}, 
      author={Nikolaus Hansen},
      year={2023},
      eprint={1604.00772},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1604.00772}, 
}


@Article{differential_evolution,
  author={Storn, Rainer and Price, Kenneth},
  title={Differential Evolution -- A Simple and Efficient Heuristic for global Optimization over Continuous Spaces},
  journal={Journal of Global Optimization},
  year={1997},
  month={Dec},
  day={01},
  volume={11},
  number={4},
  pages={341-359},
  abstract={A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
  issn={1573-2916},
  doi={10.1023/A:1008202821328},
  url={https://doi.org/10.1023/A:1008202821328}
}

@misc{frazier2018tutorialbayesianoptimization,
      title={A Tutorial on Bayesian Optimization}, 
      author={Peter I. Frazier},
      year={2018},
      eprint={1807.02811},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1807.02811}, 
}






% [Software used]

% OpenML, cf. https://www.openml.org/terms
% OpenML as a whole, cf. https://dl.acm.org/doi/10.1145/2641190.2641198
@article{OpenML,
  author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
  title = {OpenML: networked science in machine learning},
  year = {2014},
  issue_date = {December 2013},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {15},
  number = {2},
  issn = {1931-0145},
  url = {https://doi.org/10.1145/2641190.2641198},
  doi = {10.1145/2641190.2641198},
  abstract = {Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.},
  journal = {SIGKDD Explor. Newsl.},
  month = jun,
  pages = {49–60},
  numpages = {12}
}
% OpenML Python package, cf. https://github.com/openml/openml-python
@article{OpenMLPython,
  author  = {Matthias Feurer and Jan N. van Rijn and Arlind Kadra and Pieter Gijsbers and Neeratyoy Mallik and Sahithya Ravi and Andreas Müller and Joaquin Vanschoren and Frank Hutter},
  title   = {OpenML-Python: an extensible Python API for OpenML},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {100},
  pages   = {1--5},
  url     = {http://jmlr.org/papers/v22/19-920.html}
}


% NumPy, cf. https://numpy.org/citing-numpy/
@Article{         NumPy,
  title         = {Array programming with {NumPy}},
  author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                  van der Walt and Ralf Gommers and Pauli Virtanen and David
                  Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                  Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                  and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                  Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                  R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                  G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                  Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                  Travis E. Oliphant},
  year          = {2020},
  month         = sep,
  journal       = {Nature},
  volume        = {585},
  number        = {7825},
  pages         = {357--362},
  doi           = {10.1038/s41586-020-2649-2},
  publisher     = {Springer Science and Business Media {LLC}},
  url           = {https://doi.org/10.1038/s41586-020-2649-2} 
}


% pandas, cf. https://pandas.pydata.org/about/citing.html
% Sagemaker uses v1.5.3, cf. https://zenodo.org/records/7549438
@software{pandas1,
  author       = {The pandas development team},
  title        = {pandas-dev/pandas: Pandas},
  month        = jan,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v1.5.3},
  doi          = {10.5281/zenodo.7549438},
  url          = {https://doi.org/10.5281/zenodo.7549438},
}

@InProceedings{ pandas2,
  author    = { {W}es {M}c{K}inney },
  title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
  booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
  pages     = { 56 - 61 },
  year      = { 2010 },
  editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
  doi       = { 10.25080/Majora-92bf1922-00a }
}


% pytorch, cf. https://github.com/pytorch/pytorch/blob/main/CITATION.cff
@inproceedings{PyTorch,
  author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  title = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
  year = {2024},
  isbn = {9798400703850},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3620665.3640366},
  doi = {10.1145/3620665.3640366},
  abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages = {929–947},
  numpages = {19},
  location = {La Jolla, CA, USA},
  series = {ASPLOS '24}
}


% scikit-learn, cf. https://scikit-learn.org/stable/about.html
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}


% scipy, cf. https://scipy.org/citing-scipy/
@ARTICLE{SciPy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}


% pymoo, cf. https://pymoo.org/references.html
@ARTICLE{pymoo,
  author={J. {Blank} and K. {Deb}},
  journal={IEEE Access},
  title={pymoo: Multi-Objective Optimization in Python},
  year={2020},
  volume={8},
  number={},
  pages={89497-89509},
}


% tqdm, cf. https://github.com/tqdm/tqdm + https://zenodo.org/records/14231923
@software{tqdm,
  author       = {Casper da Costa-Luis and
                  Stephen Karl Larroque and
                  Kyle Altendorf and
                  Hadrien Mary and
                  richardsheridan and
                  Mikhail Korobov and
                  Noam Yorav-Raphael and
                  Ivan Ivanov and
                  Marcel Bargull and
                  Nishant Rodrigues and
                  Shawn and
                  Mikhail Dektyarev and
                  Michał Górny and
                  mjstevens777 and
                  Matthew D. Pagel and
                  Martin Zugnoni and
                  JC and
                  CrazyPython and
                  Charles Newey and
                  Antony Lee and
                  pgajdos and
                  Todd and
                  Staffan Malmgren and
                  redbug312 and
                  Orivej Desh and
                  Nikolay Nechaev and
                  Mike Boyle and
                  Max Nordlund and
                  MapleCCC and
                  Jack McCracken},
  title        = {tqdm: A fast, Extensible Progress Bar for Python
                   and CLI
                  },
  month        = nov,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v4.67.1},
  doi          = {10.5281/zenodo.14231923},
  url          = {https://doi.org/10.5281/zenodo.14231923},
  swhid        = {swh:1:dir:4556fa8b9e2bffacf634b5378660ecb3db68431e
                   ;origin=https://doi.org/10.5281/zenodo.595120;visi
                   t=swh:1:snp:3b7ead939423abc779ad7715fdd3e6f2a93e7e
                   71;anchor=swh:1:rel:1900ab2ad612c3d46028815e6bb1fd
                   cd0caf6166;path=/
                  },
}