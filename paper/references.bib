% [1 Introduction]

% cf https://github.com/sumny/eagga/ + https://dl.acm.org/doi/10.1145/3583131.3590380
@inproceedings{EAGGA,
  author = {Schneider, Lennart and Bischl, Bernd and Thomas, Janek},
  title = {Multi-Objective Optimization of Performance and Interpretability of Tabular Supervised Machine Learning Models},
  year = {2023},
  isbn = {9798400701191},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3583131.3590380},
  doi = {10.1145/3583131.3590380},
  abstract = {We present a model-agnostic framework for jointly optimizing the predictive performance and interpretability of supervised machine learning models for tabular data. Interpretability is quantified via three measures: feature sparsity, interaction sparsity of features, and sparsity of non-monotone feature effects. By treating hyperparameter optimization of a machine learning algorithm as a multi-objective optimization problem, our framework allows for generating diverse models that trade off high performance and ease of interpretability in a single optimization run. Efficient optimization is achieved via augmentation of the search space of the learning algorithm by incorporating feature selection, interaction and monotonicity constraints into the hyperparameter search space. We demonstrate that the optimization problem effectively translates to finding the Pareto optimal set of groups of selected features that are allowed to interact in a model, along with finding their optimal monotonicity constraints and optimal hyperparameters of the learning algorithm itself. We then introduce a novel evolutionary algorithm that can operate efficiently on this augmented search space. In benchmark experiments, we show that our framework is capable of finding diverse models that are highly competitive or outperform state-of-the-art XGBoost or Explainable Boosting Machine models, both with respect to performance and interpretability.},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  pages = {538–547},
  numpages = {10},
  keywords = {group structure, evolutionary computation, multi-objective, tabular data, interpretability, performance, supervised learning},
  location = {Lisbon, Portugal},
  series = {GECCO '23}
}


% [2 Background and Related Work]

@inproceedings{Zach2019InterpretabilityOD,
  title={Interpretability of deep neural networks},
  author={J. Zach},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:198979458}
}


@article{review_NN_interpretability,
  title = {Interpretability of deep neural networks: A review of methods, classification and hardware},
  journal = {Neurocomputing},
  volume = {601},
  pages = {128204},
  year = {2024},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2024.128204},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231224009755},
  author = {Thanasis Antamis and Anastasis Drosou and Thanasis Vafeiadis and Alexandros Nizamis and Dimosthenis Ioannidis and Dimitrios Tzovaras},
  keywords = {XAI, Deep neural networks, xDNN, Survey},
  abstract = {Artificial intelligence, and especially deep neural networks, have evolved substantially in the recent years, infiltrating numerous domains of applications, often greatly impactful to society’s well-being. As a result, the need to understand how these models operate in depth and to access explanations of their decisions has become more vital than ever. Tending to this demand, the following paper aims to provide a thorough overview of the methods that have so far been developed to explain deep neural networks. Key aspects of explainability are defined and a straightforward classification of existing approaches is introduced, along with numerous examples. The task of realizing these methods on hardware is also discussed to complete the understanding of their application.}
}


@ARTICLE{survey_NN_interpretability,
  author={Zhang, Yu and Tiňo, Peter and Leonardis, Aleš and Tang, Ke},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={A Survey on Neural Network Interpretability}, 
  year={2021},
  volume={5},
  number={5},
  pages={726-742},
  keywords={Taxonomy;Deep learning;Tools;Reliability;Decision trees;Training;Task analysis;Machine learning;neural networks;inter-pretability;survey},
  doi={10.1109/TETCI.2021.3100641}
}


@book{molnar2022,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  year       = {2022},
  subtitle   = {A Guide for Making Black Box Models Explainable},
  edition    = {2},
  url        = {https://christophm.github.io/interpretable-ml-book}
}


@article{DoshiVelez2017TowardsAR,
  title={Towards A Rigorous Science of Interpretable Machine Learning},
  author={Finale Doshi-Velez and Been Kim},
  journal={arXiv: Machine Learning},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:11319376}
}


@inproceedings{feurer_hyperparameter_2019,
    author = {Feurer, Matthias and Hutter, Frank},
    title = {Hyperparameter Optimization},
    pages = {3-38},
    chapter = {1},
    crossref = {automl}
}
@inproceedings{elsken_neural_2019,
    author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
    title = {Neural Architecture Search},
    pages = {69-86},
    chapter = {3},
    crossref = {automl}
}
@book{automl,
    editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
    publisher = {Springer},
    title = {Automatic Machine Learning: Methods, Systems, Challenges},
    year = {2019}
}


@book{genetic_algos,
  author = {Goldberg, David E.},
  title = {Genetic Algorithms in Search, Optimization and Machine Learning},
  year = {1989},
  isbn = {0201157675},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  address = {USA},
  edition = {1st},
  abstract = {From the Publisher:This book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.}
}


@misc{hansen2023cmaevolutionstrategytutorial,
      title={The CMA Evolution Strategy: A Tutorial}, 
      author={Nikolaus Hansen},
      year={2023},
      eprint={1604.00772},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1604.00772}, 
}


@Article{differential_evolution,
  author={Storn, Rainer and Price, Kenneth},
  title={Differential Evolution -- A Simple and Efficient Heuristic for global Optimization over Continuous Spaces},
  journal={Journal of Global Optimization},
  year={1997},
  month={Dec},
  day={01},
  volume={11},
  number={4},
  pages={341-359},
  abstract={A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
  issn={1573-2916},
  doi={10.1023/A:1008202821328},
  url={https://doi.org/10.1023/A:1008202821328}
}


@misc{frazier2018tutorialbayesianoptimization,
  title={A Tutorial on Bayesian Optimization}, 
  author={Peter I. Frazier},
  year={2018},
  eprint={1807.02811},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/abs/1807.02811}, 
}


@article{Borisov_2024,
  title={Deep Neural Networks and Tabular Data: A Survey},
  volume={35},
  ISSN={2162-2388},
  url={http://dx.doi.org/10.1109/TNNLS.2022.3229161},
  DOI={10.1109/tnnls.2022.3229161},
  number={6},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  publisher={Institute of Electrical and Electronics Engineers (IEEE)},
  author={Borisov, Vadim and Leemann, Tobias and Seßler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  year={2024},
  month=jun, pages={7499–7519}
}


@misc{zoph2017neuralarchitecturesearchreinforcement,
  title={Neural Architecture Search with Reinforcement Learning}, 
  author={Barret Zoph and Quoc V. Le},
  year={2017},
  eprint={1611.01578},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1611.01578}, 
}


@InProceedings{Zoph_2018_CVPR,
  author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  title = {Learning Transferable Architectures for Scalable Image Recognition},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2018}
}


@misc{saxena2017convolutionalneuralfabrics,
  title={Convolutional Neural Fabrics}, 
  author={Shreyas Saxena and Jakob Verbeek},
  year={2017},
  eprint={1606.02492},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/1606.02492}, 
}


@article{10.1145/3610536,
  author = {Karl, Florian and Pielok, Tobias and Moosbauer, Julia and Pfisterer, Florian and Coors, Stefan and Binder, Martin and Schneider, Lennart and Thomas, Janek and Richter, Jakob and Lang, Michel and Garrido-Merch\'{a}n, Eduardo C. and Branke, Juergen and Bischl, Bernd},
  title = {Multi-Objective Hyperparameter Optimization in Machine Learning—An Overview},
  year = {2023},
  issue_date = {December 2023},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {3},
  number = {4},
  url = {https://doi.org/10.1145/3610536},
  doi = {10.1145/3610536},
  abstract = {Hyperparameter optimization constitutes a large part of typical modern machine learning (ML) workflows. This arises from the fact that ML methods and corresponding preprocessing steps often only yield optimal performance when hyperparameters are properly tuned. But in many applications, we are not only interested in optimizing ML pipelines solely for predictive accuracy; additional metrics or constraints must be considered when determining an optimal configuration, resulting in a multi-objective optimization problem. This is often neglected in practice, due to a lack of knowledge and readily available software implementations for multi-objective hyperparameter optimization. In this work, we introduce the reader to the basics of multi-objective hyperparameter optimization and motivate its usefulness in applied ML. Furthermore, we provide an extensive survey of existing optimization strategies from the domains of evolutionary algorithms and Bayesian optimization. We illustrate the utility of multi-objective optimization in several specific ML applications, considering objectives such as operating conditions, prediction time, sparseness, fairness, interpretability, and robustness.},
  journal = {ACM Trans. Evol. Learn. Optim.},
  month = dec,
  articleno = {16},
  numpages = {50},
  keywords = {Multi-objective hyperparameter optimization, neural architecture search, Bayesian optimization}
}


@article{NSGA,
  author = {Srinivas, N. and Deb, Kalyanmoy},
  title = {Multiobjective optimization using nondominated sorting in genetic algorithms},
  year = {1994},
  issue_date = {Fall 1994},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  volume = {2},
  number = {3},
  issn = {1063-6560},
  url = {https://doi.org/10.1162/evco.1994.2.3.221},
  doi = {10.1162/evco.1994.2.3.221},
  abstract = {In trying to solve multiobjective optimization problems, many traditional methods scalarize the objective vector into a single objective. In those cases, the obtained solution is highly sensitive to the weight vector used in the scalarization process and demands that the user have knowledge about the underlying problem. Moreover, in solving multiobjective problems, designers may be interested in a set of Pareto-optimal points, instead of a single point. Since genetic algorithms (GAs) work with a population of points, it seems natural to use GAs in multiobjective optimization problems to capture a number of solutions simultaneously. Although a vector evaluated GA (VEGA) has been implemented by Schaffer and has been tried to solve a number of multiobjective problems, the algorithm seems to have bias toward some regions. In this paper, we investigate Goldberg's notion of nondominated sorting in GAs along with a niche and speciation method to find multiple Pareto-optimal points simultaneously. The proof-of-principle results obtained on three problems used by Schaffer and others suggest that the proposed method can be extended to higher dimensional and more difficult multiobjective problems. A number of suggestions for extension and application of the algorithm are also discussed.},
  journal = {Evol. Comput.},
  month = sep,
  pages = {221–248},
  numpages = {28},
  keywords = {Multiobjective optimization, nondominated sorting, phenotypic sharing, ranking selection}
}


@article{lexicographic_MOO,
  title = {A review of hydrogen production and supply chain modeling and optimization},
  journal = {International Journal of Hydrogen Energy},
  volume = {48},
  number = {37},
  pages = {13731-13755},
  year = {2023},
  issn = {0360-3199},
  doi = {https://doi.org/10.1016/j.ijhydene.2022.12.242},
  url = {https://www.sciencedirect.com/science/article/pii/S0360319922060505},
  author = {Jefferson A. Riera and Ricardo M. Lima and Omar M. Knio},
  abstract = {This paper reviews recent optimization models for hydrogen supply chains and production. Optimization is a central component of systematic methodologies to support hydrogen expansion. Hydrogen production is expected to evolve in the coming years to help replace fossil fuels; these high expectations arise from the potential to produce low-carbon hydrogen via electrolysis using electricity generated by renewable sources. However, hydrogen is currently mainly used in refinery and industrial operations; therefore, physical infrastructures for transmission, distribution, integration with other energy systems, and efficient hydrogen production processes are lacking. Given the potential of hydrogen, the greenfield state of infrastructures, and the variability of renewable sources, systematic methodologies are needed to reach competitive hydrogen prices, and design hydrogen supply chains. Future research topics are identified: 1) improved hydrogen demand projections, 2) integrated sector modeling, 3) improving temporal and spatial resolutions, 4) accounting for climate change, 5) new methods to address sophisticated models.}
}


@ARTICLE{ParEGO,
  author={Knowles, J.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={ParEGO: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems}, 
  year={2006},
  volume={10},
  number={1},
  pages={50-66},
  keywords={Approximation algorithms;Performance evaluation;Evolutionary computation;Testing;Pareto optimization;Search methods;Optimization methods;Instruments;Gaussian processes;Pareto analysis;Design and analysis of computer experiments (DACE);efficient global optimization (EGO);expensive black-box functions;Kriging;landscape approximation;metamodels;multiobjective optimization;nondominated sorting genetic algorithm II (NSGA-II);Pareto optima;performance assessment;response surfaces;test suites},
  doi={10.1109/TEVC.2005.851274}
}


@ARTICLE{EHI,
  author={Emmerich, M.T.M. and Giannakoglou, K.C. and Naujoks, B.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Single- and multiobjective evolutionary optimization assisted by Gaussian random field metamodels}, 
  year={2006},
  volume={10},
  number={4},
  pages={421-439},
  keywords={Uncertainty;Costs;Search methods;Evolutionary computation;Aerodynamics;Design optimization;Metamodeling;Computer science;Computational fluid dynamics;Artificial neural networks;Evolutionary optimization;Gaussian random field models;Kriging;metamodeling;multiobjective design optimization;uncertainty prediction},
  doi={10.1109/TEVC.2005.859463}
}


@ARTICLE{NSGA_II,
  author={Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A fast and elitist multiobjective genetic algorithm: NSGA-II}, 
  year={2002},
  volume={6},
  number={2},
  pages={182-197},
  keywords={Genetic algorithms;Sorting;Computational complexity;Evolutionary computation;Computational modeling;Testing;Decision making;Associate members;Diversity reception;Constraint optimization},
  doi={10.1109/4235.996017}
}


% [3 Experimental Results and Discussion]

@article{10.5555/1943267.1943271,
  author = {Bringmann, Karl and Friedrich, Tobias},
  title = {An efficient algorithm for computing hypervolume contributions**},
  year = {2010},
  issue_date = {Fall 2010},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  volume = {18},
  number = {3},
  issn = {1063-6560},
  abstract = {The hypervolume indicator serves as a sorting criterion in many recent multi-objective evolutionary algorithms (MOEAs). Typical algorithms remove the solution with the smallest loss with respect to the dominated hypervolume from the population. We present a new algorithm which determines for a population of size n with d objectives, a solution with minimal hypervolume contribution in time <inline-formula><inline-graphic xlink="EVCO_a_00012inline1.gif" mimetype="image" xlink:type="simple"/></inline-formula>(nd//2 log n) for d > 2. This improves all previously published algorithms by a factor of n for all d > 3 and by a factor of <inline-formula><inline-graphic xlink="EVCO_a_00012inline2.gif" mimetype="image" xlink:type="simple"/></inline-formula> for d == 3.We also analyze hypervolume indicator based optimization algorithms which remove λλ > 1 solutions from a population of size n == μµ ++ λλ. We show that there are populations such that the hypervolume contribution of iteratively chosen λλ solutions is much larger than the hypervolume contribution of an optimal set of λλ solutions. Selecting the optimal set of λλ solutions implies calculating <inline-formula><inline-graphic xlink="EVCO_a_00012inline3.gif" mimetype="image" xlink:type="simple"/></inline-formula> conventional hypervolume contributions, which is considered to be computationally too expensive. We present the first hypervolume algorithm which directly calculates the contribution of every set of λλ solutions. This gives an additive term of <inline-formula><inline-graphic xlink="EVCO_a_00012inline3.gif" mimetype="image" xlink:type="simple"/></inline-formula> in the runtime of the calculation instead of a multiplicative factor of <inline-formula><inline-graphic xlink="EVCO_a_00012inline3.gif" mimetype="image" xlink:type="simple"/></inline-formula>. More precisely, for a population of size n with d objectives, our algorithm can calculate a set of λλ solutions with minimal hypervolume contribution in time <inline-formula><inline-graphic xlink="EVCO_a_00012inline1.gif" mimetype="image" xlink:type="simple"/></inline-formula>(nd//2 log n ++ nλλ) for d > 2. This improves all previously published algorithms by a factor of nmin{λλ,d//2} for d > 3 and by a factor of n for d == 3.},
  journal = {Evol. Comput.},
  month = sep,
  pages = {383–402},
  numpages = {20}
}






% [Software used]

% OpenML, cf. https://www.openml.org/terms
% OpenML as a whole, cf. https://dl.acm.org/doi/10.1145/2641190.2641198
@article{OpenML,
  author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
  title = {OpenML: networked science in machine learning},
  year = {2014},
  issue_date = {December 2013},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {15},
  number = {2},
  issn = {1931-0145},
  url = {https://doi.org/10.1145/2641190.2641198},
  doi = {10.1145/2641190.2641198},
  abstract = {Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.},
  journal = {SIGKDD Explor. Newsl.},
  month = jun,
  pages = {49–60},
  numpages = {12}
}
% OpenML Python package, cf. https://github.com/openml/openml-python
@article{OpenMLPython,
  author  = {Matthias Feurer and Jan N. van Rijn and Arlind Kadra and Pieter Gijsbers and Neeratyoy Mallik and Sahithya Ravi and Andreas Müller and Joaquin Vanschoren and Frank Hutter},
  title   = {OpenML-Python: an extensible Python API for OpenML},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {100},
  pages   = {1--5},
  url     = {http://jmlr.org/papers/v22/19-920.html}
}


% NumPy, cf. https://numpy.org/citing-numpy/
@Article{         NumPy,
  title         = {Array programming with {NumPy}},
  author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                  van der Walt and Ralf Gommers and Pauli Virtanen and David
                  Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                  Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                  and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                  Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                  R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                  G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                  Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                  Travis E. Oliphant},
  year          = {2020},
  month         = sep,
  journal       = {Nature},
  volume        = {585},
  number        = {7825},
  pages         = {357--362},
  doi           = {10.1038/s41586-020-2649-2},
  publisher     = {Springer Science and Business Media {LLC}},
  url           = {https://doi.org/10.1038/s41586-020-2649-2} 
}


% pandas, cf. https://pandas.pydata.org/about/citing.html
% Sagemaker uses v1.5.3, cf. https://zenodo.org/records/7549438
@software{pandas1,
  author       = {The pandas development team},
  title        = {pandas-dev/pandas: Pandas},
  month        = jan,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v1.5.3},
  doi          = {10.5281/zenodo.7549438},
  url          = {https://doi.org/10.5281/zenodo.7549438},
}

@InProceedings{ pandas2,
  author    = { {W}es {M}c{K}inney },
  title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
  booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
  pages     = { 56 - 61 },
  year      = { 2010 },
  editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
  doi       = { 10.25080/Majora-92bf1922-00a }
}


% pytorch, cf. https://github.com/pytorch/pytorch/blob/main/CITATION.cff
@inproceedings{PyTorch,
  author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  title = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
  year = {2024},
  isbn = {9798400703850},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3620665.3640366},
  doi = {10.1145/3620665.3640366},
  abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages = {929–947},
  numpages = {19},
  location = {La Jolla, CA, USA},
  series = {ASPLOS '24}
}


% scikit-learn, cf. https://scikit-learn.org/stable/about.html
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}


% scipy, cf. https://scipy.org/citing-scipy/
@ARTICLE{SciPy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}


% pymoo, cf. https://pymoo.org/references.html
@ARTICLE{pymoo,
  author={J. {Blank} and K. {Deb}},
  journal={IEEE Access},
  title={pymoo: Multi-Objective Optimization in Python},
  year={2020},
  volume={8},
  number={},
  pages={89497-89509},
}


% tqdm, cf. https://github.com/tqdm/tqdm + https://zenodo.org/records/14231923
@software{tqdm,
  author       = {Casper da Costa-Luis and
                  Stephen Karl Larroque and
                  Kyle Altendorf and
                  Hadrien Mary and
                  richardsheridan and
                  Mikhail Korobov and
                  Noam Yorav-Raphael and
                  Ivan Ivanov and
                  Marcel Bargull and
                  Nishant Rodrigues and
                  Shawn and
                  Mikhail Dektyarev and
                  Michał Górny and
                  mjstevens777 and
                  Matthew D. Pagel and
                  Martin Zugnoni and
                  JC and
                  CrazyPython and
                  Charles Newey and
                  Antony Lee and
                  pgajdos and
                  Todd and
                  Staffan Malmgren and
                  redbug312 and
                  Orivej Desh and
                  Nikolay Nechaev and
                  Mike Boyle and
                  Max Nordlund and
                  MapleCCC and
                  Jack McCracken},
  title        = {tqdm: A fast, Extensible Progress Bar for Python
                   and CLI
                  },
  month        = nov,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v4.67.1},
  doi          = {10.5281/zenodo.14231923},
  url          = {https://doi.org/10.5281/zenodo.14231923},
  swhid        = {swh:1:dir:4556fa8b9e2bffacf634b5378660ecb3db68431e
                   ;origin=https://doi.org/10.5281/zenodo.595120;visi
                   t=swh:1:snp:3b7ead939423abc779ad7715fdd3e6f2a93e7e
                   71;anchor=swh:1:rel:1900ab2ad612c3d46028815e6bb1fd
                   cd0caf6166;path=/
                  },
}