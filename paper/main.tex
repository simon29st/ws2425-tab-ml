\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\tabmlheading{WS 2024/25}{1-\pageref{LastPage}}{15.03.2025}{}{Simon Stürzebecher}

% Short headings should be running head and authors last names

\ShortHeadings{Extending EAGGA to Neural Networks}{Simon Stürzebecher}
\firstpageno{1}

\begin{document}

\title{Extending EAGGA to Neural Networks}

\author{\name Simon Stürzebecher \email simon.stuerzebecher@campus.lmu.de}

%\editor{My editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
%\blindtext
\end{abstract}

\begin{keywords}
  tabular data, multi-objective optimization, deep learning
\end{keywords}

\section{Introduction}

%\blindmathpaper


\section{Background and Related Works}

\subsection{Tabular Data}
- most common type of data
- bad performance of deep learning models on tabular data

% feels better in front of AutoML, as it feels more qualitative, less quantitative
\subsection{Explainable Artificial Intelligence and Interpretability}
\subsubsection{Why is interpretability desirable?}
\subsubsection{Practical implementations}
(1) post-hoc methods
  - give examples
  - why are they not perfectly suited
(2) model-based

\subsection{Automated Machine Learning (AutoML)}
\subsubsection{Hyperparameter Optimization (HPO)}
- grid + random search
- Bayesian optimisation
  * short intro
  * why is it unsuitable for our task? (makes no sense to have it here, unless we introduce our problem (feature selection is high-dim binary space) in the introduction)
- evolutionary algorithm
  * CMA-ES
  * NSGA-II
    - non-dominated sorting
    - crowding distance
\subsubsection{Neural Architecture Search (NAS)}

\subsection{Multi-Objective Optimization}
- intro


%likely shouldn't be an entire section
\section{EAGGA}
- NSGA-II inspired evolutionary / genetic algorithm
- introduces group structure for more efficient optimization over the entire search space


\section{Extension to neural networks}
- why extend it to NNs?
  * NNs notoriously uninterpretable due to complex transformation of the feature space
  * due to (lin alg) non-linear activation functions, interactions can be modelled
  * no monotonicity guarantees
- our approach
  * feature sparsity: straightforward, only train model on selected features, goes somewhat against idea of deep learning,
    where the model is supposed to decide on its own, which features to put importance on -> WHY?
  * feature interaction
    - create non-fully connected network by introducing network-groups based on the equivalence relations introduced in the EAGGA paper
    - show formula why the interacting features need to be grouped together when using ReLU (cf. photos)
  * monotonicity constraints
    - for groups with contraint on monotonicity: clip weights to [0, infty), bias clipping not necessary
    - show formula of how monotonicity is enforced with this
  * changes (-) + similarities (+) to EAGGA paper (~ in-between)
    + 2/3 holdout outer split for 'CV'- vs test-set
    - train + eval each individual via 5-fold CV with 20\% of train set again reserved to determine early stopping -> best to visualise,
      * early stopping necessary (as opposed to XGB) as NN can be trained forever, whereas tree implementations used by EAGGA paper have
        max depth, etc.
      * early stopping using min-epochs 200, patience 100, i.e. train for $\ge$ 200 epochs, always safe model with lowest loss, whenever
        current loss > mean(losses of last 100 (patience) epochs), stop early + return model with lowest loss
    ~ if no early stopping, train each fold (in 5-fold CV) for max 120 secs, do entire EAGGA loop per dataset for max 8 hrs
    ~ training on Sagemaker Notebook instance
      * initial experiments on ml.g4dn.xlarge instance (2 vCPUs, 16GiB RAM, 1 NVIDIA T4, cf. https://aws.amazon.com/de/ec2/instance-types/g4/)
      using cuda not much faster than on ml.t3.medium (2 Intel Xeon 8000 vCPUs, 4GiB RAM, cf. https://aws.amazon.com/de/ec2/instance-types/t3/)
      * thus decided for more economic + ressourcenschonend t3.medium
    - evaluation
      * AUC + NF: straightforward
      * NI: for each group with $\ge$ 2 features: sum from 1 to (\# features - 1), i.e ${\# features \choose 2}$,
        then sum over all groups like this, then divide by p(p-1)/2, i.e. ${p \choose 2}$
      * NNM: \# unconstrained features / \# all features
    ~ feature + interaction detector: examination of decision trees not super straightforward in sklearn, hence simply use p=0.5
    ? interaction detector (FAST)
      * train logistic model on 80\% of data (only holdout train portion or all ??), evaluate on 20
      * use mean accuracy (logistic metric) instead of RSS (LM metric)
    + monotonicity detector as in paper, also use dec tree HPs from mlr3
    ~ encoding of group structures similar, each feature will get its individual monotonicity sign from the detector, this is then fed to the dataset,
      which then outputs the data according multiplied with the sign



\section{Experimental Results and Discussion}


\section{Conclusion and Future Outlook}

Here is a citation \cite{EAGGA}.

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section{}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\section{}

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{references}

\end{document}
