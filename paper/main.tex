\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}

\usepackage{mathtools}
\usepackage{amsmath}  % use argmin + argmax, cf. https://tex.stackexchange.com/a/5255
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{bm}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\tabmlheading{WS 2024/25}{1-\pageref{LastPage}}{15.03.2025}{}{Simon Stürzebecher}

% Short headings should be running head and authors last names

\ShortHeadings{Interpretable Neural Networks using EAGGA}{Simon Stürzebecher}
\firstpageno{1}

\begin{document}

\title{Interpretable Neural Networks using EAGGA}

\author{\name Simon Stürzebecher \email simon.stuerzebecher@campus.lmu.de}

%\editor{My editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
%\blindtext
\end{abstract}

\begin{keywords}
  tabular data, multi-objective optimization, interpretability, deep learning
\end{keywords}

\section{Introduction}
Tabular Data
- most common type of data
- bad performance of deep learning models on tabular data, recent research suggests that heavy regularisation is necessary

regularisation
- LM: ridge (L2) + lasso (L1)
  * ridge: overcome multi-collinearity by pulling coeffs close to 0
  * lasso: variable selection by setting coeffs = 0
- NN:
  * L2 regularisation implicitly included in SGD when using weight decay (pull params towards 0) -> does this combat multi-collinearity as well or serve another purpose?
  * L1
    + key claim of DL: feature selection obsolete, network finds important ones itself
    + still, feature usually always has some impact (back this up), no explicit feature selection
    + explicit feature selection simply by not inluding a feature
  * another classical NN regularisation technique: dropout against co-adaptation
  * early stopping against overfitting when using iterative method
-> all of those require specifying Hyperparameters in advance

%\blindmathpaper


\section{Background and Related Works}

% feels better in front of AutoML, as it feels more qualitative, less quantitative
\subsection{Interpretability}
As there is no clear definition for interpretability, we will consider it as ``the ability to provide \textit{explanations} in \textit{understandable terms} to a human'',
where \textit{explanations} are logical decision rules and \textit{understandable terms} relate to commonly used terms in the domain of the problem,
as suggested by \citet[chap. 1]{survey_NN_interpretability}.
Further, we use the term ``explanability'' in an exchangeable manner with ``interpretability'', as is commonly done.

(Why Interpretability is desirable?)
- in many ways desirable, as per \citet[pp. 3-4]{Zach2019InterpretabilityOD}, e.g.
  * \textbf{gaining trust}: most notable use case, e.g. medical diagnosis: predictions need to be validated by doctors \citep[p. 1]{review_NN_interpretability}
  * \textbf{model debugging}
    - model is optimised w.r.t. loss and judged on accuracy
    - might have unexpected performance drops in certain situations, makes model unreliable \citep[1B]{survey_NN_interpretability}
    - having an interpretable model + model induction process can help find ways to improve accuracy + reliability
  * \textbf{scientific understanding}: when only models can make sense of increasingly complex data anymore, interpretability enables extraction of learnt knowledge encoded
    in the model to make it accessible + reliable to humans (also mentioned here \citep[p. 1]{review_NN_interpretability})
  * \textbf{subconcious bias}, e.g. loan approval: must ensure that decision is not discriminatory
  * \textbf{regulatory}
    - such as EU's "Right to Explanation" warranted by the GDPR \citep[p. 1]{review_NN_interpretability}, \citep[1B]{survey_NN_interpretability} or
    - drug approval processes in situations where a machine learning model discovered a new drug, process needs to be transparent for the regulator to approve \citep[1B]{survey_NN_interpretability}

Commonly, methods for model interpretation are divided into intrinsic methods, where the search space only comprises models with a structure simple enough to be
considered ``explainable'' (such as tree-based or simple linear models), and post-hoc methods, where interpretation methods are applied after model training.
Amongst post-hoc techniques, we can further divide the space into model-specific (such as analysing GLM coefficients) and model-agnostic
(e.g. partial depence plots, ALE) methods. \citet[chap. 3.2]{molnar2022}

(Taxonomy)
\citet[chap. 2]{survey_NN_interpretability} extend this distinction to a 3-dimensional taxonomy, allowing for better categorisation of neural networks (NN), a model class
that in its fully-connected feedforwad form is inherently non-interpretable.
% alternative
%As fully-connected feedforward neural networks (NNs) are inherently non-interpretable models, the taxonomy introduced by \citet[chap. 2]{survey_NN_interpretability} proves
%particulary useful to categorise NN explanability methods along 3 dimensions:
- \textbf{Passive vs Active Approaches}
  * passive: post-hoc
  * active: changing architecture or training process to make model interpretable
- \textbf{Type of Explanations}
  * examples: providing examples of what leads to desired output
  * attribution: attributing effect on output for a specific feature
  * hidden semantics: examine what type of inputs specific neurons / layers pick up on
  * rules: logical rules, e.g. if-then, trees
- \textbf{Local vs Global Interpretability}
  * local: explanation based on individual samples
  * semi-local: explanation based on set of samples, e.g. grouped together by some criterion
  * global: explaining network as a whole

(Evaluation)
- objective evaluation difficult, as no clear definition of interpretability
- \citet[3]{DoshiVelez2017TowardsAR} propose a taxonomy to categorise possible evaluation methods
  * \textbf{application-grounded evaluation}
    - most rigorous method, also most expensive + time consuming
    - idea: evaluate ``interpretability model'' directly w.r.t. the task -> human experts evaluate the outcome
    - e.g.: model performing medical diagnosis on patients would be evaluated by based on doctors doing the same
  * \textbf{human-grounded metrics}
    - similar to application-grounded, but tries to simplify task (while preserving its essence) so that laypersons can do it
    - cheaper due to larger, less qualified subject pool
    - especially suitable if only general concepts of the tasks need to be validated
    - examples for evaluation set-up
      * binary forced choice: human evaluator chooses which of two explanations he finds better
      * forward simulation: evaluator must correctly simulate model output when presented model input + explanation % perhaps binary forced choice is already enough to include, this (and the third one is a bit awkward to explain why this is good for interpretability imo)
  * \textbf{functionally-grounded evaluation}
    - assess explanation quality via some formally defined proxy for interpretability
    - no human time nor cost required beyond initial formulation of proxy
    - e.g. if we already have an interpretable model class (e.g. identified via human-grounded evaluation), can then rank different models of that class based on proxy
    - main challenge: what proxies

\subsection{Hyperparameter Optimization (HPO)}
- in contrast to model parameters, which are optimised during training, hyperparameters are parameters describing the ML algo + are specified before training
- still often have great impact on final (trained) model performance -> can be optimised too
- formal definition
  * dataset $\mathcal{D}\subseteq\mathcal{X}\times\mathcal{Y}$ consisting of $n$ tuples $(\boldsymbol{x}^{(i)}, y^{(i)})\stackrel{i.i.d.}{\sim}\mathbb{P}_{\boldsymbol{x}y}$
    (data-generating distribution), i.e. $\mathcal{D}=\{(\boldsymbol{x}^{(i)}, y^{(i)})\}_{i=1}^n$
  * ML algo $\mathcal{I}(\cdot,\boldsymbol\lambda)$ with HP config $\boldsymbol\lambda\in\Lambda$ maps dataset to fitted model
    $f:\mathcal{X}\rightarrow\mathbb{R}^g\subseteq\mathcal{Y}$ (where $g\in\mathbb{N}$ is \# of classes of target),
    i.e. $\mathcal{I}:(\mathbb{D}\times\Lambda)\rightarrow\mathcal{H}$, with $\mathbb{D}$ space of all finite datasets and $\mathcal{H}$ hypothesis space (space of all models $f$)
  * denote $\mathcal{I}_{\boldsymbol\lambda}$ as (untrained) model with fixed HP config $\boldsymbol\lambda$
  * loss function $L$
  * goal of model training: for fixed $\boldsymbol\lambda$, have $\mathcal{I}$ find model $f$ minimising expected generalisation error
    $GE(\mathcal{I},\boldsymbol\lambda,\mathcal{D},L)=\mathbb{E}_{(\boldsymbol{x},y)\sim\mathbb{P}_{\boldsymbol{x}y}}[L(y,\mathcal{I}_{\boldsymbol\lambda}(\mathcal{D})(\boldsymbol{x}))]$
  * goal of HP optim: find $\boldsymbol\lambda$ minimising expected generalisation error, i.e. $\argmin_{\boldsymbol\lambda\in\Lambda} GE(\mathcal{I},\boldsymbol\lambda,\mathcal{D},L)$
- \citep[p. 3]{10.1145/3610536}
- HPO = black-box optim problem -> we can apply any black box algo to tune

[model free]
(grid + random search)
- most basic strategy: grid search, for all HPs define range of interest, then evaluate cartesian product of those -> scales poorly in \# of HP dimensions + \# of query points per HP range
- random search
  * randomly sample value for each HP until it runs out of budget
  * explorative: for budget $B$, each HP will (likely) be queried with $B$ different values, whereas grid search only $B^{1/N}$ for $N$ HPs
  * thus better than grid in case of some HPs being non-informative (almost always the case), as it doesn't waste time specifically exploring these (as does grid)
  * usually useful as baseline, as no assumptions about model, easy to parallelise, high exploration, and in expectation will ``achieve performance arbitrarily close to the optimum''

(evolutionary algorithms)
- another model-free class of optimisation algorithms, inspired by naturally occuring evolution
- based on a population, which iteratively (iteration = generation) generates $\lambda$ offspring via 3 operators (all sub-bullets from \citet[pp. 10-]{genetic_algos})
  * \textbf{reproduction}: pick an individual to reproduce with p proportional to its fitness
  * \textbf{crossover}
    - select 2 ``parents'' from pool of reproducing individuals
    - for a position $k$ in their HP configs, with certain p, swap their values after $k$, yielding 2 children
    %use example from book page 12
  * \textbf{mutation}: with certain p, change values of an individual, e.g. by adding Gaussian noise (Gaussian mutation) to real values, or flipping bit for binary values
- selection at end of each genertion: keep $\mu$ best individuals (based on some fitness function), either from only offspring (``$(\mu,\lambda)$-selection'')
  or (more commonly) from population + offspring (``$(\mu+\lambda)$-selection'', guaranteed to keep best individual)
- pro: conceptually simple + can handle even complex parameter spaces (continuous, discrete, hierarchical, etc.) given appropriate implementation of operators
all above starting at HPO from \citep[chap. 1.3]{feurer_hyperparameter_2019}
- prominent examples
  * CMA-ES
    - ``Covariance Matrix Adapation Evolution Strategy''
    - offspring generation exclusively via multivariate normal \citep[p. 8]{hansen2023cmaevolutionstrategytutorial}
      * mean = weighted average of previous generation
      * covariance = weighted covariance of previous generation, with weighting as for mean
    - weighing scheme done to sample in a way as to reproduce previously successful (i.e. selected) steps \citep[p. 11]{hansen2023cmaevolutionstrategytutorial}
  * differential evolution \citep[-]{differential_evolution}
    - init population randomly so that entire param space is covered
      * $D$-dim vectors $\boldsymbol{x}_{i,G}$ with $i=1,2,...,n$ for gen $G$
    - mutation
      * for each $\boldsymbol{x}_{i,G}$, generate \textbf{mutant vector} $\boldsymbol{v}_{i,G+1}=\boldsymbol{x}_{r_1,G}+F\cdot(\boldsymbol{x}_{r_2,G}-\boldsymbol{x}_{r_3,G})$
      * $r_1,r_2,r_3\in\{1,2,...,n\}$ mutually different random idx + different from $i$
      * constant $F\in[0,2]$
    - crossover
      * \textbf{trial vector} $\boldsymbol{u}_{i,G+1}=(u_{i,G+1}^{(1)},u_{i,G+1}^{(2)},...,u_{i,G+1}^{(D)})$
        - chose random index $R\in\{1,2,...,D\}$
        - sample $p\sim U[0,1]$
        - define crossover constant $\text{CR}\in[0,1]$
        - $u_{i,G+1} = \begin{dcases}
          v_{i,G+1}^{(j)} & \text{if } p\le\text{CR}\text{ or } j=R \\
          x_{i,G}^{(j)} & \text{else}
         \end{dcases}$
    - selection: greedy, compare fitness of $\boldsymbol{u}_{i,G+1}$ with $\boldsymbol{x}_{i,G}$, pick better

[model based]
- contrast to model free, fit a surrogate model on blackbox problem + optimise this
(Bayesian optimisation)
- iterative algo comprising of
  * surrogate model for black box problem, needs to be able to model mean + variance
  * acquisition function, to decide which point to query next
- in each iteration
  * fit surrogate model on all data points (posterior distribution)
  * get highest utility point from acquisition function for newly fitted surrogate model -> add to data points
  -> hence Bayesian, get new query point given already previously fitted points
  * acquisition function trades off exploration + exploitation of surrogate
  * BO up to here from \citep[chap. 1.3.2]{feurer_hyperparameter_2019} + \citep[pp. 2-3]{frazier2018tutorialbayesianoptimization}
- no optimising of model directly, instead iteratively optimise acquisition function
- common choices for surrogate models
  * \textbf{gaussian processes}
    - pros
      * fully specified by mean + covariance
        - covariance function (aka kernel) solely determines quality of GP
        - kernel as function of two points from search space, yields their covariance
        % TODO: write down kernel equation, cf. p 10ff https://ieeexplore.ieee.org/document/7352306
        - usual property of kernels: the closer points are in search space, the more strongly their correlation \citep[p. 5]{frazier2018tutorialbayesianoptimization}
      * well-calibrated uncertainty estimates
      * closed-form computability
    - con: neither scales well in \# of data points nor in \# of HP dimensions BUT workaround: sparse GPs, approx full GP with small subset of original dataset
  * \textbf{random forests}
    - can handle complex search spaces (high-dim, categorical, hierarchical), unlike GPs
    - computational complexity scales far better than GPs
      * GPs $O(n^3)$ fitting, $O(n^2)$ predicting
      * RFs $O(n \log n)$ fitting, $O(\log n)$ predicting
- common choices for acquisition functions
  % TODO: cf. p. 13 cf. p 10ff https://ieeexplore.ieee.org/document/7352306
  * \textbf{expected improvement}
  * \textbf{Thompson sampling}
- all BO stuff if not specified differently comes from \citep[chap. 1.3.2]{feurer_hyperparameter_2019}

  
\subsection{Neural Architecture Search (NAS)}
- approaches specifically for NN HPO due to flexible structure of NNs and thus very large search space (each layer can have different \# nodes, different activations, for CNN different pooling, etc. operations)
- we don't employ NAS for our extension as research focusses on the NLP and image domains, % TODO: find some more references
  whose datasets exhibit strong correlation amongst features (i.e. tokens or pixels, respectively), an effect that is much weaker for tabular data \citep[p. 1]{Borisov_2024}
- at least want to give a little overview over most notable approaches
  * \textbf{cell search space}
    - modularise NN architecture into cells, NN as chain-structure of those
    - cell = basic building block, fixed component of an NN
      * e.g. linear layer with specific \# of neurons in feedforward
      * or convolutional / pooling / etc layer for CNN
    - then optimise sequential placement of the cells as HPO problem
      * e.g. via random search or BO
      * \citet[p. 3]{zoph2017neuralarchitecturesearchreinforcement} + \citet[pp. 2-4]{Zoph_2018_CVPR} use RNN to recursively optimise HPs of a cell given already determined
        previous cells (+ HPs of current cell), which is trained via RL with the different HPs the RNN predicts being the actions and performance on held-out data being reward
    - \citep[chap. 3.2]{elsken_neural_2019}
  * \textbf{one-shot model}
    - trains one overall ``fabric'' comprising all architectures of search space
    - visualisation as DAG, nodes are nodes, edges in-between are operations on a node
    - each path through DAG (from input to output node) represents one architecture of search space -> train entire DAG, then pick optimal path (architecture)
    - pro: very efficient training, individual architectures share operations along edges they share, training one-shot model trains all subsumed models (more expensive
      than training a single model but less expensive than trying out all configurations included in the fabric)
    - \citep[pp. 1-2, p.8]{saxena2017convolutionalneuralfabrics}
    % don't necessarily talk about co-adaptation problem

\subsection{Multi-Objective Optimization}
- in most practical use cases one doesn't just want to optimise for performance but e.g. also for interpretability using some proxy metrics
- formal definition
  * vector of objectives $c_1,c_2,...,c_m=\boldsymbol{c}:\Lambda\rightarrow\mathbb{R}^m$
  * goal: minimise vector
  * \citep[p. 11]{10.1145/3610536}
\subsubsection{Pareto-optimality}
- problem: usually conflicting objectives, i.e. minimisation of $\boldsymbol{c}$ not possible along all dimensions
- thus aim to find trade-off solutions of non-dominated points
- we say a point dominates another
  * if there is no other point strictly better in at least one dimension and better or equal in the remaining ones
  * formally: $\boldsymbol\lambda$ dominates $\boldsymbol\lambda'$ ($\boldsymbol\lambda\prec\boldsymbol\lambda'$) if and only if
    $\forall i\in\{1,...,m\}:c_i(\boldsymbol\lambda) \le c_i(\boldsymbol\lambda') \wedge \exists j\in\{1,...,m\}:c_j(\boldsymbol\lambda) < c_j(\boldsymbol\lambda')$
  * \citep[pp. 7f]{10.1145/3610536} + \citep[pp. 198f]{genetic_algos}
- Pareto set: set of nondominated points $\mathcal{P}:=\{\boldsymbol\lambda\in\Lambda|\nexists\boldsymbol\lambda'\in\Lambda\text{ s.t. }\boldsymbol\lambda'\prec\boldsymbol\lambda\}$
- Pareto front: image of nondominated points
- goal: find set of nondominated points $\hat{\mathcal{P}}$ approximating true Pareto set $\mathcal{P}$ well
\subsubsection{A-priori}
- requires specifying trade-off between objectives a-priori, will outline 2 popular approaches
- e.g. different versions of \textbf{scalarization}
  * weighted sum of objective functions
    - $\argmin_{\boldsymbol\lambda\in\Lambda} w_i c_i(\boldsymbol\lambda)$ with $\sum_{i=1}^k w_i=1$ and $w_i>0,\forall i=1,...,k$
    - drawbacks
      * solution sensitive to weights
      * different users might have different opinions on weights \citep[chap. 3.1]{NSGA}
    - \citep[p. 11]{10.1145/3610536}
  * $\epsilon$-constraint
    - translate all but one objective into constraint, then optimise remaining objective under subject to the constraints
    - $\text{w.l.o.g.} \argmin_{\boldsymbol\lambda\in\Lambda} c_1(\boldsymbol\lambda) \text{ s.t. } c_2(\boldsymbol\lambda)\le\epsilon_2,...,c_m(\boldsymbol\lambda)\le\epsilon_m$
    - conceputally similar to weighted sum: also sensitive to constraints, must be chosen sensibly
    - \citep[p. 12]{10.1145/3610536}
- \textbf{lexicographic method}
  * define priorisation of objectives
  * greedily optimise objectives in order of priority, constraint to the solutions of the already optimised (higher-priority) objectives
  * again very dependent on user-defined priorisation
  * \citep[p. 13749]{lexicographic_MOO}  % TODO: perhaps find a different source more related to ML
\subsubsection{A-posteriori}
- problem a priori
  * either restrict search space to ``enforce our will'' (e.g. only use 50\% of features), optimise only prediction performance + use this
  * or leave search space unrestricted but adjust loss function to incorporate multiple objectives + take optimum from there
  * in either case: no knowledge of interplay between HP config + performance on all objectives
  * in practical applications, it is oftentimes useful to make the decision of which point of the Pareto set to use a-posteriori, e.g. if only a slight decrease in
    one objective translates to a significant improvement in another that would have been missed if the problem was optimised using a-priori methods
  * a-posteriori evaluates configs just as a-priori, but keeps track of multiple ``best'' (non-dominated) solutions -> makes relationship between HP config + objectives visible
- aside from usualy baselines grid and random seach there are multi-objective BO adaptations, mainly using either of two approaches
  (1) fit single surrogate model on scalarised objectives, e.g. ParEGO, which employs the augemented Tchebycheff function as scalarisation to ensure the Pareto front is
      explored sufficiently \citep[pp. 15f]{10.1145/3610536} + \citep[pp. 54-56]{ParEGO}
  (2) fit one surrogate per objective, then
    - either one acquisition function per objective -> return set of promising next candidates
    - or one overall acquisition function aggregating surrogates, e.g. EHI, maximises expected improvement of hypervolume \citep[p. 16]{10.1145/3610536} + \citep[pp. 8f]{EHI}
- lastly, there is also multi-objective EA (MOEA) algorithms to explore Pareto front, one of which is NSGA-II (Nondominated Sorting Genetic Algorithm)
  * improves upon predecessor NSGA \citep{NSGA} by making it parameterless, ensuring elitism, and reducing computational complexity of ranking of individuals \citep[p. 182]{NSGA_II}
  * uses all the regular operators, i.e. reproduction, mutation, crossover can be used from single-objective EA (SOEA)
  * difference to SOEA: ranking of individuals (SOEA uses scalar fitness, MOEA mutliple objectives)
    - ranking mechanism based on 2 parts
      * non-dominated sorting, ensures elitism
        - iterative procedure to rank individuals in population by their fronts
        (1) determine pareto front, assign rank 0
        (2) remove pareto front from population
        (3) if individuals left, repeat from 1, increment rank \#
        - \citep[p. 201]{genetic_algos} + \citep[pp. 183f]{NSGA_II}
      * crowding distance, ensures sufficient diversity, i.e. exploration of pareto front
        - assigns score depending on how crowded area around individual in objective space is
        - crowding distance of an individual = mean side length of cuboid spanned by its nearest neighbours as vertices in objective space
        - individuals without two neighbour in any dimension are assigned infinitely high distance value
        - \citep[p. 185]{NSGA_II}
        -> the less the crowding distance, the more ``crowded'' an area is by other individuals
      -> rank individuals by nds front rank (ascending) + crowding distance (descending) as tie breaker
  * ranking used for selecting $\mu$ best individuals to keep for next generation + for reproduction: via binary tournament solution: select two random individuals,
    pick best w.r.t. nds + cd for offspring creation (mutation / crossover)
  * using nds for ranking / fitness makes intuitive sense, but why cd?
    - goal of MOO: not simply optimise HV, but approx true Pareto front well
    - only having individuals representing one area in objective space makes Pareto front estimate very ``unstable'': removing this area (i.e. solutions form that area)
      would ``collapse'' entire front
    - instead having multiple areas be represented makes the estimate ``stable'': removing one area wouldn't impact Pareto front estimate a lot
    % TODO: reference?


%likely shouldn't be an entire section
\section{EAGGA}
- combines all previously mentioned topics: interpretability, HPO, MOO
- for our problem (interpretability) we cannot use regular EA algorithms as we are also interested in reducing pairwise interactions + non-monotone feature effects
- this creates high-dim search space as shown in EAGGA paper -> they provide a framework to efficiently traverse through a space that is reduced to only sensible HP
  configuration (e.g. interaction of A,B with A monot incr and B monot decr -> couldn't be guaranteed -> don't look at it in first place)
- NSGA-II inspired evolutionary / genetic algorithm
- introduces group structure for more efficient optimization over the entire search space


\section{Extension to neural networks}
- why extend it to NNs?
  * NNs notoriously uninterpretable due to complex transformation of the feature space
  * due to (lin alg) non-linear activation functions, interactions can be modelled
  * no monotonicity guarantees
- our approach
  (general algorithm)
  * EAGGA algorithm implemented just as described in the paper
  * HP init
    - NN total layers $\in\{3, ..., 10\}$, i.e. hidden layers $\in\{1, ..., 8\}$, init from trunc geom with p=0.5
    - NN nodes per hidden layer (for each group) $\in\{3, ..., 20\}$, init from trunc geom with p=0.5
    - NN dropout \% $\in[0, 1]$, init from trunc gamma with shape=2, scale=0.15
  * group structure init
    - feature detector
      * as described in paper
      * but instead of fitting 10 trees + taking rel. \# of features used as p for trunc geom, we simply use 0.5 (sklearn dectree examination not straightforward)
      * also in preliminary experiments we found the sampled \# of features to be used occassionally to be > \# of non-0 values in normalised information gain filter
        (e.g. if former is = total \# of features and one feature is indep. from target) in these edge cases we only use the features with non-0 filter values
    - interaction detector
      * as described in paper
      * same as for feat detect: use p=0.5 to sample \# of interactions used instead of 10 dec trees
      * also for FAST algorithm don't use RSS (lin reg metric) but mean accuracy, as we fit log reg
    - monotonicity detector
      * as described in paper
      * use default HPs (max depth 30, minsplie = 20) of mlr3 classification tree implementation, as this is the library the paper uses
    -> both interaction + monotonicity detectors fit their models on 80\% of train split from holdout + eval on remaining 20\%
  (implementation details)
  * group structures + datasets
    - group structures implemented as described in the paper with additional list encoding sign of monotonicity of the individual features as detected by the
      monotonicity detector
    - included features are passed to dataset, dataset outputs included features, multiplied by the features' individual monotonicity (-1 / 1)
    - entire group structure is passed to NN, where architecture is built accordingly
  * neural network
    - "instead of XGB as in the original paper, we apply the method to NNs to examine whether this type of regularisation can achieve interpretability on NNs while
      outperforming EAGGA on XGB (original paper), ..., this requires special architecture, etc. pp."
    - NN
      * comprises of "sub-NNs", one for each equivalence relation -> basically non-fully connected NN
      * hidden layers use ReLU activation and dropout afterwards, then a shared output layer with concatenated sub-NNs' outputs as input and sigmoid activation
        (implicit in the loss function for better numerical stability, NN itself outputs "logits", which refers to pre-activation output in pytorch)
    - feature sparsity thus achieved by only training on included feature groups -> goes somewhat against of deep learning where model is supposed to decide itself,
      which feature to "use" / put importance on -> ELABORATE
    - feature interaction achieved by grouping, max-operation in ReLU induces interaction effect (different kind of interaction than e.g. in LM with multiplication)
      -> equation why the interacting features need to be grouped together when using ReLU, cf. photos
    - monotonicty constraint achieved by clipping weights to [0, infty) for restricted equivalence relations (monotonic decrease achieved via dataset object multiplying
      features with their individual signs), bias clipping not necessary (constant additive term)
      -> equation how monotonicity is enforced with this
  * evaluation, holdout, cv, early stopping
    - evaluation via dominated hypervolume along AUC-ROC, NF, NI, NNM as defined in original paper
      * NF simply rel. \# of included features
      * NI = sum of all possible pairwise interactions in each group over all possible pairwise interactions among all features = $\frac{\sum_g^G {p_g \choose 2}}{{p \choose 2}}$
      * NNM = rel. \# unconstrained features
    - outer holdout split: 2/3 train, 1/3 test (as in paper)
      * run EAGGA on holdout train split, i.e. train + select best $\mu=100$ individuals based on non-dom-sorting (ascending) + crowding distance (descending) as tie breaker
    - inner CV split on outer train portion: 5-fold (as in paper)
      * in each fold fit model on 80\% of CV-train portion, use remaining 20\% of CV-train for early stopping
    - early stopping criterion
      * for each fold, always train for min 200 epochs, keep track of model with lowest loss
      * after that, use patience of 100: if current model's loss is > mean of last 100 epochs' losses
      * if no early stopping, train each fold for max 10 minutes
      * then stop training, return model with lowest loss
    -> add graphic of entire dataset split
    - then evaluate on last fold from CV + keep best $\mu$ individuals + generate $\lambda=10$ offspring for new generation + repeat
  * hardware: training on Sagemaker Notebook instance
    - initial experiments on ml.g4dn.xlarge instance (2 vCPUs, 16GiB RAM, 1 NVIDIA T4, cf. https://aws.amazon.com/de/ec2/instance-types/g4/)
    using cuda not much faster than on ml.t3.medium (2 Intel Xeon 8000 vCPUs, 4GiB RAM, cf. https://aws.amazon.com/de/ec2/instance-types/t3/)
    - thus decided for more economic + ressourcen-schonend t3.medium
  * known bugs
    - in rare cases (anecdotally once every ~5-10 datasets), gga\_mutate seems to be generating -1 as monotonicity attribute, despite np.random.randint(low=0, high=2, size=1)
      * loop crashes at group\_structure creation, for these cases subtract previous runtime from 8hrs + load from last generation in output via load\_population
      * so far only happened for madeline in after gen-8.json after 5hrs 45mins (at start of gen-9, which wasn't exported yet) -> loaded this and ran for another 2hrs 15mins


\section{Experimental Results and Discussion}


\section{Conclusion and Future Outlook}
- MO BO on group structure space possible?
- here is a citation \cite{EAGGA}.

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section{Software used}
for implementation we used
openml \cite{OpenML}, \cite{OpenMLPython},
numpy \cite{numpy},
pandas \cite{pandas1}, \cite{pandas2},
pytorch \cite{PyTorch},
scikit-learn \cite{scikit-learn},
scipy \cite{SciPy},
pymoo \cite{pymoo}, and
tqdm \cite{tqdm}

\section{}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\section{}

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{references}

\end{document}
