\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}

\usepackage{mathtools}
\usepackage{bm}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\tabmlheading{WS 2024/25}{1-\pageref{LastPage}}{15.03.2025}{}{Simon Stürzebecher}

% Short headings should be running head and authors last names

\ShortHeadings{Interpretable Neural Networks using EAGGA}{Simon Stürzebecher}
\firstpageno{1}

\begin{document}

\title{Interpretable Neural Networks using EAGGA}

\author{\name Simon Stürzebecher \email simon.stuerzebecher@campus.lmu.de}

%\editor{My editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
%\blindtext
\end{abstract}

\begin{keywords}
  tabular data, multi-objective optimization, deep learning
\end{keywords}

\section{Introduction}
Tabular Data
- most common type of data
- bad performance of deep learning models on tabular data, recent research suggests that heavy regularisation is necessary

regularisation
- LM: ridge (L2) + lasso (L1)
  * ridge: overcome multi-collinearity by pulling coeffs close to 0
  * lasso: variable selection by setting coeffs = 0
- NN:
  * L2 regularisation implicitly included in SGD when using weight decay (pull params towards 0) -> does this combat multi-collinearity as well or serve another purpose?
  * L1
    + key claim of DL: feature selection obsolete, network finds important ones itself
    + still, feature usually always has some impact (back this up), no explicit feature selection
    + explicit feature selection simply by not inluding a feature
  * another classical NN regularisation technique: dropout against co-adaptation
  * early stopping against overfitting when using iterative method
-> all of those require specifying Hyperparameters in advance

%\blindmathpaper


\section{Background and Related Works}

% feels better in front of AutoML, as it feels more qualitative, less quantitative
\subsection{Interpretability}
As there is no clear definition for interpretability, we will consider it as ``the ability to provide \textit{explanations} in \textit{understandable terms} to a human'',
where \textit{explanations} are logical decision rules and \textit{understandable terms} relate to commonly used terms in the domain of the problem,
as suggested by \citet[chap. 1]{survey_NN_interpretability}.
Further, we use the term ``explanability'' in an exchangeable manner with ``interpretability'', as is commonly done.

(Why Interpretability is desirable?)
- in many ways desirable, as per \citet[pp. 3-4]{Zach2019InterpretabilityOD}, e.g.
  * \textbf{gaining trust}: most notable use case, e.g. medical diagnosis: predictions need to be validated by doctors \citep[p. 1]{review_NN_interpretability}
  * \textbf{model debugging}
    - model is optimised w.r.t. loss and judged on accuracy
    - might have unexpected performance drops in certain situations, makes model unreliable \citep[1B]{survey_NN_interpretability}
    - having an interpretable model + model induction process can help find ways to improve accuracy + reliability
  * \textbf{scientific understanding}: when only models can make sense of increasingly complex data anymore, interpretability enables extraction of learnt knowledge encoded
    in the model to make it accessible + reliable to humans (also mentioned here \citep[p. 1]{review_NN_interpretability})
  * \textbf{subconcious bias}, e.g. loan approval: must ensure that decision is not discriminatory
  * \textbf{regulatory}
    - such as EU's "Right to Explanation" warranted by the GDPR \citep[p. 1]{review_NN_interpretability}, \citep[1B]{survey_NN_interpretability} or
    - drug approval processes in situations where a machine learning model discovered a new drug, process needs to be transparent for the regulator to approve \citep[1B]{survey_NN_interpretability}

Commonly, methods for model interpretation are divided into intrinsic methods, where the search space only comprises models with a structure simple enough to be
considered ``explainable'' (such as tree-based or simple linear models), and post-hoc methods, where interpretation methods are applied after model training.
Amongst post-hoc techniques, we can further divide the space into model-specific (such as analysing GLM coefficients) and model-agnostic
(e.g. partial depence plots, ALE) methods. \citet[chap. 3.2]{molnar2022}

(Taxonomy)
\citet[chap. 2]{survey_NN_interpretability} extend this distinction to a 3-dimensional taxonomy, allowing for better categorisation of neural networks (NN), a model class
that in its fully-connected feedforwad form is inherently non-interpretable.
% alternative
%As fully-connected feedforward neural networks (NNs) are inherently non-interpretable models, the taxonomy introduced by \citet[chap. 2]{survey_NN_interpretability} proves
%particulary useful to categorise NN explanability methods along 3 dimensions:
- \textbf{Passive vs Active Approaches}
  * passive: post-hoc
  * active: changing architecture or training process to make model interpretable
- \textbf{Type of Explanations}
  * examples: providing examples of what leads to desired output
  * attribution: attributing effect on output for a specific feature
  * hidden semantics: examine what type of inputs specific neurons / layers pick up on
  * rules: logical rules, e.g. if-then, trees
- \textbf{Local vs Global Interpretability}
  * local: explanation based on individual samples
  * semi-local: explanation based on set of samples, e.g. grouped together by some criterion
  * global: explaining network as a whole

(Evaluation)
- objective evaluation difficult, as no clear definition of interpretability
- \citet[3]{DoshiVelez2017TowardsAR} propose a taxonomy to categorise possible evaluation methods
  * \textbf{application-grounded evaluation}
    - most rigorous method, also most expensive + time consuming
    - idea: evaluate ``interpretability model'' directly w.r.t. the task -> human experts evaluate the outcome
    - e.g.: model performing medical diagnosis on patients would be evaluated by based on doctors doing the same
  * \textbf{human-grounded metrics}
    - similar to application-grounded, but tries to simplify task (while preserving its essence) so that laypersons can do it
    - cheaper due to larger, less qualified subject pool
    - especially suitable if only general concepts of the tasks need to be validated
    - examples for evaluation set-up
      * binary forced choice: human evaluator chooses which of two explanations he finds better
      * forward simulation: evaluator must correctly simulate model output when presented model input + explanation % perhaps binary forced choice is already enough to include, this (and the third one is a bit awkward to explain why this is good for interpretability imo)
  * \textbf{functionally-grounded evaluation}
    - assess explanation quality via some formally defined proxy for interpretability
    - no human time nor cost required beyond initial formulation of proxy
    - e.g. if we already have an interpretable model class (e.g. identified via human-grounded evaluation), can then rank different models of that class based on proxy
    - main challenge: what proxies

\subsection{Hyperparameter Optimization (HPO)}
- give intro to HPO = black-box optim problem -> we can apply any black box algo

[model free]
(grid + random search)
- most basic strategy: grid search, for all HPs define range of interest, then evaluate cartesian product of those
- random search
  * randomly sample value for each HP until it runs out of budget
  * explorative: for budget $B$, each HP will (almost guaranteed) be queried with $B$ different values, whereas grid search only $B^{1/N}$ for $N$ HPs
  * thus better than grid in case of some HPs being non-informative (almost always the case), as it doesn't waste time specifically exploring these (as does grid)
  * usually useful as baseline, as no assumptions about model, easy to parallelise, high exploration, and in expectation will ``achieve performance arbitrarily close to the optimum''

(evolutionary algorithms)
- another model-free class of optimisation algorithms, inspired by naturally occuring evolution
- based on a population, which in each generation (iteration) generates $\lambda$ offspring via 3 operators (all sub-bullets from \citet[pp. 10-]{genetic_algos})
  * reproduction: pick an individual to reproduce with p proportional to its fitness
  * crossover
    - select 2 ``parents'' from pool of reproducing individuals
    - for a position $k$ in their HP configs, with certain p, swap their values after $k$, yielding 2 children
    %use example from book page 12
  * mutation: with certain p, change values of an individual, e.g. by adding Gaussian noise (Gaussian mutation) to real values, or flipping bit for binary values
- selection at end of each genertion: keep $\mu$ best individuals (based on some fitness function), either from only offspring (``$(\mu,\lambda)$-selection'')
  or (more commonly) from population + offspring (``$(\mu+\lambda)$-selection'', guaranteed to keep best individual)
- pro: conceptually simple + can handle even complex parameter spaces (continuous, discrete, hierarchical, etc.) given appropriate implementation of operators
all above starting at HPO from \citep[chap. 1.3]{feurer_hyperparameter_2019}
- prominent examples
  * CMA-ES
    - ``Covariance Matrix Adapation Evolution Strategy''
    - offspring generation exclusively via multivariate normal \citep[p. 8]{hansen2023cmaevolutionstrategytutorial}
      * mean = weighted average of previous generation
      * covariance = weighted covariance of previous generation, with weighting as for mean
    - weighing scheme done to sample in a way as to reproduce previously successful (i.e. selected) steps \citep[p. 11]{hansen2023cmaevolutionstrategytutorial}
  * differential evolution \citep[-]{differential_evolution}
    - init population randomly so that entire param space is covered
      * $D$-dim vectors $\boldsymbol{x}_{i,G}$ with $i=1,2,...,n$ for gen $G$
    - mutation
      * for each $\boldsymbol{x}_{i,G}$, generate \textbf{mutant vector} $\boldsymbol{v}_{i,G+1}=\boldsymbol{x}_{r_1,G}+F\cdot(\boldsymbol{x}_{r_2,G}-\boldsymbol{x}_{r_3,G})$
      * $r_1,r_2,r_3\in\{1,2,...,n\}$ mutually different random idx + different from $i$
      * constant $F\in[0,2]$
    - crossover
      * \textbf{trial vector} $\boldsymbol{u}_{i,G+1}=(u_{i,G+1}^{(1)},u_{i,G+1}^{(2)},...,u_{i,G+1}^{(D)})$
        - chose random index $R\in\{1,2,...,D\}$
        - sample $p\sim U[0,1]$
        - define crossover constant $\text{CR}\in[0,1]$
        - $u_{i,G+1} = \begin{dcases}
          v_{i,G+1}^{(j)} & \text{if } p\le\text{CR}\text{ or } j=R \\
          x_{i,G}^{(j)} & \text{else}
         \end{dcases}$
    - selection: greedy, compare fitness of $\boldsymbol{u}_{i,G+1}$ with $\boldsymbol{x}_{i,G}$, pick better

[model based]
(Bayesian optimisation)

  
\subsection{Neural Architecture Search (NAS)}
give background on NAS (e.g. one-shot, etc.), but for this project wanted to have a "minimal viable product" to test whether NNs can yield competitive performance
on tabular ML + be interpretable using EAGGA at all before optimising the inner networks to the max

\subsection{Multi-Objective Optimization}
- problem: all aforementioned approaches are a-priori approaches -> we choose best HPs (weight decay, features used, dropout) w.r.t. performance
- intro MOO
\subsubsection{Pareto-optimality}
\subsubsection{A-priori}
\subsubsection{A-posteriori}
- NSGA-II (MOEA)
  * non-dominated sorting
  * crowding distance
  -> give intuition why non-dom-sort (asc) + crowding dist (desc) makes sense for selecting individuals


%likely shouldn't be an entire section
\section{EAGGA}
- for our problem (interpretability) we cannot use regular EA algorithms as we are also interested in reducing pairwise interactions + non-monotone feature effects
- this creates high-dim search space as shown in EAGGA paper -> they provide a framework to efficiently traverse through a space that is reduced to only sensible HP
  configuration (e.g. interaction of A,B with A monot incr and B monot decr -> couldn't be guaranteed -> don't look at it in first place)
- NSGA-II inspired evolutionary / genetic algorithm
- introduces group structure for more efficient optimization over the entire search space


\section{Extension to neural networks}
- why extend it to NNs?
  * NNs notoriously uninterpretable due to complex transformation of the feature space
  * due to (lin alg) non-linear activation functions, interactions can be modelled
  * no monotonicity guarantees
- our approach
  (general algorithm)
  * EAGGA algorithm implemented just as described in the paper
  * HP init
    - NN total layers $\in\{3, ..., 10\}$, i.e. hidden layers $\in\{1, ..., 8\}$, init from trunc geom with p=0.5
    - NN nodes per hidden layer (for each group) $\in\{3, ..., 20\}$, init from trunc geom with p=0.5
    - NN dropout \% $\in[0, 1]$, init from trunc gamma with shape=2, scale=0.15
  * group structure init
    - feature detector
      * as described in paper
      * but instead of fitting 10 trees + taking rel. \# of features used as p for trunc geom, we simply use 0.5 (sklearn dectree examination not straightforward)
      * also in preliminary experiments we found the sampled \# of features to be used occassionally to be > \# of non-0 values in normalised information gain filter
        (e.g. if former is = total \# of features and one feature is indep. from target) in these edge cases we only use the features with non-0 filter values
    - interaction detector
      * as described in paper
      * same as for feat detect: use p=0.5 to sample \# of interactions used instead of 10 dec trees
      * also for FAST algorithm don't use RSS (lin reg metric) but mean accuracy, as we fit log reg
    - monotonicity detector
      * as described in paper
      * use default HPs (max depth 30, minsplie = 20) of mlr3 classification tree implementation, as this is the library the paper uses
    -> both interaction + monotonicity detectors fit their models on 80\% of train split from holdout + eval on remaining 20\%
  (implementation details)
  * group structures + datasets
    - group structures implemented as described in the paper with additional list encoding sign of monotonicity of the individual features as detected by the
      monotonicity detector
    - included features are passed to dataset, dataset outputs included features, multiplied by the features' individual monotonicity (-1 / 1)
    - entire group structure is passed to NN, where architecture is built accordingly
  * neural network
    - "instead of XGB as in the original paper, we apply the method to NNs to examine whether this type of regularisation can achieve interpretability on NNs while
      outperforming EAGGA on XGB (original paper), ..., this requires special architecture, etc. pp."
    - NN
      * comprises of "sub-NNs", one for each equivalence relation -> basically non-fully connected NN
      * hidden layers use ReLU activation and dropout afterwards, then a shared output layer with concatenated sub-NNs' outputs as input and sigmoid activation
        (implicit in the loss function for better numerical stability, NN itself outputs "logits", which refers to pre-activation output in pytorch)
    - feature sparsity thus achieved by only training on included feature groups -> goes somewhat against of deep learning where model is supposed to decide itself,
      which feature to "use" / put importance on -> ELABORATE
    - feature interaction achieved by grouping, max-operation in ReLU induces interaction effect (different kind of interaction than e.g. in LM with multiplication)
      -> equation why the interacting features need to be grouped together when using ReLU, cf. photos
    - monotonicty constraint achieved by clipping weights to [0, infty) for restricted equivalence relations (monotonic decrease achieved via dataset object multiplying
      features with their individual signs), bias clipping not necessary (constant additive term)
      -> equation how monotonicity is enforced with this
  * evaluation, holdout, cv, early stopping
    - evaluation via dominated hypervolume along AUC-ROC, NF, NI, NNM as defined in original paper
      * NF simply rel. \# of included features
      * NI = sum of all possible pairwise interactions in each group over all possible pairwise interactions among all features = $\frac{\sum_g^G {p_g \choose 2}}{{p \choose 2}}$
      * NNM = rel. \# unconstrained features
    - outer holdout split: 2/3 train, 1/3 test (as in paper)
      * run EAGGA on holdout train split, i.e. train + select best $\mu=100$ individuals based on non-dom-sorting (ascending) + crowding distance (descending) as tie breaker
    - inner CV split on outer train portion: 5-fold (as in paper)
      * in each fold fit model on 80\% of CV-train portion, use remaining 20\% of CV-train for early stopping
    - early stopping criterion
      * for each fold, always train for min 200 epochs, keep track of model with lowest loss
      * after that, use patience of 100: if current model's loss is > mean of last 100 epochs' losses
      * if no early stopping, train each fold for max 10 minutes
      * then stop training, return model with lowest loss
    -> add graphic of entire dataset split
    - then evaluate on last fold from CV + keep best $\mu$ individuals + generate $\lambda=10$ offspring for new generation + repeat
  * hardware: training on Sagemaker Notebook instance
    - initial experiments on ml.g4dn.xlarge instance (2 vCPUs, 16GiB RAM, 1 NVIDIA T4, cf. https://aws.amazon.com/de/ec2/instance-types/g4/)
    using cuda not much faster than on ml.t3.medium (2 Intel Xeon 8000 vCPUs, 4GiB RAM, cf. https://aws.amazon.com/de/ec2/instance-types/t3/)
    - thus decided for more economic + ressourcen-schonend t3.medium
  * known bugs
    - in rare cases (anecdotally once every ~5-10 datasets), gga\_mutate seems to be generating -1 as monotonicity attribute, despite np.random.randint(low=0, high=2, size=1)
      * loop crashes at group\_structure creation, for these cases subtract previous runtime from 8hrs + load from last generation in output via load\_population
      * so far only happened for madeline in after gen-8.json after 5hrs 45mins (at start of gen-9, which wasn't exported yet) -> loaded this and ran for another 2hrs 15mins


\section{Experimental Results and Discussion}


\section{Conclusion and Future Outlook}
- MO BO on group structure space possible?
- here is a citation \cite{EAGGA}.

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section{Software used}
for implementation we used
openml \cite{OpenML}, \cite{OpenMLPython},
numpy \cite{numpy},
pandas \cite{pandas1}, \cite{pandas2},
pytorch \cite{PyTorch},
scikit-learn \cite{scikit-learn},
scipy \cite{SciPy},
pymoo \cite{pymoo}, and
tqdm \cite{tqdm}

\section{}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\section{}

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{references}

\end{document}
