{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import tasks\n",
    "\n",
    "from classes import EAGGA\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\openml\\tasks\\functions.py:372: UserWarning: `download_data` will default to False starting in 0.16. Please set `download_data` explicitly to suppress this warning.\n",
      "  warnings.warn(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\openml\\tasks\\functions.py:380: UserWarning: `download_qualities` will default to False starting in 0.16. Please set `download_qualities` explicitly to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "oml_task_ids = [37, 43, 3903, 3904, 3913, 3918, 10093, 9946, 146819, 359955, 189922, 359962, 190392, 167120, 190137, 190410, 168350, 359975, 359972, 146820]\n",
    "oml_tasks = tasks.get_tasks(oml_task_ids)\n",
    "\n",
    "oml_datasets = [oml_task.get_dataset() for oml_task in oml_tasks]\n",
    "\n",
    "# define positive classes\n",
    "positive_classes = ['tested_positive', '1', True, True, 'yes', True, '2', '2', '1', '2', '1', True, '1', '1', '2', '1', '2', 'Anomaly', '1', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset diabetes\n",
      "Starting init population\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished init population\n",
      "Start EAGGA at 2025-02-27T22:50:28.666434\n",
      "Generation 1, evaluate 15 individuals\n",
      "Running 5-fold CV for individual: 3 total layers, 4 nodes per hidden layer, gs: ([0, 2, 3, 4, 5, 6, 7], [[[1], 1]])\n",
      "Stop early: 0.6359412317474684 < 0.6667567193508148, epoch stop: 18\n",
      "Fold 1/5 | trained for 18 epochs | metrics: {'loss': 0.6625715834753854, 'auc': np.float64(0.7725953565505805), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Stop early: 0.6545042832692464 < 0.6554080347220103, epoch stop: 21\n",
      "Fold 2/5 | trained for 21 epochs | metrics: {'loss': 0.6368585910115924, 'auc': np.float64(0.8289800995024876), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Stop early: 0.6634538481632869 < 0.6640152931213379, epoch stop: 47\n",
      "Fold 3/5 | trained for 47 epochs | metrics: {'loss': 0.6530900767871312, 'auc': np.float64(0.2638888888888889), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Stop early: 0.6855474104483923 < 0.6965460081895193, epoch stop: 14\n",
      "Fold 4/5 | trained for 14 epochs | metrics: {'loss': 0.6736158813749041, 'auc': np.float64(0.20707070707070704), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Stop early: 0.6469117552042007 < 0.6495522856712341, epoch stop: 34\n",
      "Fold 5/5 | trained for 34 epochs | metrics: {'loss': 0.6514342171805245, 'auc': np.float64(0.8217484008528784), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual: 7 total layers, 3 nodes per hidden layer, gs: ([0, 2, 3, 6], [[[1, 5, 7], 1], [[4], 0]])\n",
      "Stop early: 0.6188191155592601 < 0.6196133693059286, epoch stop: 26\n",
      "Fold 1/5 | trained for 26 epochs | metrics: {'loss': 0.6105022260120937, 'auc': np.float64(0.6774461028192371), 'nf': 0.5, 'ni': 0.10714285714285714, 'nnm': 0.125}\n",
      "Stop early: 0.6927923321723937 < 0.6931642989317576, epoch stop: 12\n",
      "Fold 2/5 | trained for 12 epochs | metrics: {'loss': 0.6429996320179531, 'auc': np.float64(0.46434494195688225), 'nf': 0.5, 'ni': 0.10714285714285714, 'nnm': 0.125}\n",
      "Stop early: 0.6619213829437892 < 0.6621972322463989, epoch stop: 13\n",
      "Fold 3/5 | trained for 13 epochs | metrics: {'loss': 0.6308788316590446, 'auc': np.float64(0.18686868686868685), 'nf': 0.5, 'ni': 0.10714285714285714, 'nnm': 0.125}\n",
      "Stop early: 0.5240042855342228 < 0.5308337410291036, epoch stop: 75\n",
      "Fold 4/5 | trained for 75 epochs | metrics: {'loss': 0.476388299039432, 'auc': np.float64(0.8396464646464646), 'nf': 0.5, 'ni': 0.10714285714285714, 'nnm': 0.125}\n",
      "Stop early: 0.4770462920268376 < 0.4787555088599523, epoch stop: 54\n",
      "Fold 5/5 | trained for 54 epochs | metrics: {'loss': 0.5690823708261762, 'auc': np.float64(0.7573560767590619), 'nf': 0.5, 'ni': 0.10714285714285714, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual: 3 total layers, 3 nodes per hidden layer, gs: ([0, 1, 2, 3, 4, 5, 6], [[[7], 0]])\n",
      "Stop early: 0.5831342726945877 < 0.5847130517164866, epoch stop: 109\n",
      "Fold 1/5 | trained for 109 epochs | metrics: {'loss': 0.6189126372337341, 'auc': np.float64(0.7008706467661692), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Stop early: 0.659770264228185 < 0.6598533193270365, epoch stop: 148\n",
      "Fold 2/5 | trained for 148 epochs | metrics: {'loss': 0.6390923772539411, 'auc': np.float64(0.63287728026534), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Stop early: 0.6177658905585608 < 0.6178049047787985, epoch stop: 147\n",
      "Fold 3/5 | trained for 147 epochs | metrics: {'loss': 0.6297720542975834, 'auc': np.float64(0.5), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Stop early: 0.6860674828290939 < 0.6860804259777069, epoch stop: 27\n",
      "Fold 4/5 | trained for 27 epochs | metrics: {'loss': 0.6593387382371085, 'auc': np.float64(0.7295875420875421), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Stop early: 0.6612573891878128 < 0.6612660884857178, epoch stop: 19\n",
      "Fold 5/5 | trained for 19 epochs | metrics: {'loss': 0.6341186421258109, 'auc': np.float64(0.685501066098081), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual: 6 total layers, 3 nodes per hidden layer, gs: ([0, 2, 3, 4, 6, 7], [[[1, 5], 0]])\n",
      "Stop early: 0.6022173355023066 < 0.6045776704947153, epoch stop: 51\n",
      "Fold 1/5 | trained for 51 epochs | metrics: {'loss': 0.6128394986901965, 'auc': np.float64(0.6390961857379768), 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Stop early: 0.660040341814359 < 0.6600717802842458, epoch stop: 37\n",
      "Fold 2/5 | trained for 37 epochs | metrics: {'loss': 0.6442358834402901, 'auc': np.float64(0.5), 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Stop early: 0.6502035597960154 < 0.6503838300704956, epoch stop: 48\n",
      "Fold 3/5 | trained for 48 epochs | metrics: {'loss': 0.6397384916033063, 'auc': np.float64(0.734638047138047), 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Stop early: 0.6615759541591009 < 0.6615853110949198, epoch stop: 89\n",
      "Fold 4/5 | trained for 89 epochs | metrics: {'loss': 0.6334840612752097, 'auc': np.float64(0.5), 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Stop early: 0.5721165140469869 < 0.5739272634188334, epoch stop: 95\n",
      "Fold 5/5 | trained for 95 epochs | metrics: {'loss': 0.5078486544745309, 'auc': np.float64(0.8115138592750533), 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Running 5-fold CV for individual: 5 total layers, 4 nodes per hidden layer, gs: ([0, 1, 2, 3, 6, 7], [[[4, 5], 0]])\n",
      "Stop early: 0.6031256372729938 < 0.603195051352183, epoch stop: 41\n",
      "Fold 1/5 | trained for 41 epochs | metrics: {'loss': 0.6652736578668866, 'auc': np.float64(0.5613598673300166), 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Stop early: 0.6226547926664352 < 0.6228041251500448, epoch stop: 31\n",
      "Fold 2/5 | trained for 31 epochs | metrics: {'loss': 0.6030007387910571, 'auc': np.float64(0.7048092868988391), 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Stop early: 0.6191638131936392 < 0.6204050878683726, epoch stop: 49\n",
      "Fold 3/5 | trained for 49 epochs | metrics: {'loss': 0.5839534955365318, 'auc': np.float64(0.6769781144781145), 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Stop early: 0.6679998824993769 < 0.6689004600048065, epoch stop: 12\n",
      "Fold 4/5 | trained for 12 epochs | metrics: {'loss': 0.6492668219975063, 'auc': np.float64(0.4345538720538721), 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Stop early: 0.61090939193964 < 0.6113667786121368, epoch stop: 38\n",
      "Fold 5/5 | trained for 38 epochs | metrics: {'loss': 0.6300877801009587, 'auc': np.float64(0.5223880597014925), 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Running 5-fold CV for individual: 4 total layers, 8 nodes per hidden layer, gs: ([0, 2, 3, 4, 6], [[[1, 5, 7], 1]])\n",
      "Stop early: 0.658141465485096 < 0.6586677233378092, epoch stop: 11\n",
      "Fold 1/5 | trained for 11 epochs | metrics: {'loss': 0.6616654821804592, 'auc': np.float64(0.4170812603648425), 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.0}\n",
      "Stop early: 0.6769290546576181 < 0.6838954786459605, epoch stop: 11\n",
      "Fold 2/5 | trained for 11 epochs | metrics: {'loss': 0.6349002335752759, 'auc': np.float64(0.46019900497512434), 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.0}\n",
      "Stop early: 0.6523568073908488 < 0.658035159111023, epoch stop: 12\n",
      "Fold 3/5 | trained for 12 epochs | metrics: {'loss': 0.678150304726192, 'auc': np.float64(0.4221380471380471), 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.0}\n",
      "Stop early: 0.6198993707696597 < 0.627234602967898, epoch stop: 11\n",
      "Fold 4/5 | trained for 11 epochs | metrics: {'loss': 0.6615234272820609, 'auc': np.float64(0.5589225589225589), 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.0}\n",
      "Stop early: 0.6751619984706243 < 0.6776493489742279, epoch stop: 10\n",
      "Fold 5/5 | trained for 10 epochs | metrics: {'loss': 0.6482984125614166, 'auc': np.float64(0.3859275053304905), 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual: 6 total layers, 3 nodes per hidden layer, gs: ([0, 3, 6, 7], [[[1, 2, 4, 5], 0]])\n",
      "Stop early: 0.46451458930969236 < 0.465952401359876, epoch stop: 67\n",
      "Fold 1/5 | trained for 67 epochs | metrics: {'loss': 0.556197030203683, 'auc': np.float64(0.7408789386401327), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.48049038747946426 < 0.48089352746804553, epoch stop: 84\n",
      "Fold 2/5 | trained for 84 epochs | metrics: {'loss': 0.5622991323471069, 'auc': np.float64(0.7535240464344942), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.6177548299233119 < 0.6177683373292288, epoch stop: 57\n",
      "Fold 3/5 | trained for 57 epochs | metrics: {'loss': 0.6481839844158718, 'auc': np.float64(0.5), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.6156333833932877 < 0.6173633436361948, epoch stop: 44\n",
      "Fold 4/5 | trained for 44 epochs | metrics: {'loss': 0.540996083191463, 'auc': np.float64(0.79503367003367), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.6731257895628611 < 0.6734151045481364, epoch stop: 68\n",
      "Fold 5/5 | trained for 68 epochs | metrics: {'loss': 0.6341544815472194, 'auc': np.float64(0.4878464818763326), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Running 5-fold CV for individual: 3 total layers, 3 nodes per hidden layer, gs: ([0, 2, 3, 4], [[[1, 5, 6, 7], 1]])\n",
      "Stop early: 0.6615762760241826 < 0.6615876456101736, epoch stop: 80\n",
      "Fold 1/5 | trained for 80 epochs | metrics: {'loss': 0.6394779682159424, 'auc': np.float64(0.5), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.0}\n",
      "Stop early: 0.6335522870222727 < 0.6347903410593668, epoch stop: 48\n",
      "Fold 2/5 | trained for 48 epochs | metrics: {'loss': 0.6310372012002128, 'auc': np.float64(0.7645107794361525), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.0}\n",
      "Stop early: 0.6790305435657501 < 0.6796225309371948, epoch stop: 168\n",
      "Fold 3/5 | trained for 168 epochs | metrics: {'loss': 0.657855144568852, 'auc': np.float64(0.3114478114478114), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.0}\n",
      "Stop early: 0.597457478940487 < 0.5987779647111893, epoch stop: 79\n",
      "Fold 4/5 | trained for 79 epochs | metrics: {'loss': 0.627406758921487, 'auc': np.float64(0.6923400673400674), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.0}\n",
      "Stop early: 0.65916614929835 < 0.6615111033121744, epoch stop: 40\n",
      "Fold 5/5 | trained for 40 epochs | metrics: {'loss': 0.685213463647025, 'auc': np.float64(0.32238805970149254), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual: 6 total layers, 3 nodes per hidden layer, gs: ([0, 1, 2, 3, 4, 6, 7], [[[5], 0]])\n",
      "Stop early: 0.6160246933499972 < 0.6164204527934393, epoch stop: 114\n",
      "Fold 1/5 | trained for 114 epochs | metrics: {'loss': 0.6301783323287964, 'auc': np.float64(0.6710199004975125), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Stop early: 0.6177131712436675 < 0.6177671054999033, epoch stop: 74\n",
      "Fold 2/5 | trained for 74 epochs | metrics: {'loss': 0.6581913147653852, 'auc': np.float64(0.5), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Stop early: 0.6176446328560511 < 0.6177244285742441, epoch stop: 108\n",
      "Fold 3/5 | trained for 108 epochs | metrics: {'loss': 0.6389546649796622, 'auc': np.float64(0.5), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 32\u001b[0m\n\u001b[0;32m     20\u001b[0m file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mexport\u001b[39m\u001b[39m'\u001b[39m, name)\n\u001b[0;32m     22\u001b[0m eagga \u001b[39m=\u001b[39m EAGGA(\n\u001b[0;32m     23\u001b[0m     oml_dataset\u001b[39m=\u001b[39moml_dataset,\n\u001b[0;32m     24\u001b[0m     class_positive\u001b[39m=\u001b[39mclass_positive,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     file_path\u001b[39m=\u001b[39mfile_path\n\u001b[0;32m     31\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m eagga\u001b[39m.\u001b[39;49mrun_eagga()\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:577\u001b[0m, in \u001b[0;36mrun_eagga\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mStart EAGGA at \u001b[39m\u001b[39m{\u001b[39;00mtime_start\u001b[39m.\u001b[39misoformat()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    576\u001b[0m gen \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 577\u001b[0m \u001b[39mwhile\u001b[39;00m(datetime\u001b[39m.\u001b[39mnow() \u001b[39m<\u001b[39m time_start \u001b[39m+\u001b[39m timedelta(seconds\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msecs_total)):\n\u001b[0;32m    578\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGeneration \u001b[39m\u001b[39m{\u001b[39;00mgen\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, evaluate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffspring)\u001b[39m}\u001b[39;00m\u001b[39m individuals\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    580\u001b[0m     \u001b[39mfor\u001b[39;00m individual \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffspring:\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:660\u001b[0m, in \u001b[0;36mrun_cv\u001b[1;34m(self, individual)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:697\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self, optimizer, loss_fn, model, dataset_train, dataset_stop_early)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[39mfor\u001b[39;00m batch_input, batch_target \u001b[39min\u001b[39;00m loader_train:  \u001b[39m# divide data in mini batches\u001b[39;00m\n\u001b[0;32m    696\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# set gradients to 0\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m     batch_output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39mbatch_input)\u001b[39m.\u001b[39mflatten()  \u001b[39m# expand batch_input as it is a list of tuples (Dataset getter splits according to group structure)\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     batch_loss \u001b[39m=\u001b[39m loss_fn(batch_output, batch_target)\n\u001b[0;32m    700\u001b[0m     running_epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hps = {\n",
    "    'total_layers': (3, 10),\n",
    "    'nodes_per_hidden_layer': (3, 20),\n",
    "    'mu': 15,  # TODO: 100\n",
    "    'lambda': 5,  # TODO: 10\n",
    "    'holdout_train_size': 2/3,\n",
    "    'cv_k': 5\n",
    "}\n",
    "\n",
    "batch_size = 16\n",
    "patience = 10\n",
    "\n",
    "secs_per_fold = 30\n",
    "secs_total = 15 * 60\n",
    "\n",
    "for (oml_dataset, class_positive) in zip(oml_datasets[:1], positive_classes[:1]):  # TODO: remove [:1]\n",
    "    name = oml_dataset.name\n",
    "    print(f'Dataset {name}')\n",
    "\n",
    "    file_path = os.path.join('export', name)\n",
    "    \n",
    "    eagga = EAGGA(\n",
    "        oml_dataset=oml_dataset,\n",
    "        class_positive=class_positive,\n",
    "        hps=hps,\n",
    "        batch_size=batch_size,\n",
    "        patience=patience,\n",
    "        secs_per_fold=secs_per_fold,\n",
    "        secs_total=secs_total,\n",
    "        file_path=file_path\n",
    "    )\n",
    "    eagga.run_eagga()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
