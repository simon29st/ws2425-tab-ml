{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import tasks\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nds import ndomsort\n",
    "\n",
    "from classes import Dataset, GroupStructure, WeightClipper, Prob\n",
    "from functions import run_eagga_cv, generate_offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "oml_task_diabetes = tasks.get_task(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, categorical_indicator, attribute_names = oml_task_diabetes.get_dataset().get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50.0</td>\n",
       "      <td>tested_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32.0</td>\n",
       "      <td>tested_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33.0</td>\n",
       "      <td>tested_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47.0</td>\n",
       "      <td>tested_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg   plas  pres  skin   insu  mass   pedi   age            class\n",
       "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0  tested_positive\n",
       "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0  tested_negative\n",
       "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0  tested_positive\n",
       "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0  tested_negative\n",
       "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0  tested_positive\n",
       "..    ...    ...   ...   ...    ...   ...    ...   ...              ...\n",
       "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0  tested_negative\n",
       "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0  tested_negative\n",
       "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0  tested_negative\n",
       "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0  tested_positive\n",
       "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0  tested_negative\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False, False, False, True]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "feats_selected = GroupStructure.detector_features(Xy, categorical_indicator)\n",
    "#feats_selected = {0,1,2,3,4,5,6,7}  # TODO: remove\n",
    "print(feats_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pres']\n",
      "[[ 72.]\n",
      " [ 66.]\n",
      " [ 64.]\n",
      " [ 66.]\n",
      " [ 40.]\n",
      " [ 74.]\n",
      " [ 50.]\n",
      " [  0.]\n",
      " [ 70.]\n",
      " [ 96.]\n",
      " [ 92.]\n",
      " [ 74.]\n",
      " [ 80.]\n",
      " [ 60.]\n",
      " [ 72.]\n",
      " [  0.]\n",
      " [ 84.]\n",
      " [ 74.]\n",
      " [ 30.]\n",
      " [ 70.]\n",
      " [ 88.]\n",
      " [ 84.]\n",
      " [ 90.]\n",
      " [ 80.]\n",
      " [ 94.]\n",
      " [ 70.]\n",
      " [ 76.]\n",
      " [ 66.]\n",
      " [ 82.]\n",
      " [ 92.]\n",
      " [ 75.]\n",
      " [ 76.]\n",
      " [ 58.]\n",
      " [ 92.]\n",
      " [ 78.]\n",
      " [ 60.]\n",
      " [ 76.]\n",
      " [ 76.]\n",
      " [ 68.]\n",
      " [ 72.]\n",
      " [ 64.]\n",
      " [ 84.]\n",
      " [ 92.]\n",
      " [110.]\n",
      " [ 64.]\n",
      " [ 66.]\n",
      " [ 56.]\n",
      " [ 70.]\n",
      " [ 66.]\n",
      " [  0.]\n",
      " [ 80.]\n",
      " [ 50.]\n",
      " [ 66.]\n",
      " [ 90.]\n",
      " [ 66.]\n",
      " [ 50.]\n",
      " [ 68.]\n",
      " [ 88.]\n",
      " [ 82.]\n",
      " [ 64.]\n",
      " [  0.]\n",
      " [ 72.]\n",
      " [ 62.]\n",
      " [ 58.]\n",
      " [ 66.]\n",
      " [ 74.]\n",
      " [ 88.]\n",
      " [ 92.]\n",
      " [ 66.]\n",
      " [ 85.]\n",
      " [ 66.]\n",
      " [ 64.]\n",
      " [ 90.]\n",
      " [ 86.]\n",
      " [ 75.]\n",
      " [ 48.]\n",
      " [ 78.]\n",
      " [ 72.]\n",
      " [  0.]\n",
      " [ 66.]\n",
      " [ 44.]\n",
      " [  0.]\n",
      " [ 78.]\n",
      " [ 65.]\n",
      " [108.]\n",
      " [ 74.]\n",
      " [ 72.]\n",
      " [ 68.]\n",
      " [ 70.]\n",
      " [ 68.]\n",
      " [ 55.]\n",
      " [ 80.]\n",
      " [ 78.]\n",
      " [ 72.]\n",
      " [ 82.]\n",
      " [ 72.]\n",
      " [ 62.]\n",
      " [ 48.]\n",
      " [ 50.]\n",
      " [ 90.]\n",
      " [ 72.]\n",
      " [ 60.]\n",
      " [ 96.]\n",
      " [ 72.]\n",
      " [ 65.]\n",
      " [ 56.]\n",
      " [122.]\n",
      " [ 58.]\n",
      " [ 58.]\n",
      " [ 85.]\n",
      " [ 72.]\n",
      " [ 62.]\n",
      " [ 76.]\n",
      " [ 62.]\n",
      " [ 54.]\n",
      " [ 92.]\n",
      " [ 74.]\n",
      " [ 48.]\n",
      " [ 60.]\n",
      " [ 76.]\n",
      " [ 76.]\n",
      " [ 64.]\n",
      " [ 74.]\n",
      " [ 80.]\n",
      " [ 76.]\n",
      " [ 30.]\n",
      " [ 70.]\n",
      " [ 58.]\n",
      " [ 88.]\n",
      " [ 84.]\n",
      " [ 70.]\n",
      " [ 56.]\n",
      " [ 64.]\n",
      " [ 74.]\n",
      " [ 68.]\n",
      " [ 60.]\n",
      " [ 70.]\n",
      " [ 60.]\n",
      " [ 80.]\n",
      " [ 72.]\n",
      " [ 78.]\n",
      " [ 82.]\n",
      " [ 52.]\n",
      " [ 66.]\n",
      " [ 62.]\n",
      " [ 75.]\n",
      " [ 80.]\n",
      " [ 64.]\n",
      " [ 78.]\n",
      " [ 70.]\n",
      " [ 74.]\n",
      " [ 65.]\n",
      " [ 86.]\n",
      " [ 82.]\n",
      " [ 78.]\n",
      " [ 88.]\n",
      " [ 52.]\n",
      " [ 56.]\n",
      " [ 74.]\n",
      " [ 72.]\n",
      " [ 90.]\n",
      " [ 74.]\n",
      " [ 80.]\n",
      " [ 64.]\n",
      " [ 88.]\n",
      " [ 74.]\n",
      " [ 66.]\n",
      " [ 68.]\n",
      " [ 66.]\n",
      " [ 90.]\n",
      " [ 82.]\n",
      " [ 70.]\n",
      " [  0.]\n",
      " [ 60.]\n",
      " [ 64.]\n",
      " [ 72.]\n",
      " [ 78.]\n",
      " [110.]\n",
      " [ 78.]\n",
      " [ 82.]\n",
      " [ 80.]\n",
      " [ 64.]\n",
      " [ 74.]\n",
      " [ 60.]\n",
      " [ 74.]\n",
      " [ 68.]\n",
      " [ 68.]\n",
      " [ 98.]\n",
      " [ 76.]\n",
      " [ 80.]\n",
      " [ 62.]\n",
      " [ 70.]\n",
      " [ 66.]\n",
      " [  0.]\n",
      " [ 55.]\n",
      " [ 84.]\n",
      " [ 58.]\n",
      " [ 62.]\n",
      " [ 64.]\n",
      " [ 60.]\n",
      " [ 80.]\n",
      " [ 82.]\n",
      " [ 68.]\n",
      " [ 70.]\n",
      " [ 72.]\n",
      " [ 72.]\n",
      " [ 76.]\n",
      " [104.]\n",
      " [ 64.]\n",
      " [ 84.]\n",
      " [ 60.]\n",
      " [ 85.]\n",
      " [ 95.]\n",
      " [ 65.]\n",
      " [ 82.]\n",
      " [ 70.]\n",
      " [ 62.]\n",
      " [ 68.]\n",
      " [ 74.]\n",
      " [ 66.]\n",
      " [ 60.]\n",
      " [ 90.]\n",
      " [  0.]\n",
      " [ 60.]\n",
      " [ 66.]\n",
      " [ 78.]\n",
      " [ 76.]\n",
      " [ 52.]\n",
      " [ 70.]\n",
      " [ 80.]\n",
      " [ 86.]\n",
      " [ 80.]\n",
      " [ 80.]\n",
      " [ 68.]\n",
      " [ 68.]\n",
      " [ 72.]\n",
      " [ 84.]\n",
      " [ 90.]\n",
      " [ 84.]\n",
      " [ 76.]\n",
      " [ 64.]\n",
      " [ 70.]\n",
      " [ 54.]\n",
      " [ 50.]\n",
      " [ 76.]\n",
      " [ 85.]\n",
      " [ 68.]\n",
      " [ 90.]\n",
      " [ 70.]\n",
      " [ 86.]\n",
      " [ 52.]\n",
      " [ 84.]\n",
      " [ 80.]\n",
      " [ 68.]\n",
      " [ 62.]\n",
      " [ 64.]\n",
      " [ 56.]\n",
      " [ 68.]\n",
      " [ 50.]\n",
      " [ 76.]\n",
      " [ 68.]\n",
      " [  0.]\n",
      " [ 70.]\n",
      " [ 80.]\n",
      " [ 62.]\n",
      " [ 74.]\n",
      " [  0.]\n",
      " [ 64.]\n",
      " [ 52.]\n",
      " [  0.]\n",
      " [ 86.]\n",
      " [ 62.]\n",
      " [ 78.]\n",
      " [ 78.]\n",
      " [ 70.]\n",
      " [ 70.]\n",
      " [ 60.]\n",
      " [ 64.]\n",
      " [ 74.]\n",
      " [ 62.]\n",
      " [ 70.]\n",
      " [ 76.]\n",
      " [ 88.]\n",
      " [ 86.]\n",
      " [ 80.]\n",
      " [ 74.]\n",
      " [ 84.]\n",
      " [ 86.]\n",
      " [ 56.]\n",
      " [ 72.]\n",
      " [ 88.]\n",
      " [ 62.]\n",
      " [ 78.]\n",
      " [ 48.]\n",
      " [ 50.]\n",
      " [ 62.]\n",
      " [ 70.]\n",
      " [ 84.]\n",
      " [ 78.]\n",
      " [ 72.]\n",
      " [  0.]\n",
      " [ 58.]\n",
      " [ 82.]\n",
      " [ 98.]\n",
      " [ 76.]\n",
      " [ 76.]\n",
      " [ 68.]\n",
      " [ 68.]\n",
      " [ 68.]\n",
      " [ 68.]\n",
      " [ 66.]\n",
      " [ 70.]\n",
      " [ 74.]\n",
      " [ 50.]\n",
      " [ 80.]\n",
      " [ 68.]\n",
      " [ 80.]\n",
      " [ 74.]\n",
      " [ 66.]\n",
      " [ 78.]\n",
      " [ 60.]\n",
      " [ 74.]\n",
      " [ 70.]\n",
      " [ 90.]\n",
      " [ 75.]\n",
      " [ 72.]\n",
      " [ 64.]\n",
      " [ 70.]\n",
      " [ 86.]\n",
      " [ 70.]\n",
      " [ 72.]\n",
      " [ 58.]\n",
      " [  0.]\n",
      " [ 80.]\n",
      " [ 60.]\n",
      " [ 76.]\n",
      " [  0.]\n",
      " [ 76.]\n",
      " [ 78.]\n",
      " [ 84.]\n",
      " [ 70.]\n",
      " [ 74.]\n",
      " [ 68.]\n",
      " [ 86.]\n",
      " [ 72.]\n",
      " [ 88.]\n",
      " [ 46.]\n",
      " [  0.]\n",
      " [ 62.]\n",
      " [ 80.]\n",
      " [ 80.]\n",
      " [ 84.]\n",
      " [ 82.]\n",
      " [ 62.]\n",
      " [ 78.]\n",
      " [ 88.]\n",
      " [ 50.]\n",
      " [  0.]\n",
      " [ 74.]\n",
      " [ 76.]\n",
      " [ 64.]\n",
      " [ 70.]\n",
      " [108.]\n",
      " [ 78.]\n",
      " [ 74.]\n",
      " [ 54.]\n",
      " [ 72.]\n",
      " [ 64.]\n",
      " [ 86.]\n",
      " [102.]\n",
      " [ 82.]\n",
      " [ 64.]\n",
      " [ 64.]\n",
      " [ 58.]\n",
      " [ 52.]\n",
      " [ 82.]\n",
      " [ 82.]\n",
      " [ 60.]\n",
      " [ 75.]\n",
      " [100.]\n",
      " [ 72.]\n",
      " [ 68.]\n",
      " [ 60.]\n",
      " [ 62.]\n",
      " [ 70.]\n",
      " [ 54.]\n",
      " [ 74.]\n",
      " [100.]\n",
      " [ 82.]\n",
      " [ 68.]\n",
      " [ 66.]\n",
      " [ 76.]\n",
      " [ 64.]\n",
      " [ 72.]\n",
      " [ 78.]\n",
      " [ 58.]\n",
      " [ 56.]\n",
      " [ 66.]\n",
      " [ 70.]\n",
      " [ 70.]\n",
      " [ 64.]\n",
      " [ 61.]\n",
      " [ 84.]\n",
      " [ 78.]\n",
      " [ 64.]\n",
      " [ 48.]\n",
      " [ 72.]\n",
      " [ 62.]\n",
      " [ 74.]\n",
      " [ 68.]\n",
      " [ 90.]\n",
      " [ 72.]\n",
      " [ 84.]\n",
      " [ 74.]\n",
      " [ 60.]\n",
      " [ 84.]\n",
      " [ 68.]\n",
      " [ 82.]\n",
      " [ 68.]\n",
      " [ 64.]\n",
      " [ 88.]\n",
      " [ 68.]\n",
      " [ 64.]\n",
      " [ 64.]\n",
      " [ 78.]\n",
      " [ 78.]\n",
      " [  0.]\n",
      " [ 64.]\n",
      " [ 94.]\n",
      " [ 82.]\n",
      " [  0.]\n",
      " [ 74.]\n",
      " [ 74.]\n",
      " [ 75.]\n",
      " [ 68.]\n",
      " [  0.]\n",
      " [ 85.]\n",
      " [ 75.]\n",
      " [ 70.]\n",
      " [ 88.]\n",
      " [104.]\n",
      " [ 66.]\n",
      " [ 64.]\n",
      " [ 70.]\n",
      " [ 62.]\n",
      " [ 78.]\n",
      " [ 72.]\n",
      " [ 80.]\n",
      " [ 64.]\n",
      " [ 74.]\n",
      " [ 64.]\n",
      " [ 70.]\n",
      " [ 68.]\n",
      " [  0.]\n",
      " [ 54.]\n",
      " [ 62.]\n",
      " [ 54.]\n",
      " [ 68.]\n",
      " [ 84.]\n",
      " [ 74.]\n",
      " [ 72.]\n",
      " [ 62.]\n",
      " [ 70.]\n",
      " [ 78.]\n",
      " [ 98.]\n",
      " [ 56.]\n",
      " [ 52.]\n",
      " [ 64.]\n",
      " [  0.]\n",
      " [ 78.]\n",
      " [ 82.]\n",
      " [ 70.]\n",
      " [ 66.]\n",
      " [ 90.]\n",
      " [ 64.]\n",
      " [ 84.]\n",
      " [ 80.]\n",
      " [ 76.]\n",
      " [ 74.]\n",
      " [ 86.]\n",
      " [ 70.]\n",
      " [ 88.]\n",
      " [ 58.]\n",
      " [ 82.]\n",
      " [  0.]\n",
      " [ 68.]\n",
      " [ 62.]\n",
      " [ 78.]\n",
      " [ 72.]\n",
      " [ 80.]\n",
      " [ 65.]\n",
      " [ 90.]\n",
      " [ 68.]\n",
      " [ 70.]\n",
      " [  0.]\n",
      " [ 74.]\n",
      " [ 68.]\n",
      " [ 72.]\n",
      " [ 70.]\n",
      " [ 74.]\n",
      " [ 90.]\n",
      " [ 72.]\n",
      " [ 68.]\n",
      " [ 64.]\n",
      " [ 78.]\n",
      " [ 82.]\n",
      " [ 90.]\n",
      " [ 60.]\n",
      " [ 50.]\n",
      " [ 78.]\n",
      " [ 72.]\n",
      " [ 62.]\n",
      " [ 68.]\n",
      " [ 62.]\n",
      " [ 54.]\n",
      " [ 70.]\n",
      " [ 88.]\n",
      " [ 86.]\n",
      " [ 60.]\n",
      " [ 90.]\n",
      " [ 70.]\n",
      " [ 80.]\n",
      " [  0.]\n",
      " [ 70.]\n",
      " [ 58.]\n",
      " [ 60.]\n",
      " [ 64.]\n",
      " [ 74.]\n",
      " [ 66.]\n",
      " [ 65.]\n",
      " [ 60.]\n",
      " [ 76.]\n",
      " [ 66.]\n",
      " [  0.]\n",
      " [ 56.]\n",
      " [  0.]\n",
      " [ 90.]\n",
      " [ 60.]\n",
      " [ 80.]\n",
      " [ 92.]\n",
      " [ 74.]\n",
      " [ 72.]\n",
      " [ 85.]\n",
      " [ 90.]\n",
      " [ 78.]\n",
      " [ 90.]\n",
      " [ 76.]\n",
      " [ 68.]\n",
      " [ 82.]\n",
      " [110.]\n",
      " [ 70.]\n",
      " [ 68.]\n",
      " [ 88.]\n",
      " [ 62.]\n",
      " [ 64.]\n",
      " [ 70.]\n",
      " [ 70.]\n",
      " [ 76.]\n",
      " [ 68.]\n",
      " [ 74.]\n",
      " [ 76.]\n",
      " [ 66.]\n",
      " [ 68.]\n",
      " [ 60.]\n",
      " [ 80.]\n",
      " [ 54.]\n",
      " [ 72.]\n",
      " [ 62.]\n",
      " [ 72.]\n",
      " [ 66.]\n",
      " [ 70.]\n",
      " [ 96.]\n",
      " [ 58.]\n",
      " [ 60.]\n",
      " [ 86.]\n",
      " [ 44.]\n",
      " [ 44.]\n",
      " [ 80.]\n",
      " [ 68.]\n",
      " [ 70.]\n",
      " [ 90.]\n",
      " [ 60.]\n",
      " [ 78.]\n",
      " [ 76.]\n",
      " [ 76.]\n",
      " [ 56.]\n",
      " [ 66.]\n",
      " [ 66.]\n",
      " [ 86.]\n",
      " [  0.]\n",
      " [ 84.]\n",
      " [ 78.]\n",
      " [ 80.]\n",
      " [ 52.]\n",
      " [ 72.]\n",
      " [ 82.]\n",
      " [ 76.]\n",
      " [ 24.]\n",
      " [ 74.]\n",
      " [ 38.]\n",
      " [ 88.]\n",
      " [  0.]\n",
      " [ 74.]\n",
      " [ 78.]\n",
      " [  0.]\n",
      " [ 60.]\n",
      " [ 78.]\n",
      " [ 62.]\n",
      " [ 82.]\n",
      " [ 62.]\n",
      " [ 54.]\n",
      " [ 58.]\n",
      " [ 88.]\n",
      " [ 80.]\n",
      " [ 74.]\n",
      " [ 72.]\n",
      " [ 96.]\n",
      " [ 62.]\n",
      " [ 82.]\n",
      " [  0.]\n",
      " [ 86.]\n",
      " [ 76.]\n",
      " [ 94.]\n",
      " [ 70.]\n",
      " [ 64.]\n",
      " [ 88.]\n",
      " [ 68.]\n",
      " [ 78.]\n",
      " [ 80.]\n",
      " [ 65.]\n",
      " [ 64.]\n",
      " [ 78.]\n",
      " [ 60.]\n",
      " [ 82.]\n",
      " [ 62.]\n",
      " [ 72.]\n",
      " [ 74.]\n",
      " [ 76.]\n",
      " [ 76.]\n",
      " [ 74.]\n",
      " [ 86.]\n",
      " [ 70.]\n",
      " [ 80.]\n",
      " [  0.]\n",
      " [ 72.]\n",
      " [ 74.]\n",
      " [ 74.]\n",
      " [ 50.]\n",
      " [ 84.]\n",
      " [ 60.]\n",
      " [ 54.]\n",
      " [ 60.]\n",
      " [ 74.]\n",
      " [ 54.]\n",
      " [ 70.]\n",
      " [ 52.]\n",
      " [ 58.]\n",
      " [ 80.]\n",
      " [106.]\n",
      " [ 82.]\n",
      " [ 84.]\n",
      " [ 76.]\n",
      " [106.]\n",
      " [ 80.]\n",
      " [ 60.]\n",
      " [ 80.]\n",
      " [ 82.]\n",
      " [ 70.]\n",
      " [ 58.]\n",
      " [ 78.]\n",
      " [ 68.]\n",
      " [ 58.]\n",
      " [106.]\n",
      " [100.]\n",
      " [ 82.]\n",
      " [ 70.]\n",
      " [ 86.]\n",
      " [ 60.]\n",
      " [ 52.]\n",
      " [ 58.]\n",
      " [ 56.]\n",
      " [ 76.]\n",
      " [ 64.]\n",
      " [ 80.]\n",
      " [ 82.]\n",
      " [ 74.]\n",
      " [ 64.]\n",
      " [ 50.]\n",
      " [ 74.]\n",
      " [ 82.]\n",
      " [ 80.]\n",
      " [114.]\n",
      " [ 70.]\n",
      " [ 68.]\n",
      " [ 60.]\n",
      " [ 90.]\n",
      " [ 74.]\n",
      " [  0.]\n",
      " [ 88.]\n",
      " [ 70.]\n",
      " [ 76.]\n",
      " [ 78.]\n",
      " [ 88.]\n",
      " [  0.]\n",
      " [ 76.]\n",
      " [ 80.]\n",
      " [  0.]\n",
      " [ 46.]\n",
      " [ 78.]\n",
      " [ 64.]\n",
      " [ 64.]\n",
      " [ 78.]\n",
      " [ 62.]\n",
      " [ 58.]\n",
      " [ 74.]\n",
      " [ 50.]\n",
      " [ 78.]\n",
      " [ 72.]\n",
      " [ 60.]\n",
      " [ 76.]\n",
      " [ 86.]\n",
      " [ 66.]\n",
      " [ 68.]\n",
      " [ 86.]\n",
      " [ 94.]\n",
      " [ 78.]\n",
      " [ 78.]\n",
      " [ 84.]\n",
      " [ 88.]\n",
      " [ 52.]\n",
      " [ 78.]\n",
      " [ 86.]\n",
      " [ 88.]\n",
      " [ 56.]\n",
      " [ 75.]\n",
      " [ 60.]\n",
      " [ 86.]\n",
      " [ 72.]\n",
      " [ 60.]\n",
      " [ 74.]\n",
      " [ 80.]\n",
      " [ 44.]\n",
      " [ 58.]\n",
      " [ 94.]\n",
      " [ 88.]\n",
      " [ 84.]\n",
      " [ 94.]\n",
      " [ 74.]\n",
      " [ 70.]\n",
      " [ 62.]\n",
      " [ 70.]\n",
      " [ 78.]\n",
      " [ 62.]\n",
      " [ 88.]\n",
      " [ 78.]\n",
      " [ 88.]\n",
      " [ 90.]\n",
      " [ 72.]\n",
      " [ 76.]\n",
      " [ 92.]\n",
      " [ 58.]\n",
      " [ 74.]\n",
      " [ 62.]\n",
      " [ 76.]\n",
      " [ 70.]\n",
      " [ 72.]\n",
      " [ 60.]\n",
      " [ 70.]]\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "poly.feature_names_in_=Xy.iloc[:, [*feats_selected]].columns\n",
    "interaction_terms = poly.fit_transform(\n",
    "    X=Xy.iloc[:, [*feats_selected]],\n",
    "    y=Xy.loc[:, 'class']\n",
    ")\n",
    "print(poly.get_feature_names_out())\n",
    "print(interaction_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_interactions = GroupStructure.detector_interactions(Xy, feats_selected)\n",
    "population_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[[2], 0]], [1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population = GroupStructure.detector_monotonicity(Xy, population_interactions)\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "ranks_nds: (1, 0, 0, 0)\n",
      "os_1 {'total_layers': 6, 'nodes_per_hidden_layer': 4, 'group_structure': <classes.GroupStructure object at 0x00000065C577D150>} ([3, 5], [[[7, 1, 4, 0], 0], [[6, 2], -1]])\n",
      "os_2 {'total_layers': 4, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C584E580>} ([1, 0], [[[3, 4, 5], -1], [[2, 6, 7], 1]])\n",
      "os_3 {'total_layers': 7, 'nodes_per_hidden_layer': 4, 'group_structure': <classes.GroupStructure object at 0x00000065C5930390>} ([3], [[[0, 1, 2, 4, 5, 6, 7], 1]])\n",
      "os_4 {'total_layers': 7, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C581A750>} ([2, 3, 4, 5, 7], [[[0, 1, 6], 0]])\n"
     ]
    }
   ],
   "source": [
    "gs_1 = GroupStructure(\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "    [1, 1, 1, 1, 1, 1, 1, -1],\n",
    "    [0, 1],\n",
    "    [[2, 5], 1],\n",
    "    [[4], 0],\n",
    "    [[7, 3, 6], 1]\n",
    ")\n",
    "\n",
    "gs_2 = GroupStructure(\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "    [1, 1, 1, 1, 1, 1, -1, -1],\n",
    "    [0],\n",
    "    [[1, 2, 3, 4, 5], 1],\n",
    "    [[6, 7], 1]\n",
    ")\n",
    "\n",
    "gs_3 = GroupStructure(\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7]\n",
    ")\n",
    "\n",
    "gs_4 = GroupStructure(\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [],\n",
    "    [[0, 1, 2, 3, 4, 5, 6, 7], 1]\n",
    ")\n",
    "\n",
    "tmp = Dataset(\n",
    "    X=Xy.loc[:, Xy.columns != 'class'],\n",
    "    y=Xy.loc[:, 'class'],\n",
    "    class_pos='tested_positive',\n",
    "    group_structure=gs_1\n",
    ")\n",
    "len(tmp)\n",
    "tmp[2]\n",
    "\n",
    "gs_1.get_unconstrained_features()\n",
    "\n",
    "print(gs_1.get_feature_signs()[0])\n",
    "print(gs_1.get_feature_signs()[2])\n",
    "print(gs_1.get_feature_signs()[6])\n",
    "\n",
    "population = [\n",
    "    {'total_layers': 3, 'nodes_per_hidden_layer': 14, 'group_structure': gs_1, 'metrics': {'mean': (0.75, 0.15, 0.5, 0.4)}},\n",
    "    {'total_layers': 5, 'nodes_per_hidden_layer': 5, 'group_structure': gs_2, 'metrics': {'mean': (0.8, 0.15, 0.5, 0.4)}},\n",
    "    {'total_layers': 7, 'nodes_per_hidden_layer': 3, 'group_structure': gs_3, 'metrics': {'mean': (0.6, 0.27, 0.4, 0.6)}},\n",
    "    {'total_layers': 7, 'nodes_per_hidden_layer': 3, 'group_structure': gs_4, 'metrics': {'mean': (0.6, 0.27, 0.4, 0.6)}}\n",
    "]\n",
    "ranks_nds = ndomsort.non_domin_sort(\n",
    "    [individual['metrics']['mean'] for individual in population],\n",
    "    get_objectives=lambda elem: (1 - elem[0], *[elem[i] for i in range(1, len(elem))]),\n",
    "    only_front_indices=True\n",
    ")\n",
    "print(f'ranks_nds: {ranks_nds}')\n",
    "hp_bounds = {\n",
    "    'total_layers': (3, 10),\n",
    "    'nodes_per_hidden_layer': (3, 20)\n",
    "}\n",
    "\n",
    "os_1, os_2 = generate_offspring(2, population[:2], ranks_nds, hp_bounds)# GroupStructure.gga_crossover(gs_1, gs_2)\n",
    "print('os_1', os_1, os_1['group_structure'])\n",
    "print('os_2', os_2, os_2['group_structure'])\n",
    "\n",
    "os_3, os_4 = generate_offspring(2, population[2:], ranks_nds, hp_bounds)# GroupStructure.gga_crossover(gs_3, gs_4)\n",
    "print('os_3', os_3, os_3['group_structure'])\n",
    "print('os_4', os_4, os_4['group_structure'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial population\n",
      "total layers 4, nodes_per_hidden_layer 3, gs: ([0, 1, 2, 5, 6, 7], [[[3, 4], 0]])\n",
      "total layers 3, nodes_per_hidden_layer 3, gs: ([0, 2, 3, 4, 5, 6, 7], [[[1], 1]])\n",
      "total layers 6, nodes_per_hidden_layer 3, gs: ([1, 2, 3, 4, 6, 7], [[[0, 5], 0]])\n",
      "Evolution 1, evaluate 3 individuals\n",
      "running HPO for individual 1/3: 4 total_layers, 3 nodes per hidden layer\n",
      "fold 1/5 | (0.5296434494195689, 0.25, 0.03571428571428571, 0.25)\n",
      "fold 2/5 | (0.47346600331674965, 0.25, 0.03571428571428571, 0.25)\n",
      "fold 3/5 | (0.5, 0.25, 0.03571428571428571, 0.25)\n",
      "fold 4/5 | (0.6574074074074074, 0.25, 0.03571428571428571, 0.25)\n",
      "fold 5/5 | (0.5, 0.25, 0.03571428571428571, 0.25)\n",
      "running HPO for individual 2/3: 3 total_layers, 3 nodes per hidden layer\n",
      "fold 1/5 | (0.5, 0.125, 0.0, 0.0)\n",
      "fold 2/5 | (0.8009950248756219, 0.125, 0.0, 0.0)\n",
      "fold 3/5 | (0.19907407407407407, 0.125, 0.0, 0.0)\n",
      "fold 4/5 | (0.26557239057239057, 0.125, 0.0, 0.0)\n",
      "fold 5/5 | (0.2260127931769723, 0.125, 0.0, 0.0)\n",
      "running HPO for individual 3/3: 6 total_layers, 3 nodes per hidden layer\n",
      "fold 1/5 | (0.7064676616915422, 0.25, 0.03571428571428571, 0.25)\n",
      "fold 2/5 | (0.43946932006633493, 0.25, 0.03571428571428571, 0.25)\n",
      "fold 3/5 | (0.4722222222222222, 0.25, 0.03571428571428571, 0.25)\n",
      "fold 4/5 | (0.5890151515151514, 0.25, 0.03571428571428571, 0.25)\n",
      "fold 5/5 | (0.6942430703624733, 0.25, 0.03571428571428571, 0.25)\n",
      "population: [{'total_layers': 3, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C58AEB10>, 'metrics': {'mean': array([0.39833086, 0.125     , 0.        , 0.        ]), 'var': array([0.05189933, 0.        , 0.        , 0.        ]), 'folds': [(0.5, 0.125, 0.0, 0.0), (0.8009950248756219, 0.125, 0.0, 0.0), (0.19907407407407407, 0.125, 0.0, 0.0), (0.26557239057239057, 0.125, 0.0, 0.0), (0.2260127931769723, 0.125, 0.0, 0.0)]}, 'rank_nds': 0}, {'total_layers': 6, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C58D7550>, 'metrics': {'mean': array([0.58028349, 0.25      , 0.03571429, 0.25      ]), 'var': array([0.01209827, 0.        , 0.        , 0.        ]), 'folds': [(0.7064676616915422, 0.25, 0.03571428571428571, 0.25), (0.43946932006633493, 0.25, 0.03571428571428571, 0.25), (0.4722222222222222, 0.25, 0.03571428571428571, 0.25), (0.5890151515151514, 0.25, 0.03571428571428571, 0.25), (0.6942430703624733, 0.25, 0.03571428571428571, 0.25)]}, 'rank_nds': 0}, {'total_layers': 4, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C58C4730>, 'metrics': {'mean': array([0.53210337, 0.25      , 0.03571429, 0.25      ]), 'var': array([0.00424135, 0.        , 0.        , 0.        ]), 'folds': [(0.5296434494195689, 0.25, 0.03571428571428571, 0.25), (0.47346600331674965, 0.25, 0.03571428571428571, 0.25), (0.5, 0.25, 0.03571428571428571, 0.25), (0.6574074074074074, 0.25, 0.03571428571428571, 0.25), (0.5, 0.25, 0.03571428571428571, 0.25)]}, 'rank_nds': 1}]\n",
      "{'total_layers': 6, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C58972A0>, 'rank_nds': 0} ([5, 2, 3, 4, 6], [[[0, 1, 7], -1]])\n",
      "{'total_layers': 5, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C5896E40>, 'rank_nds': 0} ([5, 1, 4, 6, 7], [[[0, 2, 3], -1]])\n",
      "Evolution 2, evaluate 2 individuals\n",
      "running HPO for individual 1/2: 6 total_layers, 3 nodes per hidden layer\n",
      "fold 1/5 | (0.5, 0.375, 0.10714285714285714, 0.0)\n",
      "fold 2/5 | (0.5, 0.375, 0.10714285714285714, 0.0)\n",
      "fold 3/5 | (0.8017676767676768, 0.375, 0.10714285714285714, 0.0)\n",
      "fold 4/5 | (0.5, 0.375, 0.10714285714285714, 0.0)\n",
      "fold 5/5 | (0.5074626865671642, 0.375, 0.10714285714285714, 0.0)\n",
      "running HPO for individual 2/2: 5 total_layers, 3 nodes per hidden layer\n",
      "fold 1/5 | (0.5232172470978441, 0.375, 0.10714285714285714, 0.0)\n",
      "fold 2/5 | (0.5128524046434495, 0.375, 0.10714285714285714, 0.0)\n",
      "fold 3/5 | (0.5128367003367004, 0.375, 0.10714285714285714, 0.0)\n",
      "fold 4/5 | (0.4288720538720539, 0.375, 0.10714285714285714, 0.0)\n",
      "fold 5/5 | (0.5070362473347548, 0.375, 0.10714285714285714, 0.0)\n",
      "population: [{'total_layers': 3, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C58AEB10>, 'metrics': {'mean': array([0.39833086, 0.125     , 0.        , 0.        ]), 'var': array([0.05189933, 0.        , 0.        , 0.        ]), 'folds': [(0.5, 0.125, 0.0, 0.0), (0.8009950248756219, 0.125, 0.0, 0.0), (0.19907407407407407, 0.125, 0.0, 0.0), (0.26557239057239057, 0.125, 0.0, 0.0), (0.2260127931769723, 0.125, 0.0, 0.0)]}, 'rank_nds': 0}, {'total_layers': 6, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C58D7550>, 'metrics': {'mean': array([0.58028349, 0.25      , 0.03571429, 0.25      ]), 'var': array([0.01209827, 0.        , 0.        , 0.        ]), 'folds': [(0.7064676616915422, 0.25, 0.03571428571428571, 0.25), (0.43946932006633493, 0.25, 0.03571428571428571, 0.25), (0.4722222222222222, 0.25, 0.03571428571428571, 0.25), (0.5890151515151514, 0.25, 0.03571428571428571, 0.25), (0.6942430703624733, 0.25, 0.03571428571428571, 0.25)]}, 'rank_nds': 0}, {'total_layers': 6, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C58972A0>, 'rank_nds': 0, 'metrics': {'mean': array([0.56184607, 0.375     , 0.10714286, 0.        ]), 'var': array([0.01439895, 0.        , 0.        , 0.        ]), 'folds': [(0.5, 0.375, 0.10714285714285714, 0.0), (0.5, 0.375, 0.10714285714285714, 0.0), (0.8017676767676768, 0.375, 0.10714285714285714, 0.0), (0.5, 0.375, 0.10714285714285714, 0.0), (0.5074626865671642, 0.375, 0.10714285714285714, 0.0)]}}]\n",
      "{'total_layers': 4, 'nodes_per_hidden_layer': 4, 'group_structure': <classes.GroupStructure object at 0x00000065CDE372A0>, 'rank_nds': 0} ([3, 4, 7], [[[1, 0, 2, 5, 6], 1]])\n",
      "{'total_layers': 3, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065CDE370E0>, 'rank_nds': 0} ([1, 0, 2, 7], [[[3, 4, 5, 6], 0]])\n",
      "Evolution 3, evaluate 2 individuals\n",
      "running HPO for individual 1/2: 4 total_layers, 4 nodes per hidden layer\n",
      "fold 1/5 | (0.3018242122719734, 0.625, 0.35714285714285715, 0.0)\n",
      "fold 2/5 | (0.7591210613598673, 0.625, 0.35714285714285715, 0.0)\n",
      "fold 3/5 | (0.8000841750841751, 0.625, 0.35714285714285715, 0.0)\n",
      "fold 4/5 | (0.6561447811447811, 0.625, 0.35714285714285715, 0.0)\n",
      "fold 5/5 | (0.3556503198294243, 0.625, 0.35714285714285715, 0.0)\n",
      "running HPO for individual 2/2: 3 total_layers, 3 nodes per hidden layer\n",
      "fold 1/5 | (0.5783582089552239, 0.5, 0.21428571428571427, 0.5)\n",
      "fold 2/5 | (0.5269485903814262, 0.5, 0.21428571428571427, 0.5)\n",
      "fold 3/5 | (0.6079545454545454, 0.5, 0.21428571428571427, 0.5)\n",
      "fold 4/5 | (0.6708754208754208, 0.5, 0.21428571428571427, 0.5)\n",
      "fold 5/5 | (0.43624733475479743, 0.5, 0.21428571428571427, 0.5)\n",
      "population: [{'total_layers': 3, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C58AEB10>, 'metrics': {'mean': array([0.39833086, 0.125     , 0.        , 0.        ]), 'var': array([0.05189933, 0.        , 0.        , 0.        ]), 'folds': [(0.5, 0.125, 0.0, 0.0), (0.8009950248756219, 0.125, 0.0, 0.0), (0.19907407407407407, 0.125, 0.0, 0.0), (0.26557239057239057, 0.125, 0.0, 0.0), (0.2260127931769723, 0.125, 0.0, 0.0)]}, 'rank_nds': 0}, {'total_layers': 6, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C58D7550>, 'metrics': {'mean': array([0.58028349, 0.25      , 0.03571429, 0.25      ]), 'var': array([0.01209827, 0.        , 0.        , 0.        ]), 'folds': [(0.7064676616915422, 0.25, 0.03571428571428571, 0.25), (0.43946932006633493, 0.25, 0.03571428571428571, 0.25), (0.4722222222222222, 0.25, 0.03571428571428571, 0.25), (0.5890151515151514, 0.25, 0.03571428571428571, 0.25), (0.6942430703624733, 0.25, 0.03571428571428571, 0.25)]}, 'rank_nds': 0}, {'total_layers': 6, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C58972A0>, 'rank_nds': 0, 'metrics': {'mean': array([0.56184607, 0.375     , 0.10714286, 0.        ]), 'var': array([0.01439895, 0.        , 0.        , 0.        ]), 'folds': [(0.5, 0.375, 0.10714285714285714, 0.0), (0.5, 0.375, 0.10714285714285714, 0.0), (0.8017676767676768, 0.375, 0.10714285714285714, 0.0), (0.5, 0.375, 0.10714285714285714, 0.0), (0.5074626865671642, 0.375, 0.10714285714285714, 0.0)]}}]\n",
      "{'total_layers': 5, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065C5896F90>, 'rank_nds': 0} ([0, 4], [[[1, 7, 2, 3, 5, 6], -1]])\n",
      "{'total_layers': 3, 'nodes_per_hidden_layer': 3, 'group_structure': <classes.GroupStructure object at 0x00000065CDE37150>, 'rank_nds': 0} ([1, 6, 7], [[[0, 2, 3, 4, 5], 1]])\n"
     ]
    }
   ],
   "source": [
    "# outer\n",
    "data_train_test, data_val = train_test_split(\n",
    "    Xy,\n",
    "    train_size=2/3,\n",
    "    shuffle=True,\n",
    "    stratify=Xy.loc[:, 'class']\n",
    ")\n",
    "\n",
    "# reset indices as StratifiedKFold assumes consecutive index\n",
    "data_train_test = data_train_test.reset_index(drop=True)\n",
    "data_val = data_val.reset_index(drop=True)\n",
    "\n",
    "# inner\n",
    "cv_inner = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=False  # TODO: set to True\n",
    ")\n",
    "\n",
    "'''\n",
    "- for each individual in the configuration, run k folds + average its performance\n",
    "    -> in each fold, additionally split the training data, train on larger split, use smaller split to determine early stopping\n",
    "    -> average early stopping epoch over all folds, report back with average performance\n",
    "- find pareto front\n",
    "- evaluate pareto front's performance on holdout test set, each model of the front is trained for the average of the epochs determined by early stopping in CV\n",
    "'''\n",
    "mu = 3  # TODO: set to 100\n",
    "la = 2  # TODO: set to 10\n",
    "monotonicity_clipper = WeightClipper(0, None)  # enforce monotonicity by clipping weights to [0, infty) after each epoch (in def train)\n",
    "run_eagga_cv(mu, la, cv_inner, data_train_test, categorical_indicator, epochs=10, batch_size=8, weight_clipper=monotonicity_clipper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
