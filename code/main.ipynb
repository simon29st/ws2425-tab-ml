{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import tasks\n",
    "\n",
    "from classes import EAGGA\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\openml\\tasks\\functions.py:372: UserWarning: `download_data` will default to False starting in 0.16. Please set `download_data` explicitly to suppress this warning.\n",
      "  warnings.warn(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\openml\\tasks\\functions.py:380: UserWarning: `download_qualities` will default to False starting in 0.16. Please set `download_qualities` explicitly to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "oml_task_ids = [37, 43, 3903, 3904, 3913, 3918, 10093, 9946, 146819, 359955, 189922, 359962, 190392, 167120, 190137, 190410, 168350, 359975, 359972, 146820]\n",
    "oml_tasks = tasks.get_tasks(oml_task_ids)\n",
    "\n",
    "oml_datasets = [oml_task.get_dataset() for oml_task in oml_tasks]\n",
    "\n",
    "# define positive classes\n",
    "positive_classes = ['tested_positive', '1', True, True, 'yes', True, '2', '2', '1', '2', '1', True, '1', '1', '2', '1', '2', 'Anomaly', '1', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset diabetes\n",
      "Starting init population\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished init population\n",
      "Start EAGGA at 2025-02-28T17:37:11.703645\n",
      "Generation 8, evaluate 5 individuals\n",
      "Running 5-fold CV for individual: 3 total layers, 5 nodes per hidden layer, gs: ([5, 0], [[[7, 3, 6, 2], 0], [[4, 1], 0]])\n",
      "Stop early: 0.6037228271365165 < 0.6094867338736852, epoch stop: 20\n",
      "Fold 1/5 | trained for 20 epochs | metrics: {'loss': 0.6114453673362732, 'auc': np.float64(0.650497512437811), 'nf': 0.75, 'ni': 0.25, 'nnm': 0.75}\n",
      "Stop early: 0.6040599187215169 < 0.6054402391115824, epoch stop: 14\n",
      "Fold 2/5 | trained for 14 epochs | metrics: {'loss': 0.6966107445103782, 'auc': np.float64(0.5688225538971806), 'nf': 0.75, 'ni': 0.25, 'nnm': 0.75}\n",
      "Stop early: 0.6958878974119823 < 0.6970799714326859, epoch stop: 34\n",
      "Fold 3/5 | trained for 34 epochs | metrics: {'loss': 0.6241407138960702, 'auc': np.float64(0.6857142857142857), 'nf': 0.75, 'ni': 0.25, 'nnm': 0.75}\n",
      "Stop early: 0.632022702197234 < 0.632324347893397, epoch stop: 25\n",
      "Fold 4/5 | trained for 25 epochs | metrics: {'loss': 0.6107225928987775, 'auc': np.float64(0.6388888888888888), 'nf': 0.75, 'ni': 0.25, 'nnm': 0.75}\n",
      "Stop early: 0.7956493268410365 < 0.7963051994641622, epoch stop: 40\n",
      "Fold 5/5 | trained for 40 epochs | metrics: {'loss': 0.6417015876088824, 'auc': np.float64(0.5728114478114478), 'nf': 0.75, 'ni': 0.25, 'nnm': 0.75}\n",
      "Running 5-fold CV for individual: 4 total layers, 10 nodes per hidden layer, gs: ([1, 3, 2, 7], [[[0, 5, 4, 6], 0]])\n",
      "Stop early: 0.6089178924759229 < 0.6091157694657644, epoch stop: 24\n",
      "Fold 1/5 | trained for 24 epochs | metrics: {'loss': 0.6437316622052874, 'auc': np.float64(0.6090381426202323), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.6535054763158163 < 0.6601778964201609, epoch stop: 17\n",
      "Fold 2/5 | trained for 17 epochs | metrics: {'loss': 0.649489666734423, 'auc': np.float64(0.5232172470978441), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.6287729024887084 < 0.6332945028940836, epoch stop: 44\n",
      "Fold 3/5 | trained for 44 epochs | metrics: {'loss': 0.6888153042112078, 'auc': np.float64(0.646908315565032), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.6046109730998674 < 0.6127835462490717, epoch stop: 35\n",
      "Fold 4/5 | trained for 35 epochs | metrics: {'loss': 0.5908078380993435, 'auc': np.float64(0.6885521885521886), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.6199109544356664 < 0.6264625241359075, epoch stop: 17\n",
      "Fold 5/5 | trained for 17 epochs | metrics: {'loss': 0.6088208428450993, 'auc': np.float64(0.6207912457912458), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Running 5-fold CV for individual: 3 total layers, 5 nodes per hidden layer, gs: ([0], [[[6, 2], 1], [[3], 0], [[5, 4], 1], [[7, 1], 0]])\n",
      "Stop early: 0.6413676699002583 < 0.6475001374880472, epoch stop: 15\n",
      "Fold 1/5 | trained for 15 epochs | metrics: {'loss': 0.6334929381098066, 'auc': np.float64(0.5381426202321724), 'nf': 0.875, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Stop early: 0.6316876510779064 < 0.6373136639595032, epoch stop: 3\n",
      "Fold 2/5 | trained for 3 epochs | metrics: {'loss': 0.6432153327124459, 'auc': np.float64(0.5269485903814263), 'nf': 0.875, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Stop early: 0.6053251609206199 < 0.6176114082336426, epoch stop: 26\n",
      "Fold 3/5 | trained for 26 epochs | metrics: {'loss': 0.6321078496319907, 'auc': np.float64(0.5471215351812367), 'nf': 0.875, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Stop early: 0.5866678784290951 < 0.5918315201997757, epoch stop: 21\n",
      "Fold 4/5 | trained for 21 epochs | metrics: {'loss': 0.6310796907969883, 'auc': np.float64(0.6738215488215488), 'nf': 0.875, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Stop early: 0.6534073303143183 < 0.6645328303178152, epoch stop: 15\n",
      "Fold 5/5 | trained for 15 epochs | metrics: {'loss': 0.6310745818274361, 'auc': np.float64(0.7133838383838385), 'nf': 0.875, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Running 5-fold CV for individual: 4 total layers, 8 nodes per hidden layer, gs: ([6, 3, 2], [[[7, 5, 1, 0, 4], 0]])\n",
      "Stop early: 0.6602022657791773 < 0.6610474586486816, epoch stop: 21\n",
      "Fold 1/5 | trained for 21 epochs | metrics: {'loss': 0.648924205984388, 'auc': np.float64(0.5572139303482587), 'nf': 0.625, 'ni': 0.35714285714285715, 'nnm': 0.625}\n",
      "Stop early: 0.5890428880850475 < 0.589572936296463, epoch stop: 33\n",
      "Fold 2/5 | trained for 33 epochs | metrics: {'loss': 0.6394788708005633, 'auc': np.float64(0.6417910447761194), 'nf': 0.625, 'ni': 0.35714285714285715, 'nnm': 0.625}\n",
      "Stop early: 0.6474834943811099 < 0.6772364675998688, epoch stop: 13\n",
      "Fold 3/5 | trained for 13 epochs | metrics: {'loss': 0.6682094591004508, 'auc': np.float64(0.520682302771855), 'nf': 0.625, 'ni': 0.35714285714285715, 'nnm': 0.625}\n",
      "Stop early: 0.5825231085220974 < 0.5840896169344584, epoch stop: 53\n",
      "Fold 4/5 | trained for 53 epochs | metrics: {'loss': 0.6563016091074262, 'auc': np.float64(0.5963804713804713), 'nf': 0.625, 'ni': 0.35714285714285715, 'nnm': 0.625}\n",
      "Stop early: 0.653278257449468 < 0.6579762895901998, epoch stop: 48\n",
      "Fold 5/5 | trained for 48 epochs | metrics: {'loss': 0.6292239086968558, 'auc': np.float64(0.6826599326599327), 'nf': 0.625, 'ni': 0.35714285714285715, 'nnm': 0.625}\n",
      "Running 5-fold CV for individual: 3 total layers, 3 nodes per hidden layer, gs: ([0, 1, 2, 3, 4, 5, 6, 7], [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     22\u001b[0m eagga \u001b[39m=\u001b[39m EAGGA(\n\u001b[0;32m     23\u001b[0m     oml_dataset\u001b[39m=\u001b[39moml_dataset,\n\u001b[0;32m     24\u001b[0m     class_positive\u001b[39m=\u001b[39mclass_positive,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     file_path\u001b[39m=\u001b[39mfile_path\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m eagga\u001b[39m.\u001b[39mload_population(\u001b[39m6\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m eagga\u001b[39m.\u001b[39;49mrun_eagga()\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:583\u001b[0m, in \u001b[0;36mEAGGA.run_eagga\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGeneration \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, evaluate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffspring)\u001b[39m}\u001b[39;00m\u001b[39m individuals\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    582\u001b[0m \u001b[39mfor\u001b[39;00m individual \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffspring:\n\u001b[1;32m--> 583\u001b[0m     individual[\u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_cv(individual)\n\u001b[0;32m    584\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation\u001b[39m.\u001b[39mappend(individual)\n\u001b[0;32m    586\u001b[0m     \u001b[39mif\u001b[39;00m datetime\u001b[39m.\u001b[39mnow() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m time_start \u001b[39m+\u001b[39m timedelta(seconds\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msecs_total):\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:664\u001b[0m, in \u001b[0;36mEAGGA.run_cv\u001b[1;34m(self, individual)\u001b[0m\n\u001b[0;32m    661\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m    662\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m--> 664\u001b[0m model, stop_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(optimizer, loss_fn, model, dataset_train, dataset_stop_early)\n\u001b[0;32m    666\u001b[0m metrics[\u001b[39m'\u001b[39m\u001b[39mperformance\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval(loss_fn, model, dataset_val))\n\u001b[0;32m    667\u001b[0m metrics[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(stop_epoch)\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:697\u001b[0m, in \u001b[0;36mEAGGA.train\u001b[1;34m(self, optimizer, loss_fn, model, dataset_train, dataset_stop_early)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[39mfor\u001b[39;00m batch_input, batch_target \u001b[39min\u001b[39;00m loader_train:  \u001b[39m# divide data in mini batches\u001b[39;00m\n\u001b[0;32m    696\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# set gradients to 0\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m     batch_output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49mbatch_input)\u001b[39m.\u001b[39mflatten()  \u001b[39m# expand batch_input as it is a list of tuples (Dataset getter splits according to group structure)\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     batch_loss \u001b[39m=\u001b[39m loss_fn(batch_output, batch_target)\n\u001b[0;32m    700\u001b[0m     running_epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:61\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, *xs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m i, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(xs):\n\u001b[0;32m     60\u001b[0m     output_networks\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetworks[i](x))\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_out(torch\u001b[39m.\u001b[39;49mcat(output_networks, \u001b[39m1\u001b[39;49m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "hps = {\n",
    "    'total_layers': (3, 10),\n",
    "    'nodes_per_hidden_layer': (3, 20),\n",
    "    'mu': 15,  # TODO: 100\n",
    "    'lambda': 5,  # TODO: 10\n",
    "    'holdout_train_size': 2/3,\n",
    "    'cv_k': 5\n",
    "}\n",
    "\n",
    "batch_size = 16\n",
    "patience = 10\n",
    "\n",
    "secs_per_fold = 30\n",
    "secs_total = 20 * 60\n",
    "\n",
    "for (oml_dataset, class_positive) in zip(oml_datasets[:1], positive_classes[:1]):  # TODO: remove [:1]\n",
    "    name = oml_dataset.name\n",
    "    print(f'Dataset {name}')\n",
    "\n",
    "    file_path = os.path.join('export', name)\n",
    "    \n",
    "    eagga = EAGGA(\n",
    "        oml_dataset=oml_dataset,\n",
    "        class_positive=class_positive,\n",
    "        hps=hps,\n",
    "        batch_size=batch_size,\n",
    "        patience=patience,\n",
    "        secs_per_fold=secs_per_fold,\n",
    "        secs_total=secs_total,\n",
    "        file_path=file_path\n",
    "    )\n",
    "    eagga.load_population(6)\n",
    "    eagga.run_eagga()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
