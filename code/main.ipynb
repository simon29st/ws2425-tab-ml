{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%pip install setuptools==70.3.0  # apparently setuptools < 71.0.0 is required for a required package of openml to install on here\\n%pip install openml\\n%pip install numpy\\n%pip install pandas\\n# cf. https://pytorch.org/get-started/locally/\\n#%pip install torch torchvision torchaudio  # cuda 12.4, cf. nvidia-smi shell command\\n%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu  # cpu version\\n%pip install -U scikit-learn\\n%pip install scipy\\n%pip install -U pymoo\\n%pip list'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%pip install setuptools==70.3.0  # apparently setuptools < 71.0.0 is required for a required package of openml to install on here\n",
    "%pip install openml\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "# cf. https://pytorch.org/get-started/locally/\n",
    "#%pip install torch torchvision torchaudio  # cuda 12.4, cf. nvidia-smi shell command\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu  # cpu version\n",
    "%pip install -U scikit-learn\n",
    "%pip install scipy\n",
    "%pip install -U pymoo\n",
    "%pip list'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import tasks\n",
    "\n",
    "from classes import EAGGA\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\openml\\tasks\\functions.py:372: UserWarning: `download_data` will default to False starting in 0.16. Please set `download_data` explicitly to suppress this warning.\n",
      "  warnings.warn(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\openml\\tasks\\functions.py:380: UserWarning: `download_qualities` will default to False starting in 0.16. Please set `download_qualities` explicitly to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "oml_task_ids = [37, 43, 3903, 3904, 3913, 3918, 10093, 9946, 146819, 359955, 189922, 359962, 190392, 167120, 190137, 190410, 168350, 359975, 359972, 146820]\n",
    "oml_tasks = tasks.get_tasks(oml_task_ids)\n",
    "\n",
    "oml_datasets = [oml_task.get_dataset() for oml_task in oml_tasks]\n",
    "\n",
    "# define positive classes\n",
    "positive_classes = ['tested_positive', '1', True, True, 'yes', True, '2', '2', '1', '2', '1', True, '1', '1', '2', '1', '2', 'Anomaly', '1', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset diabetes\n",
      "Starting init population\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished init population\n",
      "Start EAGGA at 2025-03-03T18:59:37.261326\n",
      "Generation 1, evaluate 5 individuals\n",
      "Running 5-fold CV for individual 1/5: 3 total layers, 4 nodes per hidden layer, dropout p 0.47, gs: ([0, 5, 6], [[[1, 7], 0], [[3], 0], [[2], 0], [[4], 1]])\n",
      "Fold 1/5 | trained for 114 epochs / 10.011 seconds | stopped early: False | metrics: {'loss': 0.6611815989017487, 'auc': np.float64(0.35966003316749584), 'nf': 0.625, 'ni': 0.03571428571428571, 'nnm': 0.5}\n",
      "Fold 2/5 | trained for 105 epochs / 10.046 seconds | stopped early: False | metrics: {'loss': 0.6556365787982941, 'auc': np.float64(0.42330016583747926), 'nf': 0.625, 'ni': 0.03571428571428571, 'nnm': 0.5}\n",
      "Fold 3/5 | trained for 104 epochs / 10.05 seconds | stopped early: False | metrics: {'loss': 0.6397682428359985, 'auc': np.float64(0.554503367003367), 'nf': 0.625, 'ni': 0.03571428571428571, 'nnm': 0.5}\n",
      "Fold 4/5 | trained for 86 epochs / 9.306 seconds | stopped early: True | metrics: {'loss': 0.6457957625389099, 'auc': np.float64(0.6018518518518519), 'nf': 0.625, 'ni': 0.03571428571428571, 'nnm': 0.5}\n",
      "Fold 5/5 | trained for 127 epochs / 10.036 seconds | stopped early: False | metrics: {'loss': 0.6415168642997742, 'auc': np.float64(0.4639658848614072), 'nf': 0.625, 'ni': 0.03571428571428571, 'nnm': 0.5}\n",
      "Running 5-fold CV for individual 2/5: 5 total layers, 3 nodes per hidden layer, dropout p 0.5, gs: ([0, 1, 2, 3, 4, 5, 6], [[[7], 0]])\n",
      "Fold 1/5 | trained for 149 epochs / 6.641 seconds | stopped early: True | metrics: {'loss': 0.6405217051506042, 'auc': np.float64(0.5), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 121 epochs / 5.468 seconds | stopped early: True | metrics: {'loss': 0.6439118981361389, 'auc': np.float64(0.5), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 301 epochs / 10.025 seconds | stopped early: False | metrics: {'loss': 0.6446927487850189, 'auc': np.float64(0.6439393939393939), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 302 epochs / 10.013 seconds | stopped early: False | metrics: {'loss': 0.6411583721637726, 'auc': np.float64(0.5), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 93 epochs / 5.0 seconds | stopped early: True | metrics: {'loss': 0.6411842107772827, 'auc': np.float64(0.5), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 3/5: 3 total layers, 3 nodes per hidden layer, dropout p 0.46, gs: ([1, 2, 3, 4, 5, 6, 7], [[[0], 0]])\n",
      "Fold 1/5 | trained for 147 epochs / 5.333 seconds | stopped early: True | metrics: {'loss': 0.6527281701564789, 'auc': np.float64(0.5), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 20 epochs / 2.731 seconds | stopped early: True | metrics: {'loss': 0.6383607089519501, 'auc': np.float64(0.6432421227197347), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 104 epochs / 5.038 seconds | stopped early: True | metrics: {'loss': 0.6376412212848663, 'auc': np.float64(0.6245791245791246), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 30 epochs / 2.73 seconds | stopped early: True | metrics: {'loss': 0.6405200660228729, 'auc': np.float64(0.5138888888888888), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 213 epochs / 6.333 seconds | stopped early: True | metrics: {'loss': 0.6419308185577393, 'auc': np.float64(0.5488272921108741), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 4/5: 6 total layers, 6 nodes per hidden layer, dropout p 0.68, gs: ([2, 3, 6], [[[0, 1, 5], 0], [[7], 0], [[4], 1]])\n",
      "Fold 1/5 | trained for 107 epochs / 10.002 seconds | stopped early: False | metrics: {'loss': 0.6540129780769348, 'auc': np.float64(0.5211442786069652), 'nf': 0.625, 'ni': 0.10714285714285714, 'nnm': 0.5}\n",
      "Fold 2/5 | trained for 70 epochs / 10.009 seconds | stopped early: False | metrics: {'loss': 0.6467905044555664, 'auc': np.float64(0.5692371475953565), 'nf': 0.625, 'ni': 0.10714285714285714, 'nnm': 0.5}\n",
      "Fold 3/5 | trained for 112 epochs / 10.008 seconds | stopped early: False | metrics: {'loss': 0.6502375900745392, 'auc': np.float64(0.45580808080808083), 'nf': 0.625, 'ni': 0.10714285714285714, 'nnm': 0.5}\n",
      "Fold 4/5 | trained for 112 epochs / 10.032 seconds | stopped early: False | metrics: {'loss': 0.6473206579685211, 'auc': np.float64(0.510942760942761), 'nf': 0.625, 'ni': 0.10714285714285714, 'nnm': 0.5}\n",
      "Fold 5/5 | trained for 112 epochs / 10.084 seconds | stopped early: False | metrics: {'loss': 0.6399795114994049, 'auc': np.float64(0.39786780383795306), 'nf': 0.625, 'ni': 0.10714285714285714, 'nnm': 0.5}\n",
      "Running 5-fold CV for individual 5/5: 3 total layers, 4 nodes per hidden layer, dropout p 0.22, gs: ([0, 1, 2, 3, 4, 5, 7], [[[6], 0]])\n",
      "Fold 1/5 | trained for 315 epochs / 9.237 seconds | stopped early: True | metrics: {'loss': 0.6333663463592529, 'auc': np.float64(0.5422885572139303), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 246 epochs / 7.146 seconds | stopped early: True | metrics: {'loss': 0.6253317594528198, 'auc': np.float64(0.585820895522388), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 196 epochs / 5.564 seconds | stopped early: True | metrics: {'loss': 0.6211974918842316, 'auc': np.float64(0.7889309764309764), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 247 epochs / 7.323 seconds | stopped early: True | metrics: {'loss': 0.6226188838481903, 'auc': np.float64(0.6087962962962963), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 23 epochs / 2.664 seconds | stopped early: True | metrics: {'loss': 0.6538828313350677, 'auc': np.float64(0.5266524520255864), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Dominated Hypervolume: 0.349609375 for Pareto front [[0.65039062 0.         0.         0.        ]]\n",
      "Autosaving generation 1 to export/diabetes\n",
      "Saved population + offspring + pareto front of generation 1 to file\n",
      "Generation 2, evaluate 3 individuals\n",
      "Running 5-fold CV for individual 1/3: 3 total layers, 4 nodes per hidden layer, dropout p 0.47, gs: ([0, 1, 3, 5, 7], [[[6], 0], [[2], 0], [[4], 1]])\n",
      "Fold 1/5 | trained for 106 epochs / 10.056 seconds | stopped early: False | metrics: {'loss': 0.6288722455501556, 'auc': np.float64(0.6598258706467661), 'nf': 0.375, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 2/5 | trained for 137 epochs / 10.034 seconds | stopped early: False | metrics: {'loss': 0.6392655372619629, 'auc': np.float64(0.5597014925373134), 'nf': 0.375, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 3/5 | trained for 2 epochs / 8.935 seconds | stopped early: True | metrics: {'loss': 0.6185246407985687, 'auc': np.float64(0.6094276094276094), 'nf': 0.375, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 4/5 | trained for 121 epochs / 9.321 seconds | stopped early: True | metrics: {'loss': 0.6369721293449402, 'auc': np.float64(0.6767676767676767), 'nf': 0.375, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 5/5 | trained for 152 epochs / 9.655 seconds | stopped early: True | metrics: {'loss': 0.671916276216507, 'auc': np.float64(0.38166311300639655), 'nf': 0.375, 'ni': 0.0, 'nnm': 0.25}\n",
      "Running 5-fold CV for individual 2/3: 3 total layers, 4 nodes per hidden layer, dropout p 0.3653618543129913, gs: ([0, 1, 2, 3, 4, 5, 7], [[[6], 0]])\n",
      "Fold 1/5 | trained for 363 epochs / 10.014 seconds | stopped early: False | metrics: {'loss': 0.6281656622886658, 'auc': np.float64(0.5443615257048093), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 233 epochs / 6.744 seconds | stopped early: True | metrics: {'loss': 0.6326243579387665, 'auc': np.float64(0.5930762852404644), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 231 epochs / 6.852 seconds | stopped early: True | metrics: {'loss': 0.6413240134716034, 'auc': np.float64(0.6224747474747474), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 185 epochs / 6.196 seconds | stopped early: True | metrics: {'loss': 0.6399725079536438, 'auc': np.float64(0.3449074074074074), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 303 epochs / 8.466 seconds | stopped early: True | metrics: {'loss': 0.6354388892650604, 'auc': np.float64(0.6093816631130065), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 3/3: 3 total layers, 3 nodes per hidden layer, dropout p 0.22, gs: ([1, 2, 3, 4, 5, 7], [[[6], 0], [[0], 0]])\n",
      "Fold 1/5 | trained for 131 epochs / 8.91 seconds | stopped early: True | metrics: {'loss': 0.6327775716781616, 'auc': np.float64(0.6053067993366502), 'nf': 0.25, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 2/5 | trained for 228 epochs / 10.002 seconds | stopped early: False | metrics: {'loss': 0.6089996099472046, 'auc': np.float64(0.712686567164179), 'nf': 0.25, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 3/5 | trained for 221 epochs / 10.006 seconds | stopped early: False | metrics: {'loss': 0.6162002980709076, 'auc': np.float64(0.688973063973064), 'nf': 0.25, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 4/5 | trained for 176 epochs / 10.013 seconds | stopped early: False | metrics: {'loss': 0.6263311207294464, 'auc': np.float64(0.626473063973064), 'nf': 0.25, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 5/5 | trained for 193 epochs / 10.044 seconds | stopped early: False | metrics: {'loss': 0.628692239522934, 'auc': np.float64(0.6432835820895523), 'nf': 0.25, 'ni': 0.0, 'nnm': 0.25}\n",
      "Dominated Hypervolume: 0.349609375 for Pareto front [[0.65534462 0.25       0.         0.25      ]\n",
      " [0.65039062 0.         0.         0.        ]]\n",
      "Autosaving generation 2 to export/diabetes\n",
      "Saved population + offspring + pareto front of generation 2 to file\n",
      "Generation 3, evaluate 3 individuals\n",
      "Running 5-fold CV for individual 1/3: 4 total layers, 3 nodes per hidden layer, dropout p 0.22, gs: ([0, 1, 2, 3, 4, 5, 7], [[[6], 0]])\n",
      "Fold 1/5 | trained for 329 epochs / 10.017 seconds | stopped early: False | metrics: {'loss': 0.6455162465572357, 'auc': np.float64(0.35717247097844107), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 29 epochs / 3.344 seconds | stopped early: True | metrics: {'loss': 0.6390525102615356, 'auc': np.float64(0.3922056384742952), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 241 epochs / 7.64 seconds | stopped early: True | metrics: {'loss': 0.6470743715763092, 'auc': np.float64(0.585016835016835), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 22 epochs / 2.957 seconds | stopped early: True | metrics: {'loss': 0.6411379277706146, 'auc': np.float64(0.5), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 313 epochs / 10.008 seconds | stopped early: False | metrics: {'loss': 0.6214867532253265, 'auc': np.float64(0.588272921108742), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 2/3: 3 total layers, 4 nodes per hidden layer, dropout p 0.22, gs: ([0, 1, 2, 4, 7], [[[6, 3, 5], 0]])\n",
      "Fold 1/5 | trained for 34 epochs / 3.347 seconds | stopped early: True | metrics: {'loss': 0.6411680281162262, 'auc': np.float64(0.558457711442786), 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Fold 2/5 | trained for 21 epochs / 3.745 seconds | stopped early: True | metrics: {'loss': 0.6412838697433472, 'auc': np.float64(0.5787728026533997), 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Fold 3/5 | trained for 39 epochs / 3.358 seconds | stopped early: True | metrics: {'loss': 0.657230406999588, 'auc': np.float64(0.36868686868686873), 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Fold 4/5 | trained for 18 epochs / 2.953 seconds | stopped early: True | metrics: {'loss': 0.6960915625095367, 'auc': np.float64(0.45286195286195285), 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Fold 5/5 | trained for 293 epochs / 10.011 seconds | stopped early: False | metrics: {'loss': 0.638141393661499, 'auc': np.float64(0.4422174840085288), 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Running 5-fold CV for individual 3/3: 3 total layers, 4 nodes per hidden layer, dropout p 0.22, gs: ([1, 2, 3, 4, 7], [[[6, 5], 1], [[0], 0]])\n",
      "Fold 1/5 | trained for 186 epochs / 10.036 seconds | stopped early: False | metrics: {'loss': 0.6226952373981476, 'auc': np.float64(0.8030679933665008), 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 148 epochs / 9.007 seconds | stopped early: True | metrics: {'loss': 0.615247517824173, 'auc': np.float64(0.7839966832504146), 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 185 epochs / 10.003 seconds | stopped early: False | metrics: {'loss': 0.6473171412944794, 'auc': np.float64(0.507996632996633), 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 189 epochs / 10.02 seconds | stopped early: False | metrics: {'loss': 0.6440390944480896, 'auc': np.float64(0.5761784511784511), 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 190 epochs / 10.017 seconds | stopped early: False | metrics: {'loss': 0.6406263411045074, 'auc': np.float64(0.5475479744136461), 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Dominated Hypervolume: 0.349609375 for Pareto front [[0.65534462 0.25       0.         0.25      ]\n",
      " [0.65039062 0.         0.         0.        ]]\n",
      "Autosaving generation 3 to export/diabetes\n",
      "Saved population + offspring + pareto front of generation 3 to file\n",
      "Generation 4, evaluate 3 individuals\n",
      "Running 5-fold CV for individual 1/3: 3 total layers, 4 nodes per hidden layer, dropout p 0.22, gs: ([0, 1, 2, 3, 4, 5, 7], [[[6], 0]])\n",
      "Fold 1/5 | trained for 223 epochs / 7.889 seconds | stopped early: True | metrics: {'loss': 0.6494924128055573, 'auc': np.float64(0.5563847429519071), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 266 epochs / 7.829 seconds | stopped early: True | metrics: {'loss': 0.6390527486801147, 'auc': np.float64(0.5), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 313 epochs / 9.574 seconds | stopped early: True | metrics: {'loss': 0.6265163421630859, 'auc': np.float64(0.6083754208754208), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 138 epochs / 4.661 seconds | stopped early: True | metrics: {'loss': 0.6316695809364319, 'auc': np.float64(0.623526936026936), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 117 epochs / 3.681 seconds | stopped early: True | metrics: {'loss': 0.616714745759964, 'auc': np.float64(0.6300639658848614), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 2/3: 3 total layers, 4 nodes per hidden layer, dropout p 0.22, gs: ([0, 1, 2, 3, 4, 5, 7], [[[6], 0]])\n",
      "Fold 1/5 | trained for 215 epochs / 6.784 seconds | stopped early: True | metrics: {'loss': 0.6284919083118439, 'auc': np.float64(0.6602404643449419), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 165 epochs / 5.187 seconds | stopped early: True | metrics: {'loss': 0.6450690925121307, 'auc': np.float64(0.6218905472636815), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 25 epochs / 2.748 seconds | stopped early: True | metrics: {'loss': 0.6492505371570587, 'auc': np.float64(0.477483164983165), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 320 epochs / 10.007 seconds | stopped early: False | metrics: {'loss': 0.6449951231479645, 'auc': np.float64(0.5555555555555555), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 91 epochs / 3.855 seconds | stopped early: True | metrics: {'loss': 0.6212806403636932, 'auc': np.float64(0.5859275053304904), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 3/3: 3 total layers, 4 nodes per hidden layer, dropout p 0.47, gs: ([0, 1, 2, 3, 4, 5, 7], [[[6], 0]])\n",
      "Fold 1/5 | trained for 135 epochs / 5.607 seconds | stopped early: True | metrics: {'loss': 0.6395091712474823, 'auc': np.float64(0.32980928689883915), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 281 epochs / 8.009 seconds | stopped early: True | metrics: {'loss': 0.6442261338233948, 'auc': np.float64(0.4979270315091211), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 329 epochs / 9.808 seconds | stopped early: True | metrics: {'loss': 0.6421910524368286, 'auc': np.float64(0.650462962962963), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 1 epochs / 4.618 seconds | stopped early: True | metrics: {'loss': 0.629126787185669, 'auc': np.float64(0.5860690235690236), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 198 epochs / 6.393 seconds | stopped early: True | metrics: {'loss': 0.6351277232170105, 'auc': np.float64(0.5863539445628998), 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Dominated Hypervolume: 0.349609375 for Pareto front [[0.65534462 0.25       0.         0.25      ]\n",
      " [0.65039062 0.         0.         0.        ]]\n",
      "Autosaving generation 4 to export/diabetes\n",
      "Saved population + offspring + pareto front of generation 4 to file\n",
      "Generation 5, evaluate 3 individuals\n",
      "Running 5-fold CV for individual 1/3: 3 total layers, 4 nodes per hidden layer, dropout p 0.22, gs: ([1, 2, 4, 7], [[[6, 5, 3], 1], [[0], 0]])\n",
      "Fold 1/5 | trained for 200 epochs / 9.692 seconds | stopped early: True | metrics: {'loss': 0.6352047324180603, 'auc': np.float64(0.7048092868988391), 'nf': 0.5, 'ni': 0.10714285714285714, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 221 epochs / 10.007 seconds | stopped early: False | metrics: {'loss': 0.6531058847904205, 'auc': np.float64(0.5497512437810945), 'nf': 0.5, 'ni': 0.10714285714285714, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 14 epochs / 4.584 seconds | stopped early: True | metrics: {'loss': 0.6502059102058411, 'auc': np.float64(0.510942760942761), 'nf': 0.5, 'ni': 0.10714285714285714, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 220 epochs / 10.017 seconds | stopped early: False | metrics: {'loss': 0.6170508563518524, 'auc': np.float64(0.6174242424242424), 'nf': 0.5, 'ni': 0.10714285714285714, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 212 epochs / 10.007 seconds | stopped early: False | metrics: {'loss': 0.6407612264156342, 'auc': np.float64(0.5872068230277186), 'nf': 0.5, 'ni': 0.10714285714285714, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 2/3: 3 total layers, 4 nodes per hidden layer, dropout p 0.22, gs: ([1, 2, 3, 4, 7], [[[6, 5], 1], [[0], 0]])\n",
      "Fold 1/5 | trained for 190 epochs / 9.389 seconds | stopped early: True | metrics: {'loss': 0.6224380135536194, 'auc': np.float64(0.5750414593698177), 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 193 epochs / 9.988 seconds | stopped early: True | metrics: {'loss': 0.6316149830818176, 'auc': np.float64(0.6434494195688225), 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 200 epochs / 10.014 seconds | stopped early: False | metrics: {'loss': 0.6313876211643219, 'auc': np.float64(0.6296296296296297), 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 110 epochs / 6.723 seconds | stopped early: True | metrics: {'loss': 0.637582927942276, 'auc': np.float64(0.6784511784511784), 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 177 epochs / 10.034 seconds | stopped early: True | metrics: {'loss': 0.6242315471172333, 'auc': np.float64(0.6085287846481877), 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Dominated Hypervolume: 0.349609375 for Pareto front [[0.65534462 0.25       0.         0.25      ]\n",
      " [0.65039062 0.         0.         0.        ]]\n",
      "Autosaving generation 5 to export/diabetes\n",
      "Saved population + offspring + pareto front of generation 5 to file\n",
      "Finished EAGGA at 2025-03-03T19:09:58.362311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.65534462, 0.25      , 0.        , 0.25      ],\n",
       "        [0.65039062, 0.        , 0.        , 0.        ]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hps = {\n",
    "    'total_layers': (3, 10),\n",
    "    'nodes_per_hidden_layer': (3, 20),\n",
    "    'mu': 100,\n",
    "    'lambda': 10,\n",
    "    'holdout_train_size': 2/3,\n",
    "    'cv_k': 5\n",
    "}\n",
    "\n",
    "batch_size = 64\n",
    "min_epochs = 200\n",
    "patience = 100\n",
    "\n",
    "secs_per_fold = 60\n",
    "secs_total = 8 * 60 * 60\n",
    "\n",
    "pareto_fronts = list()\n",
    "for (oml_dataset, class_positive) in zip(oml_datasets, positive_classes):\n",
    "    name = oml_dataset.name\n",
    "    print(f'Dataset {name}')\n",
    "\n",
    "    file_path = os.path.join('export', name)\n",
    "    \n",
    "    eagga = EAGGA(\n",
    "        oml_dataset=oml_dataset,\n",
    "        class_positive=class_positive,\n",
    "        hps=hps,\n",
    "        batch_size=batch_size,\n",
    "        min_epochs=min_epochs,\n",
    "        patience=patience,\n",
    "        secs_per_fold=secs_per_fold,\n",
    "        secs_total=secs_total,\n",
    "        file_path=file_path\n",
    "    )\n",
    "    #eagga.load_population(3)\n",
    "    \n",
    "    pareto_front = eagga.run_eagga()\n",
    "    pareto_fronts.append(pareto_front)\n",
    "pareto_fronts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
