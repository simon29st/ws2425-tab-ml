{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import tasks\n",
    "\n",
    "import torch\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from benchmark import NeuralNetwork, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "oml_task_diabetes = tasks.get_task(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, categorical_indicator, attribute_names = oml_task_diabetes.get_dataset().get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50.0</td>\n",
       "      <td>tested_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32.0</td>\n",
       "      <td>tested_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33.0</td>\n",
       "      <td>tested_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47.0</td>\n",
       "      <td>tested_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23.0</td>\n",
       "      <td>tested_negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg   plas  pres  skin   insu  mass   pedi   age            class\n",
       "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0  tested_positive\n",
       "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0  tested_negative\n",
       "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0  tested_positive\n",
       "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0  tested_negative\n",
       "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0  tested_positive\n",
       "..    ...    ...   ...   ...    ...   ...    ...   ...              ...\n",
       "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0  tested_negative\n",
       "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0  tested_negative\n",
       "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0  tested_negative\n",
       "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0  tested_positive\n",
       "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0  tested_negative\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = Dataset(\n",
    "    X=Xy.loc[:, Xy.columns != 'class'],\n",
    "    y=Xy.loc[:, 'class'],\n",
    "    class_pos='tested_positive'\n",
    ")\n",
    "tmp[2]\n",
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False, False, False, True]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both cf. https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "def train(optimizer, loss_fn, model, epochs, dataset_train, dataset_test, batch_size):\n",
    "    eval_loss = eval(loss_fn, model, dataset_test, batch_size)\n",
    "    print(f'epoch 0 / {epochs}, eval loss {eval_loss}')\n",
    "\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=False)  # TODO: set shuffle to True\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # training mode, put here as we eval at the end of each epoch\n",
    "        running_epoch_loss = 0\n",
    "\n",
    "        for batch_input, batch_target in loader_train:  # divide data in mini batches\n",
    "            optimizer.zero_grad()  # set gradients to 0\n",
    "            batch_output = model(batch_input).flatten()\n",
    "\n",
    "            batch_loss = loss_fn(batch_output, batch_target)\n",
    "            running_epoch_loss += batch_loss.detach().item()\n",
    "            batch_loss.backward()  # compute gradients\n",
    "\n",
    "            optimizer.step()  # update weights\n",
    "        running_epoch_loss /= len(loader_train)\n",
    "\n",
    "        print(f'epoch {epoch + 1} / {epochs}, train loss {running_epoch_loss}')\n",
    "        eval_loss = eval(loss_fn, model, dataset_test, batch_size)\n",
    "        print(f'epoch {epoch + 1} / {epochs}, eval loss {eval_loss}')\n",
    "\n",
    "\n",
    "def eval(loss_fn, model, dataset_test, batch_size, verbose=False):\n",
    "    loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size)  # no need to shuffle in test\n",
    "    \n",
    "    model.eval()  # eval mode\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_input, batch_target in loader_test:\n",
    "        with torch.no_grad():\n",
    "            batch_output = model(batch_input).flatten()\n",
    "            \n",
    "        batch_loss = loss_fn(batch_output, batch_target)\n",
    "        running_loss += batch_loss.detach().item()\n",
    "    running_loss /= len(loader_test)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'eval loss {running_loss}')\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 / 5\n",
      "<generator object Module.parameters at 0x00000202A18C3220>\n",
      "epoch 0 / 5, eval loss 65.38461538461539\n",
      "epoch 1 / 5, train loss 63.94230769230769\n",
      "epoch 1 / 5, eval loss 65.38461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workplace\\uni\\ws-24-25\\seminar\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 / 5, train loss 63.94230769230769\n",
      "epoch 2 / 5, eval loss 65.38461538461539\n",
      "epoch 3 / 5, train loss 63.94230769230769\n",
      "epoch 3 / 5, eval loss 65.38461538461539\n",
      "epoch 4 / 5, train loss 63.94230769230769\n",
      "epoch 4 / 5, eval loss 65.38461538461539\n",
      "epoch 5 / 5, train loss 63.94230769230769\n",
      "epoch 5 / 5, eval loss 65.38461538461539\n",
      "fold 2 / 5\n",
      "<generator object Module.parameters at 0x00000202A5D217E0>\n",
      "epoch 0 / 5, eval loss 65.38461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workplace\\uni\\ws-24-25\\seminar\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 5, train loss 63.94230769230769\n",
      "epoch 1 / 5, eval loss 65.38461538461539\n",
      "epoch 2 / 5, train loss 63.94230769230769\n",
      "epoch 2 / 5, eval loss 65.38461538461539\n",
      "epoch 3 / 5, train loss 63.94230769230769\n",
      "epoch 3 / 5, eval loss 65.38461538461539\n",
      "epoch 4 / 5, train loss 63.94230769230769\n",
      "epoch 4 / 5, eval loss 65.38461538461539\n",
      "epoch 5 / 5, train loss 63.94230769230769\n",
      "epoch 5 / 5, eval loss 65.38461538461539\n",
      "fold 3 / 5\n",
      "<generator object Module.parameters at 0x00000202A18C3760>\n",
      "epoch 0 / 5, eval loss 66.34615384615384\n",
      "epoch 1 / 5, train loss 64.66346153846153\n",
      "epoch 1 / 5, eval loss 66.34615384615384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workplace\\uni\\ws-24-25\\seminar\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 / 5, train loss 64.66346153846153\n",
      "epoch 2 / 5, eval loss 66.34615384615384\n",
      "epoch 3 / 5, train loss 64.66346153846153\n",
      "epoch 3 / 5, eval loss 66.34615384615384\n",
      "epoch 4 / 5, train loss 64.66346153846153\n",
      "epoch 4 / 5, eval loss 66.34615384615384\n",
      "epoch 5 / 5, train loss 64.66346153846153\n",
      "epoch 5 / 5, eval loss 66.34615384615384\n",
      "fold 4 / 5\n",
      "<generator object Module.parameters at 0x00000202A18C3220>\n",
      "epoch 0 / 5, eval loss 64.74358954796425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workplace\\uni\\ws-24-25\\seminar\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 5, train loss 64.90384615384616\n",
      "epoch 1 / 5, eval loss 64.74358954796425\n",
      "epoch 2 / 5, train loss 64.90384615384616\n",
      "epoch 2 / 5, eval loss 64.74358954796425\n",
      "epoch 3 / 5, train loss 64.90384615384616\n",
      "epoch 3 / 5, eval loss 64.74358954796425\n",
      "epoch 4 / 5, train loss 64.90384615384616\n",
      "epoch 4 / 5, eval loss 64.74358954796425\n",
      "epoch 5 / 5, train loss 64.90384615384616\n",
      "epoch 5 / 5, eval loss 64.74358954796425\n",
      "fold 5 / 5\n",
      "<generator object Module.parameters at 0x00000202A18C3BC0>\n",
      "epoch 0 / 5, eval loss 65.06410275972806\n",
      "epoch 1 / 5, train loss 65.625\n",
      "epoch 1 / 5, eval loss 65.06410275972806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workplace\\uni\\ws-24-25\\seminar\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 / 5, train loss 65.625\n",
      "epoch 2 / 5, eval loss 65.06410275972806\n",
      "epoch 3 / 5, train loss 65.625\n",
      "epoch 3 / 5, eval loss 65.06410275972806\n",
      "epoch 4 / 5, train loss 65.625\n",
      "epoch 4 / 5, eval loss 65.06410275972806\n",
      "epoch 5 / 5, train loss 65.625\n",
      "epoch 5 / 5, eval loss 65.06410275972806\n"
     ]
    }
   ],
   "source": [
    "# outer\n",
    "data_train_test, data_val = train_test_split(\n",
    "    Xy,\n",
    "    train_size=2/3,\n",
    "    shuffle=True,\n",
    "    stratify=Xy.loc[:, 'class']\n",
    ")\n",
    "\n",
    "# reset indices as StratifiedKFold assumes 0-(n-1) index\n",
    "data_train_test = data_train_test.reset_index(drop=True)\n",
    "data_val = data_val.reset_index(drop=True)\n",
    "\n",
    "# inner\n",
    "cv_inner = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=False  # TODO: set to True\n",
    ")\n",
    "\n",
    "for i, (indices_train, indices_test) in enumerate(cv_inner.split(X=data_train_test, y=data_train_test.loc[:, 'class'])):\n",
    "    print(f'fold {i + 1} / {cv_inner.get_n_splits()}')\n",
    "\n",
    "    data_train = data_train_test.loc[indices_train, :]\n",
    "    dataset_train = Dataset(\n",
    "        X=data_train.loc[:, data_train.columns != 'class'],\n",
    "        y=data_train.loc[:, 'class'],\n",
    "        class_pos='tested_positive'\n",
    "    )\n",
    "\n",
    "    data_test = data_train_test.loc[indices_test, :]\n",
    "    dataset_test = Dataset(\n",
    "        X=data_test.loc[:, data_test.columns != 'class'],\n",
    "        y=data_test.loc[:, 'class'],\n",
    "        class_pos='tested_positive'\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    in each inner fold, run EAGGA + find an optimal pareto front of configurations for this specific fold\n",
    "    then in the outer fold (holdout), compare each fold-specific optimal pareto front + select optimum\n",
    "    '''\n",
    "\n",
    "    hp_configs = {\n",
    "        'total_layers': [3],\n",
    "        'nodes_per_hidden_layer': [3]\n",
    "    }\n",
    "    metrics_eval = {\n",
    "        'auc': list(),\n",
    "        'ni': list(),\n",
    "        'nf': list(),\n",
    "        'nnm': list()\n",
    "    }\n",
    "    epochs = 10\n",
    "    batch_size = 8\n",
    "    # HPO loop starts here\n",
    "    while(True):  # TODO: define stopping criterion + remove break from end of loop\n",
    "        model = NeuralNetwork(\n",
    "            input_size=len(data_train_test.columns) - 1,\n",
    "            output_size=1,  # we only use binary datasets\n",
    "            total_layers=hp_configs['total_layers'][-1],\n",
    "            nodes_per_hidden_layer=hp_configs['nodes_per_hidden_layer'][-1]\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters())\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        train(optimizer, loss_fn, model, 5, dataset_train, dataset_test, batch_size)\n",
    "\n",
    "        # TODO: evaluate performance + add to auc, NI, NF, NNM\n",
    "\n",
    "        # TODO: EA on hp_total_layers + hp_nodes\n",
    "        # TODO: implement group structure\n",
    "\n",
    "        # until stopping criterion is met\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
