{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%pip install setuptools==70.3.0  # apparently setuptools < 71.0.0 is required for a required package of openml to install on here\\n%pip install openml\\n%pip install numpy\\n%pip install pandas\\n# cf. https://pytorch.org/get-started/locally/\\n#%pip install torch torchvision torchaudio  # cuda 12.4, cf. nvidia-smi shell command\\n%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu  # cpu version\\n%pip install -U scikit-learn\\n%pip install scipy\\n%pip install -U pymoo\\n%pip list'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%pip install setuptools==70.3.0  # apparently setuptools < 71.0.0 is required for a required package of openml to install on here\n",
    "%pip install openml\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "# cf. https://pytorch.org/get-started/locally/\n",
    "#%pip install torch torchvision torchaudio  # cuda 12.4, cf. nvidia-smi shell command\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu  # cpu version\n",
    "%pip install -U scikit-learn\n",
    "%pip install scipy\n",
    "%pip install -U pymoo\n",
    "%pip list'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import tasks\n",
    "\n",
    "from classes import EAGGA\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/openml/tasks/functions.py:372: UserWarning: `download_data` will default to False starting in 0.16. Please set `download_data` explicitly to suppress this warning.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/openml/tasks/functions.py:380: UserWarning: `download_qualities` will default to False starting in 0.16. Please set `download_qualities` explicitly to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "oml_task_ids = [37, 43, 3903, 3904, 3913, 3918, 10093, 9946, 146819, 359955, 189922, 359962, 190392, 167120, 190137, 190410, 168350, 359975, 359972, 146820]\n",
    "oml_tasks = tasks.get_tasks(oml_task_ids)\n",
    "\n",
    "oml_datasets = [oml_task.get_dataset() for oml_task in oml_tasks]\n",
    "\n",
    "# define positive classes\n",
    "positive_classes = ['tested_positive', '1', True, True, 'yes', True, '2', '2', '1', '2', '1', True, '1', '1', '2', '1', '2', 'Anomaly', '1', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset diabetes\n",
      "Starting init population\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished init population\n",
      "Loaded population + offspring + pareto front of generation 1 from file, discarded previous population + offspring + pareto front\n",
      "Start EAGGA at 2025-03-03T20:37:31.406908\n",
      "Generation 2, evaluate 10 individuals\n",
      "Running 5-fold CV for individual 1/10: 4 total layers, 3 nodes per hidden layer, dropout p 0.3, gs: ([0, 2, 3, 4, 5, 6, 7], [[[1], 1]])\n",
      "Fold 1/5 | trained for 256 epochs / 6.547 seconds | stopped early: True | metrics: {'loss': 0.6351303458213806, 'auc': 0.7964344941956882, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 272 epochs / 7.903 seconds | stopped early: True | metrics: {'loss': 0.6373822689056396, 'auc': 0.8264925373134329, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 235 epochs / 7.172 seconds | stopped early: True | metrics: {'loss': 0.6352433264255524, 'auc': 0.6023454157782516, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 125 epochs / 4.536 seconds | stopped early: True | metrics: {'loss': 0.6605190634727478, 'auc': 0.5, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 213 epochs / 6.594 seconds | stopped early: True | metrics: {'loss': 0.6395467817783356, 'auc': 0.7773569023569022, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 2/10: 4 total layers, 4 nodes per hidden layer, dropout p 0.5, gs: ([2, 6, 7], [[[1], 1], [[3], 1], [[5], 0], [[4], 1], [[0], 0]])\n",
      "Fold 1/5 | trained for 166 epochs / 18.427 seconds | stopped early: True | metrics: {'loss': 0.6254876852035522, 'auc': 0.8714759535655058, 'nf': 0.625, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 2/5 | trained for 212 epochs / 16.989 seconds | stopped early: True | metrics: {'loss': 0.6510402858257294, 'auc': 0.714759535655058, 'nf': 0.625, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 3/5 | trained for 194 epochs / 15.607 seconds | stopped early: True | metrics: {'loss': 0.6415198147296906, 'auc': 0.8206823027718549, 'nf': 0.625, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 4/5 | trained for 265 epochs / 21.191 seconds | stopped early: True | metrics: {'loss': 0.6425508558750153, 'auc': 0.7243265993265993, 'nf': 0.625, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 5/5 | trained for 185 epochs / 18.594 seconds | stopped early: True | metrics: {'loss': 0.6432969868183136, 'auc': 0.8594276094276093, 'nf': 0.625, 'ni': 0.0, 'nnm': 0.25}\n",
      "Running 5-fold CV for individual 3/10: 4 total layers, 5 nodes per hidden layer, dropout p 0.1, gs: ([0, 2, 3, 4, 6], [[[7], 0], [[1], 0], [[5], 1]])\n",
      "Fold 1/5 | trained for 386 epochs / 22.669 seconds | stopped early: True | metrics: {'loss': 0.593390941619873, 'auc': 0.7475124378109452, 'nf': 0.375, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 2/5 | trained for 317 epochs / 15.064 seconds | stopped early: True | metrics: {'loss': 0.5869131088256836, 'auc': 0.8503316749585407, 'nf': 0.375, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 3/5 | trained for 307 epochs / 18.13 seconds | stopped early: True | metrics: {'loss': 0.5818475484848022, 'auc': 0.8230277185501065, 'nf': 0.375, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 4/5 | trained for 223 epochs / 11.172 seconds | stopped early: True | metrics: {'loss': 0.5704743564128876, 'auc': 0.8522727272727273, 'nf': 0.375, 'ni': 0.0, 'nnm': 0.25}\n",
      "Fold 5/5 | trained for 210 epochs / 13.277 seconds | stopped early: True | metrics: {'loss': 0.6106087863445282, 'auc': 0.7685185185185185, 'nf': 0.375, 'ni': 0.0, 'nnm': 0.25}\n",
      "Running 5-fold CV for individual 4/10: 3 total layers, 3 nodes per hidden layer, dropout p 0.6, gs: ([1, 7, 0, 2, 3, 4, 6], [[[5], 1]])\n",
      "Fold 1/5 | trained for 179 epochs / 7.049 seconds | stopped early: True | metrics: {'loss': 0.6479702293872833, 'auc': 0.7135157545605305, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 172 epochs / 5.67 seconds | stopped early: True | metrics: {'loss': 0.6607238054275513, 'auc': 0.3041044776119403, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 546 epochs / 15.223 seconds | stopped early: True | metrics: {'loss': 0.6430292129516602, 'auc': 0.5, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 254 epochs / 7.959 seconds | stopped early: True | metrics: {'loss': 0.659089982509613, 'auc': 0.6731902356902357, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 44 epochs / 5.449 seconds | stopped early: True | metrics: {'loss': 0.662868082523346, 'auc': 0.6748737373737373, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 5/10: 3 total layers, 3 nodes per hidden layer, dropout p 0.2, gs: ([2, 3, 4, 7, 1], [[[5, 6], 1], [[0], 0]])\n",
      "Fold 1/5 | trained for 193 epochs / 6.854 seconds | stopped early: True | metrics: {'loss': 0.6359841823577881, 'auc': 0.7064676616915423, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 301 epochs / 13.354 seconds | stopped early: True | metrics: {'loss': 0.5956940650939941, 'auc': 0.7524875621890547, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 153 epochs / 8.488 seconds | stopped early: True | metrics: {'loss': 0.618667870759964, 'auc': 0.6426439232409382, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 285 epochs / 11.495 seconds | stopped early: True | metrics: {'loss': 0.5874010026454926, 'auc': 0.7811447811447811, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 201 epochs / 9.758 seconds | stopped early: True | metrics: {'loss': 0.6004020571708679, 'auc': 0.7247474747474748, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 6/10: 3 total layers, 4 nodes per hidden layer, dropout p 0.1, gs: ([4, 0, 1, 2, 3, 6, 7], [[[5], 1]])\n",
      "Fold 1/5 | trained for 304 epochs / 8.817 seconds | stopped early: True | metrics: {'loss': 0.6437101662158966, 'auc': 0.6915422885572139, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 217 epochs / 6.878 seconds | stopped early: True | metrics: {'loss': 0.6400236189365387, 'auc': 0.6822139303482587, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 310 epochs / 7.513 seconds | stopped early: True | metrics: {'loss': 0.6346441209316254, 'auc': 0.6275053304904051, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 173 epochs / 5.111 seconds | stopped early: True | metrics: {'loss': 0.6469256579875946, 'auc': 0.7190656565656566, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 688 epochs / 17.902 seconds | stopped early: True | metrics: {'loss': 0.631592184305191, 'auc': 0.683080808080808, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 7/10: 6 total layers, 5 nodes per hidden layer, dropout p 0.2, gs: ([0, 2, 3, 4, 6, 7], [[[1, 5], 1]])\n",
      "Fold 1/5 | trained for 210 epochs / 8.963 seconds | stopped early: True | metrics: {'loss': 0.611282616853714, 'auc': 0.8063847429519072, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 222 epochs / 7.62 seconds | stopped early: True | metrics: {'loss': 0.6396152973175049, 'auc': 0.7877280265339967, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 344 epochs / 12.001 seconds | stopped early: True | metrics: {'loss': 0.6425890028476715, 'auc': 0.8174840085287847, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 189 epochs / 9.491 seconds | stopped early: True | metrics: {'loss': 0.6545788049697876, 'auc': 0.1031144781144781, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 197 epochs / 5.837 seconds | stopped early: True | metrics: {'loss': 0.6080601513385773, 'auc': 0.7622053872053872, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 8/10: 3 total layers, 5 nodes per hidden layer, dropout p 0.4, gs: ([0, 1, 2, 3, 6, 7], [[[4, 5], 1]])\n",
      "Fold 1/5 | trained for 203 epochs / 5.759 seconds | stopped early: True | metrics: {'loss': 0.6553446650505066, 'auc': 0.5246683250414593, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 269 epochs / 6.031 seconds | stopped early: True | metrics: {'loss': 0.6493695378303528, 'auc': 0.589759535655058, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 253 epochs / 5.208 seconds | stopped early: True | metrics: {'loss': 0.6549792587757111, 'auc': 0.4901918976545842, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 253 epochs / 5.808 seconds | stopped early: True | metrics: {'loss': 0.662240594625473, 'auc': 0.27925084175084175, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 265 epochs / 7.18 seconds | stopped early: True | metrics: {'loss': 0.6453719139099121, 'auc': 0.5812289562289562, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 9/10: 3 total layers, 3 nodes per hidden layer, dropout p 0.5, gs: ([0, 1, 2, 4, 5, 6, 7], [[[3], 1]])\n",
      "Fold 1/5 | trained for 206 epochs / 6.189 seconds | stopped early: True | metrics: {'loss': 0.6409408748149872, 'auc': 0.4280679933665008, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 68 epochs / 5.53 seconds | stopped early: True | metrics: {'loss': 0.6839319467544556, 'auc': 0.4071310116086235, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 11 epochs / 5.157 seconds | stopped early: True | metrics: {'loss': 0.7322567403316498, 'auc': 0.4415778251599147, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 255 epochs / 6.561 seconds | stopped early: True | metrics: {'loss': 0.6625825464725494, 'auc': 0.49074074074074076, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 215 epochs / 5.432 seconds | stopped early: True | metrics: {'loss': 0.6525328755378723, 'auc': 0.6161616161616161, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 10/10: 3 total layers, 5 nodes per hidden layer, dropout p 0.5, gs: ([2, 6], [[[3, 4], 0], [[0, 1], 0], [[5, 7], 1]])\n",
      "Fold 1/5 | trained for 205 epochs / 13.956 seconds | stopped early: True | metrics: {'loss': 0.6575176119804382, 'auc': 0.4821724709784411, 'nf': 0.75, 'ni': 0.10714285714285714, 'nnm': 0.5}\n",
      "Fold 2/5 | trained for 160 epochs / 11.078 seconds | stopped early: True | metrics: {'loss': 0.6350434422492981, 'auc': 0.6927860696517413, 'nf': 0.75, 'ni': 0.10714285714285714, 'nnm': 0.5}\n",
      "Fold 3/5 | trained for 29 epochs / 13.846 seconds | stopped early: True | metrics: {'loss': 0.6681943833827972, 'auc': 0.49509594882729213, 'nf': 0.75, 'ni': 0.10714285714285714, 'nnm': 0.5}\n",
      "Fold 4/5 | trained for 185 epochs / 9.372 seconds | stopped early: True | metrics: {'loss': 0.646811306476593, 'auc': 0.6056397306397306, 'nf': 0.75, 'ni': 0.10714285714285714, 'nnm': 0.5}\n",
      "Fold 5/5 | trained for 149 epochs / 10.625 seconds | stopped early: True | metrics: {'loss': 0.6571171283721924, 'auc': 0.6582491582491582, 'nf': 0.75, 'ni': 0.10714285714285714, 'nnm': 0.5}\n",
      "Dominated Hypervolume: 0.7823498848653911 for Pareto front [[0.80293    0.25       0.03571    0.        ]\n",
      " [0.781078   0.125      0.         0.125     ]\n",
      " [0.723406   0.125      0.         0.        ]\n",
      " [0.80833262 0.375      0.         0.25      ]\n",
      " [0.65039062 0.         0.         0.        ]]\n",
      "Autosaving generation 2 to export/diabetes\n",
      "Saved population + offspring + pareto front of generation 2 to file\n",
      "Generation 3, evaluate 10 individuals\n",
      "Running 5-fold CV for individual 1/10: 3 total layers, 4 nodes per hidden layer, dropout p 0.1, gs: ([0, 1, 2, 3, 4, 5], [[[7, 6], 0]])\n",
      "Fold 1/5 | trained for 337 epochs / 10.737 seconds | stopped early: True | metrics: {'loss': 0.6266076266765594, 'auc': 0.6936152570480929, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 2/5 | trained for 465 epochs / 10.781 seconds | stopped early: True | metrics: {'loss': 0.6570132374763489, 'auc': 0.6525704809286899, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 3/5 | trained for 180 epochs / 6.22 seconds | stopped early: True | metrics: {'loss': 0.6398733854293823, 'auc': 0.4769722814498934, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 4/5 | trained for 393 epochs / 12.181 seconds | stopped early: True | metrics: {'loss': 0.6602802276611328, 'auc': 0.7765151515151516, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 5/5 | trained for 473 epochs / 11.544 seconds | stopped early: True | metrics: {'loss': 0.6176516115665436, 'auc': 0.6759259259259259, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Running 5-fold CV for individual 2/10: 3 total layers, 3 nodes per hidden layer, dropout p 0.1, gs: ([0, 2, 3, 4, 6], [[[5, 7], 0], [[1], 0]])\n",
      "Fold 1/5 | trained for 363 epochs / 15.215 seconds | stopped early: True | metrics: {'loss': 0.6707234680652618, 'auc': 0.5055970149253731, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.375}\n",
      "Fold 2/5 | trained for 108 epochs / 7.132 seconds | stopped early: True | metrics: {'loss': 0.6568661034107208, 'auc': 0.7446102819237148, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.375}\n",
      "Fold 3/5 | trained for 248 epochs / 8.959 seconds | stopped early: True | metrics: {'loss': 0.606929212808609, 'auc': 0.7786780383795309, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.375}\n",
      "Fold 4/5 | trained for 201 epochs / 8.477 seconds | stopped early: True | metrics: {'loss': 0.6033312678337097, 'auc': 0.7483164983164982, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.375}\n",
      "Fold 5/5 | trained for 242 epochs / 10.448 seconds | stopped early: True | metrics: {'loss': 0.6188522279262543, 'auc': 0.8068181818181818, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.375}\n",
      "Running 5-fold CV for individual 3/10: 7 total layers, 4 nodes per hidden layer, dropout p 0.0, gs: ([2, 3, 5, 6], [[[7, 4], 0], [[1, 0], 1]])\n",
      "Fold 1/5 | trained for 203 epochs / 10.301 seconds | stopped early: True | metrics: {'loss': 0.5877315849065781, 'auc': 0.751865671641791, 'nf': 0.5, 'ni': 0.07142857142857142, 'nnm': 0.25}\n",
      "Fold 2/5 | trained for 182 epochs / 13.299 seconds | stopped early: True | metrics: {'loss': 0.6532990336418152, 'auc': 0.5696517412935324, 'nf': 0.5, 'ni': 0.07142857142857142, 'nnm': 0.25}\n",
      "Fold 3/5 | trained for 204 epochs / 12.239 seconds | stopped early: True | metrics: {'loss': 0.5033657550811768, 'auc': 0.8264392324093817, 'nf': 0.5, 'ni': 0.07142857142857142, 'nnm': 0.25}\n",
      "Fold 4/5 | trained for 246 epochs / 12.142 seconds | stopped early: True | metrics: {'loss': 0.4854647070169449, 'auc': 0.836489898989899, 'nf': 0.5, 'ni': 0.07142857142857142, 'nnm': 0.25}\n",
      "Fold 5/5 | trained for 198 epochs / 9.981 seconds | stopped early: True | metrics: {'loss': 0.480184942483902, 'auc': 0.8552188552188552, 'nf': 0.5, 'ni': 0.07142857142857142, 'nnm': 0.25}\n",
      "Running 5-fold CV for individual 4/10: 3 total layers, 3 nodes per hidden layer, dropout p 0.1, gs: ([0, 2, 3, 4, 6], [[[5, 7], 0], [[1], 0]])\n",
      "Fold 1/5 | trained for 554 epochs / 22.275 seconds | stopped early: True | metrics: {'loss': 0.6097367107868195, 'auc': 0.7236733001658375, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.375}\n",
      "Fold 2/5 | trained for 850 epochs / 36.315 seconds | stopped early: True | metrics: {'loss': 0.5493968576192856, 'auc': 0.8553067993366501, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.375}\n",
      "Fold 3/5 | trained for 548 epochs / 18.221 seconds | stopped early: True | metrics: {'loss': 0.5800654888153076, 'auc': 0.8072494669509596, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.375}\n",
      "Fold 4/5 | trained for 731 epochs / 27.537 seconds | stopped early: True | metrics: {'loss': 0.5763509273529053, 'auc': 0.8089225589225589, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.375}\n",
      "Fold 5/5 | trained for 195 epochs / 7.373 seconds | stopped early: True | metrics: {'loss': 0.5725258886814117, 'auc': 0.7718855218855218, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.375}\n",
      "Running 5-fold CV for individual 5/10: 5 total layers, 4 nodes per hidden layer, dropout p 0.4, gs: ([0, 2, 3, 6], [[[1, 4], 0], [[5, 7], 1]])\n",
      "Fold 1/5 | trained for 215 epochs / 9.418 seconds | stopped early: True | metrics: {'loss': 0.639541506767273, 'auc': 0.6762023217247097, 'nf': 0.5, 'ni': 0.07142857142857142, 'nnm': 0.25}\n",
      "Fold 2/5 | trained for 96 epochs / 6.92 seconds | stopped early: True | metrics: {'loss': 0.6705722510814667, 'auc': 0.31094527363184077, 'nf': 0.5, 'ni': 0.07142857142857142, 'nnm': 0.25}\n",
      "Fold 3/5 | trained for 208 epochs / 10.371 seconds | stopped early: True | metrics: {'loss': 0.6639536023139954, 'auc': 0.43752665245202554, 'nf': 0.5, 'ni': 0.07142857142857142, 'nnm': 0.25}\n",
      "Fold 4/5 | trained for 215 epochs / 12.37 seconds | stopped early: True | metrics: {'loss': 0.6526620388031006, 'auc': 0.5382996632996633, 'nf': 0.5, 'ni': 0.07142857142857142, 'nnm': 0.25}\n",
      "Fold 5/5 | trained for 248 epochs / 12.715 seconds | stopped early: True | metrics: {'loss': 0.6546319723129272, 'auc': 0.7138047138047139, 'nf': 0.5, 'ni': 0.07142857142857142, 'nnm': 0.25}\n",
      "Running 5-fold CV for individual 6/10: 4 total layers, 6 nodes per hidden layer, dropout p 0.3, gs: ([0, 2, 3, 4, 6, 7], [[[1, 5], 0]])\n",
      "Fold 1/5 | trained for 180 epochs / 6.051 seconds | stopped early: True | metrics: {'loss': 0.6264120042324066, 'auc': 0.7014925373134328, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 2/5 | trained for 5 epochs / 4.255 seconds | stopped early: True | metrics: {'loss': 0.6383928954601288, 'auc': 0.7052238805970149, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 3/5 | trained for 489 epochs / 14.714 seconds | stopped early: True | metrics: {'loss': 0.6307274699211121, 'auc': 0.7266524520255864, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 4/5 | trained for 204 epochs / 5.28 seconds | stopped early: True | metrics: {'loss': 0.631382942199707, 'auc': 0.7394781144781145, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 5/5 | trained for 256 epochs / 7.4 seconds | stopped early: True | metrics: {'loss': 0.6133929789066315, 'auc': 0.7882996632996634, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Running 5-fold CV for individual 7/10: 3 total layers, 3 nodes per hidden layer, dropout p 0.0, gs: ([0, 1, 2, 4, 5, 6, 7], [[[3], 0]])\n",
      "Fold 1/5 | trained for 420 epochs / 10.206 seconds | stopped early: True | metrics: {'loss': 0.6433931887149811, 'auc': 0.5876865671641791, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 386 epochs / 8.547 seconds | stopped early: True | metrics: {'loss': 0.6349044740200043, 'auc': 0.5715174129353234, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 181 epochs / 6.704 seconds | stopped early: True | metrics: {'loss': 0.6412345468997955, 'auc': 0.6110874200426439, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 496 epochs / 12.384 seconds | stopped early: True | metrics: {'loss': 0.6443989276885986, 'auc': 0.5753367003367004, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 191 epochs / 5.429 seconds | stopped early: True | metrics: {'loss': 0.6510851979255676, 'auc': 0.5311447811447811, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 8/10: 4 total layers, 3 nodes per hidden layer, dropout p 0.5, gs: ([0, 1, 2, 4, 6, 7], [[[3, 5], 0]])\n",
      "Fold 1/5 | trained for 152 epochs / 5.99 seconds | stopped early: True | metrics: {'loss': 0.6486674547195435, 'auc': 0.5963930348258706, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 2/5 | trained for 281 epochs / 8.483 seconds | stopped early: True | metrics: {'loss': 0.6511660814285278, 'auc': 0.49647595356550583, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 3/5 | trained for 16 epochs / 8.898 seconds | stopped early: True | metrics: {'loss': 0.6754201054573059, 'auc': 0.43795309168443497, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 4/5 | trained for 283 epochs / 10.037 seconds | stopped early: True | metrics: {'loss': 0.65707728266716, 'auc': 0.5046296296296297, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Fold 5/5 | trained for 241 epochs / 7.636 seconds | stopped early: True | metrics: {'loss': 0.6547698080539703, 'auc': 0.5547138047138047, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.25}\n",
      "Running 5-fold CV for individual 9/10: 3 total layers, 4 nodes per hidden layer, dropout p 0.1, gs: ([0, 1, 2, 3, 4, 5, 6], [[[7], 0]])\n",
      "Fold 1/5 | trained for 494 epochs / 12.299 seconds | stopped early: True | metrics: {'loss': 0.6189419031143188, 'auc': 0.6917495854063018, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 214 epochs / 4.951 seconds | stopped early: True | metrics: {'loss': 0.6320622861385345, 'auc': 0.7145522388059702, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 309 epochs / 7.531 seconds | stopped early: True | metrics: {'loss': 0.6333260238170624, 'auc': 0.5, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 867 epochs / 20.424 seconds | stopped early: True | metrics: {'loss': 0.6460394263267517, 'auc': 0.6759259259259259, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 281 epochs / 6.277 seconds | stopped early: True | metrics: {'loss': 0.663068413734436, 'auc': 0.5, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 10/10: 5 total layers, 5 nodes per hidden layer, dropout p 0.0, gs: ([0, 1, 2, 3, 4, 5, 6], [[[7], 0]])\n",
      "Fold 1/5 | trained for 260 epochs / 7.302 seconds | stopped early: True | metrics: {'loss': 0.6154122650623322, 'auc': 0.7106135986733002, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 151 epochs / 6.721 seconds | stopped early: True | metrics: {'loss': 0.6578851938247681, 'auc': 0.5, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 298 epochs / 7.448 seconds | stopped early: True | metrics: {'loss': 0.5648426711559296, 'auc': 0.7398720682302772, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 248 epochs / 6.178 seconds | stopped early: True | metrics: {'loss': 0.6284002363681793, 'auc': 0.6803451178451179, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 537 epochs / 14.242 seconds | stopped early: True | metrics: {'loss': 0.5764995813369751, 'auc': 0.7506313131313131, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Dominated Hypervolume: 0.7823498848653911 for Pareto front [[0.80293    0.25       0.03571    0.        ]\n",
      " [0.781078   0.125      0.         0.125     ]\n",
      " [0.80833262 0.375      0.         0.25      ]\n",
      " [0.723406   0.125      0.         0.        ]\n",
      " [0.65039062 0.         0.         0.        ]]\n",
      "Autosaving generation 3 to export/diabetes\n",
      "Saved population + offspring + pareto front of generation 3 to file\n",
      "Generation 4, evaluate 10 individuals\n",
      "Running 5-fold CV for individual 1/10: 3 total layers, 4 nodes per hidden layer, dropout p 0.1, gs: ([0, 2, 3, 4, 5, 6], [[[1, 7], 1]])\n",
      "Fold 1/5 | trained for 351 epochs / 9.857 seconds | stopped early: True | metrics: {'loss': 0.5977664589881897, 'auc': 0.7640961857379768, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 537 epochs / 15.077 seconds | stopped early: True | metrics: {'loss': 0.597470223903656, 'auc': 0.7906301824212271, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 439 epochs / 12.985 seconds | stopped early: True | metrics: {'loss': 0.6227439939975739, 'auc': 0.6560767590618336, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 838 epochs / 21.755 seconds | stopped early: True | metrics: {'loss': 0.5472380816936493, 'auc': 0.8173400673400674, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 458 epochs / 12.6 seconds | stopped early: True | metrics: {'loss': 0.5769488215446472, 'auc': 0.8897306397306398, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 2/10: 3 total layers, 3 nodes per hidden layer, dropout p 0.6, gs: ([0, 2, 3, 4, 5, 6, 7], [[[1], 1]])\n",
      "Fold 1/5 | trained for 153 epochs / 4.82 seconds | stopped early: True | metrics: {'loss': 0.6442080140113831, 'auc': 0.8594527363184079, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 236 epochs / 6.301 seconds | stopped early: True | metrics: {'loss': 0.6570043861865997, 'auc': 0.5, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 196 epochs / 5.592 seconds | stopped early: True | metrics: {'loss': 0.6444962620735168, 'auc': 0.5, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 190 epochs / 6.239 seconds | stopped early: True | metrics: {'loss': 0.6601645052433014, 'auc': 0.5, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 97 epochs / 5.7 seconds | stopped early: True | metrics: {'loss': 0.6443356871604919, 'auc': 0.7847222222222222, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 3/10: 3 total layers, 3 nodes per hidden layer, dropout p 0.1, gs: ([0, 1, 2, 3, 4, 6, 7], [[[5], 0]])\n",
      "Fold 1/5 | trained for 277 epochs / 6.856 seconds | stopped early: True | metrics: {'loss': 0.6512019634246826, 'auc': 0.6289386401326701, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 375 epochs / 9.282 seconds | stopped early: True | metrics: {'loss': 0.6438152194023132, 'auc': 0.6994195688225538, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 229 epochs / 6.114 seconds | stopped early: True | metrics: {'loss': 0.6470836400985718, 'auc': 0.5, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 241 epochs / 6.716 seconds | stopped early: True | metrics: {'loss': 0.6622376441955566, 'auc': 0.3442760942760943, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 446 epochs / 10.317 seconds | stopped early: True | metrics: {'loss': 0.6305221021175385, 'auc': 0.6534090909090909, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 4/10: 9 total layers, 4 nodes per hidden layer, dropout p 0.2, gs: ([0, 2, 3, 4, 5, 6, 7], [[[1], 1]])\n",
      "Fold 1/5 | trained for 111 epochs / 7.415 seconds | stopped early: True | metrics: {'loss': 0.6482892632484436, 'auc': 0.14842454394693202, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 148 epochs / 8.845 seconds | stopped early: True | metrics: {'loss': 0.6653764247894287, 'auc': 0.23756218905472634, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 259 epochs / 9.78 seconds | stopped early: True | metrics: {'loss': 0.633996844291687, 'auc': 0.8189765458422174, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 13 epochs / 9.071 seconds | stopped early: True | metrics: {'loss': 0.6552156805992126, 'auc': 0.24537037037037038, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 203 epochs / 9.524 seconds | stopped early: True | metrics: {'loss': 0.6477275490760803, 'auc': 0.8171296296296297, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 5/10: 4 total layers, 4 nodes per hidden layer, dropout p 0.5, gs: ([2, 7], [[[1], 1], [[3], 0], [[5], 0], [[4], 1], [[0, 6], 0]])\n",
      "Fold 1/5 | trained for 175 epochs / 21.677 seconds | stopped early: True | metrics: {'loss': 0.6411485075950623, 'auc': 0.7139303482587065, 'nf': 0.75, 'ni': 0.03571428571428571, 'nnm': 0.5}\n",
      "Fold 2/5 | trained for 271 epochs / 24.446 seconds | stopped early: True | metrics: {'loss': 0.6406247615814209, 'auc': 0.6318407960199005, 'nf': 0.75, 'ni': 0.03571428571428571, 'nnm': 0.5}\n",
      "Fold 3/5 | trained for 202 epochs / 18.203 seconds | stopped early: True | metrics: {'loss': 0.6056066453456879, 'auc': 0.8614072494669509, 'nf': 0.75, 'ni': 0.03571428571428571, 'nnm': 0.5}\n",
      "Fold 4/5 | trained for 268 epochs / 25.318 seconds | stopped early: True | metrics: {'loss': 0.6540392339229584, 'auc': 0.7546296296296297, 'nf': 0.75, 'ni': 0.03571428571428571, 'nnm': 0.5}\n",
      "Fold 5/5 | trained for 200 epochs / 15.582 seconds | stopped early: True | metrics: {'loss': 0.6448860168457031, 'auc': 0.6946548821548821, 'nf': 0.75, 'ni': 0.03571428571428571, 'nnm': 0.5}\n",
      "Running 5-fold CV for individual 6/10: 7 total layers, 4 nodes per hidden layer, dropout p 0.0, gs: ([2, 3, 5, 6, 1], [[[7], 0], [[0, 4], 1]])\n",
      "Fold 1/5 | trained for 153 epochs / 11.323 seconds | stopped early: True | metrics: {'loss': 0.6000993251800537, 'auc': 0.7483416252072969, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 2/5 | trained for 106 epochs / 11.846 seconds | stopped early: True | metrics: {'loss': 0.6442466676235199, 'auc': 0.669983416252073, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 3/5 | trained for 205 epochs / 11.155 seconds | stopped early: True | metrics: {'loss': 0.6343668699264526, 'auc': 0.5865671641791044, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 4/5 | trained for 231 epochs / 12.333 seconds | stopped early: True | metrics: {'loss': 0.6421306729316711, 'auc': 0.7270622895622896, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Fold 5/5 | trained for 143 epochs / 10.621 seconds | stopped early: True | metrics: {'loss': 0.614056408405304, 'auc': 0.6599326599326599, 'nf': 0.375, 'ni': 0.03571428571428571, 'nnm': 0.125}\n",
      "Running 5-fold CV for individual 7/10: 3 total layers, 4 nodes per hidden layer, dropout p 0.1, gs: ([0, 2, 3, 6, 4], [[[5, 1, 7], 1]])\n",
      "Fold 1/5 | trained for 653 epochs / 17.375 seconds | stopped early: True | metrics: {'loss': 0.5448294878005981, 'auc': 0.8557213930348259, 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 659 epochs / 17.395 seconds | stopped early: True | metrics: {'loss': 0.6059757471084595, 'auc': 0.7769485903814263, 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 226 epochs / 8.336 seconds | stopped early: True | metrics: {'loss': 0.6497048437595367, 'auc': 0.4520255863539446, 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 683 epochs / 17.802 seconds | stopped early: True | metrics: {'loss': 0.5581066310405731, 'auc': 0.8486952861952862, 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 635 epochs / 16.62 seconds | stopped early: True | metrics: {'loss': 0.6094680428504944, 'auc': 0.7516835016835017, 'nf': 0.375, 'ni': 0.10714285714285714, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 8/10: 3 total layers, 4 nodes per hidden layer, dropout p 0.1, gs: ([1, 0, 2, 3, 5, 6, 7], [[[4], 1]])\n",
      "Fold 1/5 | trained for 190 epochs / 5.258 seconds | stopped early: True | metrics: {'loss': 0.6208182573318481, 'auc': 0.6480099502487562, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 208 epochs / 5.155 seconds | stopped early: True | metrics: {'loss': 0.6643927991390228, 'auc': 0.6422056384742951, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 204 epochs / 4.452 seconds | stopped early: True | metrics: {'loss': 0.635977178812027, 'auc': 0.664818763326226, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 295 epochs / 7.406 seconds | stopped early: True | metrics: {'loss': 0.6475036144256592, 'auc': 0.5782828282828282, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 155 epochs / 4.333 seconds | stopped early: True | metrics: {'loss': 0.6406704485416412, 'auc': 0.571969696969697, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 9/10: 3 total layers, 3 nodes per hidden layer, dropout p 0.8, gs: ([0, 2, 3, 4, 6, 7], [[[1, 5], 1]])\n",
      "Fold 1/5 | trained for 54 epochs / 5.271 seconds | stopped early: True | metrics: {'loss': 0.6534043252468109, 'auc': 0.7077114427860697, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 419 epochs / 9.115 seconds | stopped early: True | metrics: {'loss': 0.6483146846294403, 'auc': 0.5, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 408 epochs / 10.365 seconds | stopped early: True | metrics: {'loss': 0.653148889541626, 'auc': 0.2392324093816631, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 59 epochs / 4.479 seconds | stopped early: True | metrics: {'loss': 0.6542262732982635, 'auc': 0.7975589225589225, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 599 epochs / 15.987 seconds | stopped early: True | metrics: {'loss': 0.651269406080246, 'auc': 0.5, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 10/10: 4 total layers, 3 nodes per hidden layer, dropout p 0.4, gs: ([2, 4, 6], [[[0, 1, 5], 0], [[7, 3], 0]])\n",
      "Fold 1/5 | trained for 349 epochs / 13.595 seconds | stopped early: True | metrics: {'loss': 0.6402012407779694, 'auc': 0.8308457711442786, 'nf': 0.625, 'ni': 0.14285714285714285, 'nnm': 0.625}\n",
      "Fold 2/5 | trained for 178 epochs / 8.984 seconds | stopped early: True | metrics: {'loss': 0.6530862748622894, 'auc': 0.7728026533996684, 'nf': 0.625, 'ni': 0.14285714285714285, 'nnm': 0.625}\n",
      "Fold 3/5 | trained for 119 epochs / 9.318 seconds | stopped early: True | metrics: {'loss': 0.6430465579032898, 'auc': 0.6643923240938167, 'nf': 0.625, 'ni': 0.14285714285714285, 'nnm': 0.625}\n",
      "Fold 4/5 | trained for 233 epochs / 10.951 seconds | stopped early: True | metrics: {'loss': 0.6572718918323517, 'auc': 0.5606060606060606, 'nf': 0.625, 'ni': 0.14285714285714285, 'nnm': 0.625}\n",
      "Fold 5/5 | trained for 143 epochs / 9.362 seconds | stopped early: True | metrics: {'loss': 0.6569827795028687, 'auc': 0.5096801346801347, 'nf': 0.625, 'ni': 0.14285714285714285, 'nnm': 0.625}\n",
      "Dominated Hypervolume: 0.7823498848653911 for Pareto front [[0.80293    0.25       0.03571    0.        ]\n",
      " [0.781078   0.125      0.         0.125     ]\n",
      " [0.80833262 0.375      0.         0.25      ]\n",
      " [0.723406   0.125      0.         0.        ]\n",
      " [0.65039062 0.         0.         0.        ]]\n",
      "Autosaving generation 4 to export/diabetes\n",
      "Saved population + offspring + pareto front of generation 4 to file\n",
      "Generation 5, evaluate 10 individuals\n",
      "Running 5-fold CV for individual 1/10: 3 total layers, 6 nodes per hidden layer, dropout p 0.1, gs: ([0, 1, 2, 3, 4, 7], [[[5, 6], 1]])\n",
      "Fold 1/5 | trained for 515 epochs / 11.944 seconds | stopped early: True | metrics: {'loss': 0.6484871208667755, 'auc': 0.7002487562189055, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 224 epochs / 7.536 seconds | stopped early: True | metrics: {'loss': 0.6335600018501282, 'auc': 0.6969320066334992, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 595 epochs / 14.847 seconds | stopped early: True | metrics: {'loss': 0.6680593490600586, 'auc': 0.5339019189765458, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 190 epochs / 4.649 seconds | stopped early: True | metrics: {'loss': 0.6465507745742798, 'auc': 0.6624579124579124, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 175 epochs / 5.187 seconds | stopped early: True | metrics: {'loss': 0.6390988528728485, 'auc': 0.7428451178451179, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 2/10: 3 total layers, 4 nodes per hidden layer, dropout p 0.1, gs: ([1, 0, 2, 3, 5, 6, 7], [[[4], 1]])\n",
      "Fold 1/5 | trained for 208 epochs / 4.71 seconds | stopped early: True | metrics: {'loss': 0.6681366860866547, 'auc': 0.5087064676616916, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 178 epochs / 5.699 seconds | stopped early: True | metrics: {'loss': 0.6707771420478821, 'auc': 0.5055970149253731, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 238 epochs / 5.616 seconds | stopped early: True | metrics: {'loss': 0.621475338935852, 'auc': 0.5901918976545842, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 216 epochs / 4.193 seconds | stopped early: True | metrics: {'loss': 0.6329818964004517, 'auc': 0.7523148148148148, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 179 epochs / 4.896 seconds | stopped early: True | metrics: {'loss': 0.6489567160606384, 'auc': 0.6420454545454545, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 3/10: 3 total layers, 4 nodes per hidden layer, dropout p 0.5, gs: ([1, 2, 3, 5, 6, 7], [[[0, 4], 1]])\n",
      "Fold 1/5 | trained for 215 epochs / 5.092 seconds | stopped early: True | metrics: {'loss': 0.6203435063362122, 'auc': 0.6569237147595356, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 283 epochs / 7.163 seconds | stopped early: True | metrics: {'loss': 0.6286936700344086, 'auc': 0.6154643449419569, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 176 epochs / 5.463 seconds | stopped early: True | metrics: {'loss': 0.6500782072544098, 'auc': 0.6236673773987207, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 344 epochs / 10.575 seconds | stopped early: True | metrics: {'loss': 0.6502510607242584, 'auc': 0.7030723905723906, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 311 epochs / 7.802 seconds | stopped early: True | metrics: {'loss': 0.6301308274269104, 'auc': 0.7680976430976431, 'nf': 0.25, 'ni': 0.03571428571428571, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 4/10: 3 total layers, 4 nodes per hidden layer, dropout p 0.1, gs: ([2, 4, 6, 7], [[[0, 1, 3, 5], 1]])\n",
      "Fold 1/5 | trained for 416 epochs / 13.59 seconds | stopped early: True | metrics: {'loss': 0.5813480615615845, 'auc': 0.806799336650083, 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 635 epochs / 15.918 seconds | stopped early: True | metrics: {'loss': 0.5799705982208252, 'auc': 0.8283582089552239, 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 432 epochs / 11.79 seconds | stopped early: True | metrics: {'loss': 0.5161931216716766, 'auc': 0.873773987206823, 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 283 epochs / 6.501 seconds | stopped early: True | metrics: {'loss': 0.594682365655899, 'auc': 0.7563131313131314, 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 551 epochs / 13.807 seconds | stopped early: True | metrics: {'loss': 0.5654817521572113, 'auc': 0.7937710437710438, 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 5/10: 4 total layers, 4 nodes per hidden layer, dropout p 0.7, gs: ([1, 0, 2, 3, 4, 6, 7], [[[5], 1]])\n",
      "Fold 1/5 | trained for 126 epochs / 5.742 seconds | stopped early: True | metrics: {'loss': 0.667858898639679, 'auc': 0.33499170812603646, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 2/5 | trained for 523 epochs / 17.335 seconds | stopped early: True | metrics: {'loss': 0.6452439725399017, 'auc': 0.4044361525704809, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 3/5 | trained for 29 epochs / 9.075 seconds | stopped early: True | metrics: {'loss': 0.6470260918140411, 'auc': 0.6735607675906183, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 4/5 | trained for 8 epochs / 6.035 seconds | stopped early: True | metrics: {'loss': 0.6645233333110809, 'auc': 0.6847643097643097, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Fold 5/5 | trained for 214 epochs / 7.047 seconds | stopped early: True | metrics: {'loss': 0.6526012122631073, 'auc': 0.23590067340067342, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.0}\n",
      "Running 5-fold CV for individual 6/10: 4 total layers, 4 nodes per hidden layer, dropout p 0.3, gs: ([7, 0, 1, 2, 3, 4, 6], [[[5], 0]])\n",
      "Fold 1/5 | trained for 264 epochs / 8.133 seconds | stopped early: True | metrics: {'loss': 0.6393414437770844, 'auc': 0.6697761194029851, 'nf': 0.125, 'ni': 0.0, 'nnm': 0.125}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m\n\u001b[1;32m     24\u001b[0m     eagga \u001b[38;5;241m=\u001b[39m EAGGA(\n\u001b[1;32m     25\u001b[0m         oml_dataset\u001b[38;5;241m=\u001b[39moml_dataset,\n\u001b[1;32m     26\u001b[0m         class_positive\u001b[38;5;241m=\u001b[39mclass_positive,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         file_path\u001b[38;5;241m=\u001b[39mfile_path\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m     eagga\u001b[38;5;241m.\u001b[39mload_population(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     pareto_front \u001b[38;5;241m=\u001b[39m \u001b[43meagga\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_eagga\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     pareto_fronts\u001b[38;5;241m.\u001b[39mappend(pareto_front)\n\u001b[1;32m     39\u001b[0m pareto_fronts\n",
      "File \u001b[0;32m~/SageMaker/ws2425-tab-ml/code/classes.py:618\u001b[0m, in \u001b[0;36mEAGGA.run_eagga\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, individual \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffspring):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv_k\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-fold CV for individual \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffspring)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindividual[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m total layers, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindividual[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes_per_hidden_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes per hidden layer, dropout p \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindividual[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_dropout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, gs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindividual[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 618\u001b[0m     individual[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation\u001b[38;5;241m.\u001b[39mappend(individual)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m time_start \u001b[38;5;241m+\u001b[39m timedelta(seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecs_total):\n",
      "File \u001b[0;32m~/SageMaker/ws2425-tab-ml/code/classes.py:716\u001b[0m, in \u001b[0;36mEAGGA.run_cv\u001b[0;34m(self, individual)\u001b[0m\n\u001b[1;32m    713\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    714\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m--> 716\u001b[0m model, stop_epoch, stop_secs, stopped_early, losses_stop_early \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_stop_early\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperformance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(loss_fn, model, dataset_val))\n\u001b[1;32m    719\u001b[0m \u001b[38;5;66;03m# print prior to adding losses_stop_early to avoid long + rather uninformative output of loss history, only intended to be used for plotting\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/ws2425-tab-ml/code/classes.py:756\u001b[0m, in \u001b[0;36mEAGGA.train\u001b[0;34m(self, optimizer, loss_fn, model, dataset_train, dataset_stop_early)\u001b[0m\n\u001b[1;32m    754\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m loss_fn(batch_output, batch_target)\n\u001b[1;32m    755\u001b[0m     running_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 756\u001b[0m     \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# compute gradients\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[1;32m    760\u001b[0m running_epoch_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader_train)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hps = {\n",
    "    'total_layers': (3, 10),\n",
    "    'nodes_per_hidden_layer': (3, 20),\n",
    "    'mu': 100,\n",
    "    'lambda': 10,\n",
    "    'holdout_train_size': 2/3,\n",
    "    'cv_k': 5\n",
    "}\n",
    "\n",
    "batch_size = 64\n",
    "min_epochs = 200\n",
    "patience = 100\n",
    "\n",
    "secs_per_fold = 2 * 60\n",
    "secs_total = 8 * 60 * 60\n",
    "\n",
    "pareto_fronts = list()\n",
    "for (oml_dataset, class_positive) in zip(oml_datasets, positive_classes):\n",
    "    name = oml_dataset.name\n",
    "    print(f'Dataset {name}')\n",
    "\n",
    "    file_path = os.path.join('export', name)\n",
    "    \n",
    "    eagga = EAGGA(\n",
    "        oml_dataset=oml_dataset,\n",
    "        class_positive=class_positive,\n",
    "        hps=hps,\n",
    "        batch_size=batch_size,\n",
    "        min_epochs=min_epochs,\n",
    "        patience=patience,\n",
    "        secs_per_fold=secs_per_fold,\n",
    "        secs_total=secs_total,\n",
    "        file_path=file_path\n",
    "    )\n",
    "    #eagga.load_population(0)\n",
    "    \n",
    "    pareto_front = eagga.run_eagga()\n",
    "    pareto_fronts.append(pareto_front)\n",
    "pareto_fronts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
