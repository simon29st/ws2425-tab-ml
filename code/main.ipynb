{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%pip install setuptools==70.3.0  # apparently setuptools < 71.0.0 is required for a required package of openml to install on here\\n%pip install openml\\n%pip install numpy\\n%pip install pandas\\n# cf. https://pytorch.org/get-started/locally/\\n#%pip install torch torchvision torchaudio  # cuda 12.4, cf. nvidia-smi shell command\\n%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu  # cpu version\\n%pip install -U scikit-learn\\n%pip install scipy\\n%pip install -U pymoo\\n%pip list'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%pip install setuptools==70.3.0  # apparently setuptools < 71.0.0 is required for a required package of openml to install on here\n",
    "%pip install openml\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "# cf. https://pytorch.org/get-started/locally/\n",
    "#%pip install torch torchvision torchaudio  # cuda 12.4, cf. nvidia-smi shell command\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu  # cpu version\n",
    "%pip install -U scikit-learn\n",
    "%pip install scipy\n",
    "%pip install -U pymoo\n",
    "%pip list'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import tasks\n",
    "\n",
    "from classes import EAGGA\n",
    "\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\openml\\tasks\\functions.py:372: UserWarning: `download_data` will default to False starting in 0.16. Please set `download_data` explicitly to suppress this warning.\n",
      "  warnings.warn(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\openml\\tasks\\functions.py:380: UserWarning: `download_qualities` will default to False starting in 0.16. Please set `download_qualities` explicitly to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(359955, 'blood-transfusion-service-center', '2', (748, 5)), (10093, 'banknote-authentication', '2', (1372, 5)), (146820, 'wilt', '2', (4839, 6)), (168350, 'phoneme', '2', (5404, 6)), (37, 'diabetes', 'tested_positive', (768, 9)), (146819, 'climate-model-simulation-crashes', '1', (540, 19)), (359972, 'sylvine', '1', (5124, 21)), (3913, 'kc2', 'yes', (522, 22)), (3918, 'pc1', True, (1109, 22)), (359962, 'kc1', True, (2109, 22)), (3904, 'jm1', True, (10885, 22)), (167120, 'numerai28.6', '1', (96320, 22)), (9946, 'wdbc', '2', (569, 31)), (359975, 'Satellite', 'Anomaly', (5100, 37)), (3903, 'pc3', True, (1563, 38)), (43, 'spambase', '1', (4601, 58)), (190137, 'ozone-level-8hr', '2', (2534, 73)), (190392, 'madeline', '1', (3140, 260)), (190410, 'philippine', '1', (5832, 309)), (189922, 'gina', '1', (3153, 971))]\n"
     ]
    }
   ],
   "source": [
    "oml_task_ids = [37, 43, 3903, 3904, 3913, 3918, 10093, 9946, 146819, 359955, 189922, 359962, 190392, 167120, 190137, 190410, 168350, 359975, 359972, 146820]\n",
    "oml_tasks = tasks.get_tasks(oml_task_ids)\n",
    "\n",
    "oml_datasets = [oml_task.get_dataset() for oml_task in oml_tasks]\n",
    "\n",
    "# define positive classes\n",
    "positive_classes = ['tested_positive', '1', True, True, 'yes', True, '2', '2', '1', '2', '1', True, '1', '1', '2', '1', '2', 'Anomaly', '1', '2']\n",
    "\n",
    "zipped = list(zip(oml_task_ids, oml_datasets, positive_classes))\n",
    "zipped = sorted(zipped, key=lambda item: (item[1].get_data()[0].shape[1], item[1].get_data()[0].shape[0]))  # order ascending by # of features, tiebreaker is # of samples\n",
    "print([(id, ds.name, class_pos, ds.get_data()[0].shape) for id, ds, class_pos in zipped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance 1: [(359955, 'blood-transfusion-service-center', '2', (748, 5)), (146819, 'climate-model-simulation-crashes', '1', (540, 19)), (3904, 'jm1', True, (10885, 22)), (43, 'spambase', '1', (4601, 58))]\n",
      "instance 2: [(10093, 'banknote-authentication', '2', (1372, 5)), (359972, 'sylvine', '1', (5124, 21)), (167120, 'numerai28.6', '1', (96320, 22)), (190137, 'ozone-level-8hr', '2', (2534, 73))]\n",
      "instance 3: [(146820, 'wilt', '2', (4839, 6)), (3913, 'kc2', 'yes', (522, 22)), (9946, 'wdbc', '2', (569, 31)), (190392, 'madeline', '1', (3140, 260))]\n",
      "instance 4: [(168350, 'phoneme', '2', (5404, 6)), (3918, 'pc1', True, (1109, 22)), (359975, 'Satellite', 'Anomaly', (5100, 37)), (190410, 'philippine', '1', (5832, 309))]\n",
      "instance 5: [(37, 'diabetes', 'tested_positive', (768, 9)), (359962, 'kc1', True, (2109, 22)), (3903, 'pc3', True, (1563, 38)), (189922, 'gina', '1', (3153, 971))]\n",
      "20\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "zipped_instance_1 = zipped[::5]\n",
    "zipped_instance_2 = zipped[1::5]\n",
    "zipped_instance_3 = zipped[2::5]\n",
    "zipped_instance_4 = zipped[3::5]\n",
    "zipped_instance_5 = zipped[4::5]\n",
    "\n",
    "print('instance 1:', [(id, ds.name, class_pos, ds.get_data()[0].shape) for id, ds, class_pos in zipped_instance_1])\n",
    "print('instance 2:', [(id, ds.name, class_pos, ds.get_data()[0].shape) for id, ds, class_pos in zipped_instance_2])\n",
    "print('instance 3:', [(id, ds.name, class_pos, ds.get_data()[0].shape) for id, ds, class_pos in zipped_instance_3])\n",
    "print('instance 4:', [(id, ds.name, class_pos, ds.get_data()[0].shape) for id, ds, class_pos in zipped_instance_4])\n",
    "print('instance 5:', [(id, ds.name, class_pos, ds.get_data()[0].shape) for id, ds, class_pos in zipped_instance_5])\n",
    "\n",
    "# verify that all 20 are included\n",
    "zipped_instances_union = set.union(\n",
    "    set(ds.name for _, ds, _ in zipped_instance_1),\n",
    "    set(ds.name for _, ds, _ in zipped_instance_2),\n",
    "    set(ds.name for _, ds, _ in zipped_instance_3),\n",
    "    set(ds.name for _, ds, _ in zipped_instance_4),\n",
    "    set(ds.name for _, ds, _ in zipped_instance_5)\n",
    ")\n",
    "print(len(zipped_instances_union))\n",
    "print(zipped_instances_union == set(ds.name for _, ds, _ in zipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset blood-transfusion-service-center\n",
      "Dataset blood-transfusion-service-center\n",
      "Starting init population\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished init population\n",
      "Start EAGGA Training at 2025-03-06T21:46:13.562018\n",
      "Generation 1, evaluate 100 individuals\n",
      "Running 5-fold CV for individual 1/100: 3 total layers, 7 nodes per hidden layer, dropout p 0.2, gs: ([], [[[0, 2], 0], [[1], 0], [[3], 0]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 43\u001b[0m\n\u001b[0;32m     29\u001b[0m file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mexport\u001b[39m\u001b[39m'\u001b[39m, name)\n\u001b[0;32m     31\u001b[0m eagga \u001b[39m=\u001b[39m EAGGA(\n\u001b[0;32m     32\u001b[0m     oml_dataset\u001b[39m=\u001b[39moml_dataset,\n\u001b[0;32m     33\u001b[0m     class_positive\u001b[39m=\u001b[39mclass_positive,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     file_path\u001b[39m=\u001b[39mfile_path\n\u001b[0;32m     41\u001b[0m )\n\u001b[1;32m---> 43\u001b[0m pareto_set, pareto_front_val, pareto_front_test, dhv_val, dhv_test \u001b[39m=\u001b[39m eagga\u001b[39m.\u001b[39;49mrun_eagga()\n\u001b[0;32m     45\u001b[0m pareto_sets\u001b[39m.\u001b[39mappend(pareto_set)\n\u001b[0;32m     46\u001b[0m pareto_fronts_val\u001b[39m.\u001b[39mappend(pareto_front_val)\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:640\u001b[0m, in \u001b[0;36mEAGGA.run_eagga\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    638\u001b[0m logging\u001b[39m.\u001b[39minfo(msg)\n\u001b[0;32m    639\u001b[0m \u001b[39mprint\u001b[39m(msg)\n\u001b[1;32m--> 640\u001b[0m individual[\u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_cv(individual)\n\u001b[0;32m    641\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation\u001b[39m.\u001b[39mappend(individual)\n\u001b[0;32m    643\u001b[0m \u001b[39mif\u001b[39;00m datetime\u001b[39m.\u001b[39mnow() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m time_start \u001b[39m+\u001b[39m timedelta(seconds\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msecs_total):\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:809\u001b[0m, in \u001b[0;36mEAGGA.run_cv\u001b[1;34m(self, individual)\u001b[0m\n\u001b[0;32m    806\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m    807\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m--> 809\u001b[0m model, stop_epoch, stop_secs, stopped_early, losses_stop_early \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(optimizer, loss_fn, model, dataset_train, dataset_stop_early)\n\u001b[0;32m    811\u001b[0m metrics[\u001b[39m'\u001b[39m\u001b[39mperformance\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval(loss_fn, model, dataset_val))\n\u001b[0;32m    812\u001b[0m \u001b[39m# print prior to adding losses_stop_early to avoid long + rather uninformative output of loss history, only intended to be used for plotting\u001b[39;00m\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:865\u001b[0m, in \u001b[0;36mEAGGA.train\u001b[1;34m(self, optimizer, loss_fn, model, dataset_train, dataset_stop_early, with_stop_early, stop_after_epochs)\u001b[0m\n\u001b[0;32m    862\u001b[0m             model\u001b[39m.\u001b[39mnetworks[i]\u001b[39m.\u001b[39mapply(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonotonicity_clipper)  \u001b[39m# applies weight clipper recursively to network + its children, cf. https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[39mif\u001b[39;00m with_stop_early:\n\u001b[1;32m--> 865\u001b[0m     should_stop_early, current_best \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstop_early(loss_history, current_best, loss_fn, model, dataset_stop_early)\n\u001b[0;32m    866\u001b[0m     \u001b[39mif\u001b[39;00m should_stop_early:\n\u001b[0;32m    867\u001b[0m         \u001b[39mreturn\u001b[39;00m current_best[\u001b[39m1\u001b[39m], current_best[\u001b[39m0\u001b[39m], datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m time_start, \u001b[39mTrue\u001b[39;00m, loss_history\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:882\u001b[0m, in \u001b[0;36mEAGGA.stop_early\u001b[1;34m(self, loss_history, current_best, loss_fn, model, dataset_stop_early)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mstop_early\u001b[39m(\u001b[39mself\u001b[39m, loss_history: \u001b[39mlist\u001b[39m, current_best: \u001b[39mtuple\u001b[39m, loss_fn, model: NeuralNetwork, dataset_stop_early: Dataset) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m:\n\u001b[0;32m    880\u001b[0m     \u001b[39m# stopping criterion: mean of 'patience' previous losses over [t-patience, t] < current loss at t+1\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[39m# if True -> go back to model with min loss within [t-patience, t]\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(loss_fn, model, dataset_stop_early, only_loss\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    883\u001b[0m     loss_history\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m    885\u001b[0m     epoch \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(loss_history) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:905\u001b[0m, in \u001b[0;36mEAGGA.eval\u001b[1;34m(self, loss_fn, model, dataset_eval, only_loss)\u001b[0m\n\u001b[0;32m    903\u001b[0m batch_predictions \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[0;32m    904\u001b[0m batch_targets \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m--> 905\u001b[0m \u001b[39mfor\u001b[39;49;00m batch_input, batch_target \u001b[39min\u001b[39;49;00m loader_eval:\n\u001b[0;32m    906\u001b[0m     \u001b[39mwith\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mno_grad():\n\u001b[0;32m    907\u001b[0m         batch_output \u001b[39m=\u001b[39;49m model(\u001b[39m*\u001b[39;49mbatch_input)\u001b[39m.\u001b[39;49mflatten()\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    709\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:117\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m):\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX[idx, group] \u001b[39mfor\u001b[39;49;00m group \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_groups), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[idx]\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:117\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m):\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX[idx, group] \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_groups), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[idx]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hps = {\n",
    "    'total_layers': (3, 10),\n",
    "    'nodes_per_hidden_layer': (3, 20),\n",
    "    'mu': 100,\n",
    "    'lambda': 10,\n",
    "    'holdout_train_size': 2/3,\n",
    "    'cv_k': 5\n",
    "}\n",
    "\n",
    "batch_size = 64\n",
    "min_epochs = 200\n",
    "patience = 100\n",
    "\n",
    "secs_per_fold = 2 * 60\n",
    "secs_total = 8 * 60 * 60\n",
    "\n",
    "pareto_sets = list()\n",
    "pareto_fronts_val = list()\n",
    "pareto_fronts_test= list()\n",
    "dhvs_val = list()\n",
    "dhvs_test = list()\n",
    "\n",
    "for (_, oml_dataset, class_positive) in zipped_instance_1:\n",
    "    name = oml_dataset.name\n",
    "    msg = f'Dataset {name}'\n",
    "    print(msg)\n",
    "    logging.info(msg)\n",
    "\n",
    "    file_path = os.path.join('export', name)\n",
    "    \n",
    "    eagga = EAGGA(\n",
    "        oml_dataset=oml_dataset,\n",
    "        class_positive=class_positive,\n",
    "        hps=hps,\n",
    "        batch_size=batch_size,\n",
    "        min_epochs=min_epochs,\n",
    "        patience=patience,\n",
    "        secs_per_fold=secs_per_fold,\n",
    "        secs_total=secs_total,\n",
    "        file_path=file_path\n",
    "    )\n",
    "    \n",
    "    pareto_set, pareto_front_val, pareto_front_test, dhv_val, dhv_test = eagga.run_eagga()\n",
    "    \n",
    "    pareto_sets.append(pareto_set)\n",
    "    pareto_fronts_val.append(pareto_front_val)\n",
    "    pareto_fronts_test.append(pareto_front_test)\n",
    "    dhvs_val.append(dhv_val)\n",
    "    dhvs_test.append(dhv_test)\n",
    "\n",
    "\n",
    "msg = f'pareto_set {pareto_set}'\n",
    "print(msg)\n",
    "logging.info(msg)\n",
    "\n",
    "msg = f'pareto_fronts_val {pareto_fronts_val}'\n",
    "print(msg)\n",
    "logging.info(msg)\n",
    "\n",
    "msg = f'pareto_fronts_test {pareto_fronts_test}'\n",
    "print(msg)\n",
    "logging.info(msg)\n",
    "\n",
    "msg = f'dhvs_val {dhvs_val}'\n",
    "print(msg)\n",
    "logging.info(msg)\n",
    "\n",
    "msg = f'dhvs_test {dhvs_test}'\n",
    "print(msg)\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
