{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import tasks\n",
    "\n",
    "from classes import EAGGA\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\openml\\tasks\\functions.py:372: UserWarning: `download_data` will default to False starting in 0.16. Please set `download_data` explicitly to suppress this warning.\n",
      "  warnings.warn(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\openml\\tasks\\functions.py:380: UserWarning: `download_qualities` will default to False starting in 0.16. Please set `download_qualities` explicitly to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "oml_task_ids = [37, 43, 3903, 3904, 3913, 3918, 10093, 9946, 146819, 359955, 189922, 359962, 190392, 167120, 190137, 190410, 168350, 359975, 359972, 146820]\n",
    "oml_tasks = tasks.get_tasks(oml_task_ids)\n",
    "\n",
    "oml_datasets = [oml_task.get_dataset() for oml_task in oml_tasks]\n",
    "\n",
    "# define positive classes\n",
    "positive_classes = ['tested_positive', '1', True, True, 'yes', True, '2', '2', '1', '2', '1', True, '1', '1', '2', '1', '2', 'Anomaly', '1', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset diabetes\n",
      "Starting init population\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished init population\n",
      "Loaded population + offspring from file, discarded previous population + offspring\n",
      "Start EAGGA at 2025-03-02T18:28:44.751089\n",
      "Generation 8, evaluate 5 individuals\n",
      "Running 5-fold CV for individual: 3 total layers, 5 nodes per hidden layer, gs: ([5, 0], [[[7, 3, 6, 2], 0], [[4, 1], 0]])\n",
      "Stop early: 0.539698925614357 < 0.5422048469384512, epoch stop: 55\n",
      "Fold 1/5 | trained for 55 epochs | metrics: {'loss': 0.6764723914010184, 'auc': np.float64(0.5729684908789385), 'nf': 0.75, 'ni': 0.25, 'nnm': 0.75}\n",
      "Stop early: 0.6768201420704524 < 0.6768423318862915, epoch stop: 12\n",
      "Fold 2/5 | trained for 12 epochs | metrics: {'loss': 0.655017444065639, 'auc': np.float64(0.5091210613598673), 'nf': 0.75, 'ni': 0.25, 'nnm': 0.75}\n",
      "Stop early: 0.5569929877916971 < 0.5590639313062032, epoch stop: 28\n",
      "Fold 3/5 | trained for 28 epochs | metrics: {'loss': 0.5781799384525844, 'auc': np.float64(0.7567340067340067), 'nf': 0.75, 'ni': 0.25, 'nnm': 0.75}\n",
      "Stop early: 0.6684763630231221 < 0.6729922990004221, epoch stop: 39\n",
      "Fold 4/5 | trained for 39 epochs | metrics: {'loss': 0.6629962750843593, 'auc': np.float64(0.40824915824915825), 'nf': 0.75, 'ni': 0.25, 'nnm': 0.75}\n",
      "Stop early: 0.6532930791378021 < 0.653566817442576, epoch stop: 14\n",
      "Fold 5/5 | trained for 14 epochs | metrics: {'loss': 0.6425188779830933, 'auc': np.float64(0.5577825159914712), 'nf': 0.75, 'ni': 0.25, 'nnm': 0.75}\n",
      "Running 5-fold CV for individual: 4 total layers, 10 nodes per hidden layer, gs: ([1, 3, 2, 7], [[[0, 5, 4, 6], 0]])\n",
      "Stop early: 0.5490480328599612 < 0.5497824996709824, epoch stop: 49\n",
      "Fold 1/5 | trained for 49 epochs | metrics: {'loss': 0.5653966580118451, 'auc': np.float64(0.7628524046434495), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.6494201054175694 < 0.6608321964740753, epoch stop: 11\n",
      "Fold 2/5 | trained for 11 epochs | metrics: {'loss': 0.6546791536467416, 'auc': np.float64(0.6198175787728025), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.6300121992826462 < 0.6332741131385168, epoch stop: 14\n",
      "Fold 3/5 | trained for 14 epochs | metrics: {'loss': 0.6490403924669538, 'auc': np.float64(0.5976430976430976), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.6870644112428029 < 0.6894608537356058, epoch stop: 21\n",
      "Fold 4/5 | trained for 21 epochs | metrics: {'loss': 0.6751314742224557, 'auc': np.float64(0.5340909090909091), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Stop early: 0.7162533054749172 < 0.7250781357288361, epoch stop: 5\n",
      "Fold 5/5 | trained for 5 epochs | metrics: {'loss': 0.6256413630076817, 'auc': np.float64(0.594456289978678), 'nf': 0.5, 'ni': 0.21428571428571427, 'nnm': 0.5}\n",
      "Running 5-fold CV for individual: 3 total layers, 5 nodes per hidden layer, gs: ([0], [[[6, 2], 1], [[3], 0], [[5, 4], 1], [[7, 1], 0]])\n",
      "Stop early: 0.6681493664781252 < 0.6737239956855774, epoch stop: 21\n",
      "Fold 1/5 | trained for 21 epochs | metrics: {'loss': 0.5751385859080723, 'auc': np.float64(0.7417081260364843), 'nf': 0.875, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Stop early: 0.6217087805271149 < 0.6221027572949728, epoch stop: 28\n",
      "Fold 2/5 | trained for 28 epochs | metrics: {'loss': 0.582582814352853, 'auc': np.float64(0.7255389718076285), 'nf': 0.875, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Stop early: 0.5926561211546262 < 0.6336329132318497, epoch stop: 13\n",
      "Fold 3/5 | trained for 13 epochs | metrics: {'loss': 0.6464508431298392, 'auc': np.float64(0.5972222222222222), 'nf': 0.875, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Stop early: 0.6320420424143472 < 0.6337989866733551, epoch stop: 24\n",
      "Fold 4/5 | trained for 24 epochs | metrics: {'loss': 0.6581817184175763, 'auc': np.float64(0.4882154882154882), 'nf': 0.875, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Stop early: 0.6581256439288456 < 0.660081128279368, epoch stop: 20\n",
      "Fold 5/5 | trained for 20 epochs | metrics: {'loss': 0.6046354344912938, 'auc': np.float64(0.6396588486140726), 'nf': 0.875, 'ni': 0.10714285714285714, 'nnm': 0.375}\n",
      "Running 5-fold CV for individual: 4 total layers, 8 nodes per hidden layer, gs: ([6, 3, 2], [[[7, 5, 1, 0, 4], 0]])\n",
      "Stop early: 0.6130422949790955 < 0.6162666082382202, epoch stop: 54\n",
      "Fold 1/5 | trained for 54 epochs | metrics: {'loss': 0.6120954624244145, 'auc': np.float64(0.6703980099502488), 'nf': 0.625, 'ni': 0.35714285714285715, 'nnm': 0.625}\n",
      "Stop early: 0.6741269191106161 < 0.6764007210731506, epoch stop: 20\n",
      "Fold 2/5 | trained for 20 epochs | metrics: {'loss': 0.6325539691107613, 'auc': np.float64(0.5999170812603649), 'nf': 0.625, 'ni': 0.35714285714285715, 'nnm': 0.625}\n",
      "Stop early: 0.6985809405644735 < 0.714496890703837, epoch stop: 13\n",
      "Fold 3/5 | trained for 13 epochs | metrics: {'loss': 0.6223308188574654, 'auc': np.float64(0.6856060606060606), 'nf': 0.625, 'ni': 0.35714285714285715, 'nnm': 0.625}\n",
      "Stop early: 0.6666372815767925 < 0.6666808327039083, epoch stop: 15\n",
      "Fold 4/5 | trained for 15 epochs | metrics: {'loss': 0.6372288124901908, 'auc': np.float64(0.5925925925925926), 'nf': 0.625, 'ni': 0.35714285714285715, 'nnm': 0.625}\n",
      "Stop early: 0.6397348493337631 < 0.6404085755348206, epoch stop: 39\n",
      "Fold 5/5 | trained for 39 epochs | metrics: {'loss': 0.6506350125585284, 'auc': np.float64(0.6213219616204692), 'nf': 0.625, 'ni': 0.35714285714285715, 'nnm': 0.625}\n",
      "Running 5-fold CV for individual: 3 total layers, 3 nodes per hidden layer, gs: ([0, 1, 2, 3, 4, 5, 6, 7], [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m\n\u001b[0;32m     22\u001b[0m eagga \u001b[39m=\u001b[39m EAGGA(\n\u001b[0;32m     23\u001b[0m     oml_dataset\u001b[39m=\u001b[39moml_dataset,\n\u001b[0;32m     24\u001b[0m     class_positive\u001b[39m=\u001b[39mclass_positive,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     file_path\u001b[39m=\u001b[39mfile_path\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m eagga\u001b[39m.\u001b[39mload_population(\u001b[39m6\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m eagga\u001b[39m.\u001b[39;49mrun_eagga()\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:585\u001b[0m, in \u001b[0;36mEAGGA.run_eagga\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGeneration \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, evaluate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffspring)\u001b[39m}\u001b[39;00m\u001b[39m individuals\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    584\u001b[0m \u001b[39mfor\u001b[39;00m individual \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffspring:\n\u001b[1;32m--> 585\u001b[0m     individual[\u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_cv(individual)\n\u001b[0;32m    586\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation\u001b[39m.\u001b[39mappend(individual)\n\u001b[0;32m    588\u001b[0m     \u001b[39mif\u001b[39;00m datetime\u001b[39m.\u001b[39mnow() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m time_start \u001b[39m+\u001b[39m timedelta(seconds\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msecs_total):\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:677\u001b[0m, in \u001b[0;36mEAGGA.run_cv\u001b[1;34m(self, individual)\u001b[0m\n\u001b[0;32m    674\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m    675\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m--> 677\u001b[0m model, stop_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(optimizer, loss_fn, model, dataset_train, dataset_stop_early)\n\u001b[0;32m    679\u001b[0m metrics[\u001b[39m'\u001b[39m\u001b[39mperformance\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval(loss_fn, model, dataset_val))\n\u001b[0;32m    680\u001b[0m metrics[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(stop_epoch)\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:710\u001b[0m, in \u001b[0;36mEAGGA.train\u001b[1;34m(self, optimizer, loss_fn, model, dataset_train, dataset_stop_early)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[39mfor\u001b[39;00m batch_input, batch_target \u001b[39min\u001b[39;00m loader_train:  \u001b[39m# divide data in mini batches\u001b[39;00m\n\u001b[0;32m    709\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# set gradients to 0\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     batch_output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49mbatch_input)\u001b[39m.\u001b[39mflatten()  \u001b[39m# expand batch_input as it is a list of tuples (Dataset getter splits according to group structure)\u001b[39;00m\n\u001b[0;32m    712\u001b[0m     batch_loss \u001b[39m=\u001b[39m loss_fn(batch_output, batch_target)\n\u001b[0;32m    713\u001b[0m     running_epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\workplace_github\\ws2425-tab-ml\\code\\classes.py:61\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, *xs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m i, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(xs):\n\u001b[0;32m     60\u001b[0m     output_networks\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetworks[i](x))\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_out(torch\u001b[39m.\u001b[39;49mcat(output_networks, \u001b[39m1\u001b[39;49m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "hps = {\n",
    "    'total_layers': (3, 10),\n",
    "    'nodes_per_hidden_layer': (3, 20),\n",
    "    'mu': 15,  # TODO: 100\n",
    "    'lambda': 5,  # TODO: 10\n",
    "    'holdout_train_size': 2/3,\n",
    "    'cv_k': 5\n",
    "}\n",
    "\n",
    "batch_size = 16\n",
    "patience = 10\n",
    "\n",
    "secs_per_fold = 30\n",
    "secs_total = 20 * 60\n",
    "\n",
    "for (oml_dataset, class_positive) in zip(oml_datasets[:1], positive_classes[:1]):  # TODO: remove [:1]\n",
    "    name = oml_dataset.name\n",
    "    print(f'Dataset {name}')\n",
    "\n",
    "    file_path = os.path.join('export', name)\n",
    "    \n",
    "    eagga = EAGGA(\n",
    "        oml_dataset=oml_dataset,\n",
    "        class_positive=class_positive,\n",
    "        hps=hps,\n",
    "        batch_size=batch_size,\n",
    "        patience=patience,\n",
    "        secs_per_fold=secs_per_fold,\n",
    "        secs_total=secs_total,\n",
    "        file_path=file_path\n",
    "    )\n",
    "    eagga.load_population(6)\n",
    "    eagga.run_eagga()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
